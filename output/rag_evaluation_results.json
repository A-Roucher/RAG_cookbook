[{"question": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n", "true_answer": "x86_64-unknown-linux-musl", "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md", "generated_answer": "The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture. (Answer according to context provided.)", "retrieved_docs": ["`tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-arm64-musl`\n\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-android-arm64`\n\nThis is the **aarch64-linux-android** binary for `tokenizers`", "`tokenizers-linux-x64-gnu`\n\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-linux-arm64-gnu`\n\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-win32-x64-msvc`\n\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`", "`tokenizers-win32-arm64-msvc`\n\nThis is the **aarch64-pc-windows-msvc** binary for `tokenizers`"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, indicating that the `tokenizers-linux-x64-musl` binary is designed for the x86_64-unknown-linux-musl architecture."}, {"question": "What is the purpose of the BLIP-Diffusion model?\n", "true_answer": "The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md", "generated_answer": "The purpose of the BLIP-Diffusion model, as described in the context provided, is to enable zero-shot subject-driven generation and control-guided zero-shot generation with a pre-trained subject representation. It is proposed in the paper \"BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing\" and can be found in the codebase at salesforce/LAVIS. The model allows for efficient fine-", "retrieved_docs": [". Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).*", "BLIP is a model that is able to perform various multi-modal tasks including:\n- Visual Question Answering \n- Image-Text retrieval (Image-text matching)\n- Image Captioning\n\nThe abstract from the paper is the following:", "## What about the general-purpose Stable Diffusion model?\n\nAs we showed with the Pokemon image generation task, it is possible to achieve a high level of optimization of the Stable Diffusion pipeline when using a relatively small amount of training resources. At the same time, it is well-known that training a general-purpose Stable Diffusion model is an [expensive task](https://www.mosaicml.com/blog/training-stable-diffusion-from-scratch-part-2). However, with enough budget and HW resources, it is possible to optimize the general-purpose model using the described approach and tune it to produce high-quality images. The only caveat we have is related to the token merging method that reduces the model capacity substantially. The rule of thumb here is the more complicated the dataset you have for the training, the less merging ratio you should use during the optimization.", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# BLIP-Diffusion\n\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. \n\n\nThe abstract from the paper is:", "The original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization.\n\n`BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/).\n\n<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\n\n</Tip>\n\n\n## BlipDiffusionPipeline\n[[autodoc]] BlipDiffusionPipeline\n    - all\n    - __call__\n\n## BlipDiffusionControlNetPipeline\n[[autodoc]] BlipDiffusionControlNetPipeline\n    - all\n    - __call__", "[Improved Vector Quantized Diffusion Models](https://arxiv.org/abs/2205.16007) improves upon VQ-Diffusion's sample quality with discrete classifier-free guidance and an alternative inference strategy to address the \"joint distribution issue\" -- see section 3.2 for more details. Discrete classifier-free guidance is merged into diffusers but the alternative inference strategy has not been added yet.\n\nContributions are welcome!", ". We demonstrate SEGA's effectiveness on both latent and pixel-based diffusion models such as Stable Diffusion, Paella, and DeepFloyd-IF using a variety of tasks, thus providing strong evidence for its versatility, flexibility, and improvements over existing methods.*"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies the purpose of the BLIP-Diffusion model as enabling zero-shot subject-driven generation and control-guided zero-shot generation, which aligns with the reference answer's mention of controllable text-to-image generation and editing. However, the response adds details about the pre-trained subject representation and the location of the codebase, which are not required to answer the question but do not detract from the accuracy of the response. The response is cut off at the end, but the essential information regarding the purpose of the model is provided. Therefore, the response is mostly correct, accurate, and factual."}, {"question": "How can a user claim authorship of a paper on the Hugging Face Hub?\n", "true_answer": "By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/paper-pages.md", "generated_answer": "To claim authorship of a paper on the Hugging Face Hub, a user should follow these steps:\n\n1. Go to the Paper page of the paper they wish to claim authorship for.\n2. If the paper is not already linked to their account, they should click on their name in the corresponding Paper page and then click \"claim authorship\".\n3. This will redirect them to their paper settings where they can confirm the request.\n4. The admin team will validate", "retrieved_docs": ["If your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings-dark.png\"/>\n</div>\n\n\n## Frequently Asked Questions\n\n### Can I control which Paper pages show in my profile?\n\nYes! You can visit your Papers in [settings](https://huggingface.co/settings/papers), where you will see a list of verified papers. There, you can click the \"Show on profile\" checkbox to hide/show it in your profile.", "* Visit the Paper page.\n* Filter for other models or datasets on the Hub that cite the same paper.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/>\n</div>\n\n## Claiming authorship to a Paper\n\nThe Hub will attempt to automatically match paper to users based on their email. \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/>\n</div>", "1. First, upload the model associated with the ArXiv paper onto the Hugging Face Hub if it is not already there. ([Detailed instructions are here](./models-uploading))\n\n2. When writing the model card (README.md) for the model, include a link to the ArXiv paper. It's good practice to include a full citation as well. You can see an example of a link and a citation on the [LayoutLM model card](https://huggingface.co/microsoft/layoutlm-base-uncased)\n\n    *Note*: you can verify this step has been carried out successfully by seeing if an ArXiv button appears above the model card. In the case of LayoutLM, the button says: \"arxiv:1912.13318\" and links to the LayoutLM paper on ArXiv.\n\n    ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/arxiv-button.png)\n\n3. Then, create a demo on Spaces that loads this model. Somewhere within the code, the model name must be included in order for Hugging Face to detect that a Space is associated with it.", "The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card", "The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card", "--\ntitle: 'Welcome fastai to the Hugging Face Hub'\nthumbnail: /blog/assets/64_fastai/fastai_hf_blog.png\nauthors:\n- user: espejelomar\n---\n\n# Welcome fastai to the Hugging Face Hub\n\n\n## Making neural nets uncool again... and sharing them\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/64_fastai_hub.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\nFew have done as much as the [fast.ai](https://www.fast.ai/) ecosystem to make Deep Learning accessible. Our mission at Hugging Face is to democratize good Machine Learning. Let's make exclusivity in access to Machine Learning, including [pre-trained models](https://huggingface.co/models), a thing of the past and let's push this amazing field even further.", "### Do you support ACL anthology?\n\nWe're starting with Arxiv as it accounts for 95% of the paper URLs Hugging Face users have linked in their repos organically. We'll check how this evolve and potentially extend to other paper hosts in the future.\n\n### Can I have a Paper page even if I have no model/dataset/Space?\n\nYes. You can go to [the main Papers page](https://huggingface.co/papers), click search and write the name of the paper or the full Arxiv id. If the paper does not exist, you will get an option to index it. You can also just visit the page `hf.co/papers/xxxx.yyyyy` replacing with the arxiv id of the paper you wish to index."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It outlines a clear step-by-step process for claiming authorship of a paper on the Hugging Face Hub, which aligns well with the reference answer. However, the response includes an additional initial step of going to the Paper page, which is not mentioned in the reference answer but is implied as necessary. The final step is also slightly less detailed than the reference answer, as it does not specify that it is the admin team that will validate the claim. Despite these minor differences, the response captures the essence of the process accurately."}, {"question": "What is the purpose of the /healthcheck endpoint in the Datasets server API?\n", "true_answer": "Ensure the app is running", "source_doc": "huggingface/datasets-server/blob/main/services/api/README.md", "generated_answer": "The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the server is running properly. When this endpoint is accessed, the server returns a response indicating that it is healthy and operational. This endpoint can be used to perform basic server health checks and ensure that the Datasets server is functioning correctly.", "retrieved_docs": ["Datasets server API - rows endpoint\n\n> /rows endpoint\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split", "Check dataset validity\n\nBefore you download a dataset from the Hub, it is helpful to know if a specific dataset you're interested in is available. Datasets Server provides the `/is-valid` endpoint to check if a specific dataset works without any errors.\n\nThe API endpoint will return an error for datasets that cannot be loaded with the [\ud83e\udd17 Datasets](https://github.com/huggingface/datasets) library, for example, because the data hasn't been uploaded or the format is not supported.\n\n<Tip warning={true}>\n  The largest datasets are partially supported by Datasets Server. If they are{\" \"}\n  <a href=\"https://huggingface.co/docs/datasets/stream\">streamable</a>, Datasets\n  Server can extract the first 100 rows without downloading the whole dataset.\n  This is especially useful for previewing large datasets where downloading the\n  whole dataset may take hours! See the <code>preview</code> field in the\n  response of <code>/is-valid</code> to check if a dataset is partially\n  supported.\n</Tip>", "## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub_cache` field is null for deleted entries, or when the response is an error. The `num_rows` value is `0` if it could not be determined.\n\n```typescript\n{\n    dataset: string;\n    hub_cache: null | {\n        preview: boolean;\n        viewer: boolean;\n        partial: boolean;\n        num_rows: int;\n    };\n}\n```", "Datasets server API - search service\n\n> /search endpoint\n> /filter endpoint\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### Duckdb index full text search\n- `DUCKDB_INDEX_CACHE_DIRECTORY`: directory where the temporal duckdb index files are downloaded. Defaults to empty.\n- `DUCKDB_INDEX_TARGET_REVISION`: the git revision of the dataset where the index file is stored in the dataset repository.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /search: get a slice of a search result over a dataset split\n- /filter: filter rows of a dataset split", "\ud83e\udd17 Datasets Server\n\nDatasets Server is a lightweight web API for visualizing and exploring all types of datasets - computer vision, speech, text, and tabular - stored on the Hugging Face [Hub](https://huggingface.co/datasets).\n\nThe main feature of the Datasets Server is to auto-convert all the [Hub datasets](https://huggingface.co/datasets) to [Parquet](https://parquet.apache.org/). Read more in the [Parquet section](./parquet).\n\nAs datasets increase in size and data type richness, the cost of preprocessing (storage and compute) these datasets can be challenging and time-consuming.\nTo help users access these modern datasets, Datasets Server runs a server behind the scenes to generate the API responses ahead of time and stores them in a database so they are instantly returned when you make a query through the API.\n\nLet Datasets Server take care of the heavy lifting so you can use a simple **REST API** on any of the **30,000+ datasets on Hugging Face** to:", "Datasets server SSE API\n\n> Server-sent events API for the Datasets server. It's used to update the Hub's backend cache.\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server", "## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /webhook: Add, update or remove a dataset\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by explaining that the /healthcheck endpoint is used to confirm the server's operational status. The response expands on the reference answer by detailing the functionality of the endpoint, which is to return a response indicating the server's health, and by mentioning its use in performing basic server health checks. This additional information supports the correctness and factual nature of the response without deviating from the reference answer's intent."}, {"question": "What is the default context window size for Local Attention in the LongT5 model?\n", "true_answer": "127 tokens", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md", "generated_answer": "The default context window size for Local Attention in the LongT5 model is 127 tokens, as specified in the context under the heading \"Local attention\". The window size can be configured differently for each layer by defining a list in `config.attention_window`.", "retrieved_docs": ["- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). *Local Attention* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l*r)`.", "## Longformer Self Attention\n\nLongformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\n`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\nconventionally done for all tokens in `BertSelfAttention`.\n\nNote that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\nattending tokens so that global attention is *symmetric*.", "## Local attention\n\n[Longformer](#longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\nleft and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\nwindow, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\nrepresentation of the whole sentence.\n\nSome preselected input tokens are also given global attention: for those few tokens, the attention matrix can access\nall tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\ntheir local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:\n\n<div class=\"flex justify-center\">\n    <img scale=\"50 %\" align=\"center\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png\"/>\n</div>", "The abstract from the paper is the following:\n\n*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.*", "-->\n\n# LongT5\n\n## Overview\n\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention.\n\n\nThe abstract from the paper is the following:", "The user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\n`global_attention_mask`:\n\n- 0: the token attends \"locally\",\n- 1: the token attends \"globally\".\n\nFor more information please also refer to [`~LongformerModel.forward`] method.\n\nUsing Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to\n\\\\(\\mathcal{O}(n_s \\times w)\\\\), with \\\\(n_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize. It is assumed that the number of \"globally\" attending tokens is insignificant as compared to the number of\n\"locally\" attending tokens.\n\nFor more information, please refer to the official [paper](https://arxiv.org/pdf/2004.05150.pdf).\n\n\n## Training", "Iz Beltagy, Matthew E. Peters, Arman Cohan\n\nLongformer addresses the memory bottleneck of transformers by replacing conventional self-attention with a combination of windowed/local/sparse (cf. [Sparse Transformers (2019)](https://arxiv.org/abs/1904.10509)) attention and global attention that scales linearly with the sequence length. As opposed to previous long-range transformer models (e.g. [Transformer-XL (2019)](https://arxiv.org/abs/1901.02860), [Reformer (2020)](https://arxiv.org/abs/2001.04451), [Adaptive Attention Span (2019)](https://arxiv.org/abs/1905.07799)), Longformer\u2019s self-attention layer is designed as a drop-in replacement for the standard self-attention, thus making it possible to leverage pre-trained checkpoints for further pre-training and/or fine-tuning on long sequence tasks.\n\nThe standard self-attention matrix (Figure a) scales quadratically with the input length:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer by stating the default context window size as 127 tokens and further explains that this can be configured, which aligns with the detailed nature of the LongT5 model's configuration options."}, {"question": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n", "true_answer": "from_pretrained()", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md", "generated_answer": "The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically detects the correct pipeline class to use based on the task, making it easier to load a checkpoint for a specific task without knowing the specific pipeline class name. For example, to load a Stable Diffusion checkpoint for text-to-image, you can use `AutoPipelineForText2Image.from_pretrained()`. This method also supports", "retrieved_docs": ["The `AutoPipeline` class is designed to simplify the variety of pipelines in \ud83e\udd17 Diffusers. It is a generic, *task-first* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name.\n\n<Tip>\n\nTake a look at the [AutoPipeline](../api/pipelines/auto_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting.\n\n</Tip>\n\nThis tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights.\n\n## Choose an AutoPipeline for your task\n\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:", "# AutoPipeline\n\n\ud83e\udd17 Diffusers is able to complete many different tasks, and you can often reuse the same pretrained weights for multiple tasks such as text-to-image, image-to-image, and inpainting. If you're new to the library and diffusion models though, it may be difficult to know which pipeline to use for a task. For example, if you're using the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint for text-to-image, you might not know that you could also use it for image-to-image and inpainting by loading the checkpoint with the [`StableDiffusionImg2ImgPipeline`] and [`StableDiffusionInpaintPipeline`] classes respectively.", "To seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:", "You can also load a checkpoint with its specific pipeline class. The example above loaded a Stable Diffusion model; to get the same result, use the [`StableDiffusionPipeline`] class:\n\n```python\nfrom diffusers import StableDiffusionPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(repo_id, use_safetensors=True)\n```\n\nA checkpoint (such as [`CompVis/stable-diffusion-v1-4`](https://huggingface.co/CompVis/stable-diffusion-v1-4) or [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)) may also be used for more than one task, like text-to-image or image-to-image. To differentiate what task you want to use the checkpoint for, you have to load it directly with its corresponding task-specific pipeline class:\n\n```python\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(repo_id)\n```\n\n### Local pipeline", "As you can see, loading a model within a pipeline is extremely simple. The only thing you need to watch out for is that the chosen checkpoint is suitable for the task it's going to be used for. For example, here we are loading the `camembert-base` checkpoint in the `fill-mask` pipeline, which is completely fine. But if we were to load this checkpoint in the `text-classification` pipeline, the results would not make any sense because the head of `camembert-base` is not suitable for this task! We recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints:\n\n<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/tasks.png\" alt=\"The task selector on the web interface.\" width=\"80%\"/>\n</div>\n\nYou can also instantiate the checkpoint using the model architecture directly:", "However, for more basic tasks like erasing an object from an image (like the rocks in the road for example), a regular checkpoint yields pretty good results. There isn't as noticeable of difference between the regular and inpaint checkpoint.\n\n<hfoptions id=\"inpaint\">\n<hfoption id=\"runwayml/stable-diffusion-v1-5\">\n\n```py\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\"\n).to(\"cuda\")\npipeline.enable_model_cpu_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable_xformers_memory_efficient_attention()", "You can also instantiate the checkpoint using the model architecture directly:\n\n{#if fw === 'pt'}\n```py\nfrom transformers import CamembertTokenizer, CamembertForMaskedLM\n\ntokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\nmodel = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n```\n\nHowever, we recommend using the [`Auto*` classes](https://huggingface.co/transformers/model_doc/auto.html?highlight=auto#auto-classes) instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the `Auto*` classes makes switching checkpoints simple:\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"camembert-base\")\n```\n{:else}\n```py\nfrom transformers import CamembertTokenizer, TFCamembertForMaskedLM"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies `from_pretrained()` as the method used to load a checkpoint using `AutoPipeline`. The explanation provided is accurate and factual, aligning with the reference answer. The response goes beyond the reference answer by giving an example of how to use the method with a specific pipeline, which demonstrates a clear understanding of the concept."}, {"question": "What is the purpose of Diffusers library?\n", "true_answer": "To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md", "generated_answer": "The purpose of the Diffusers library is to provide a modular toolbox for generating images, audio, and even 3D structures of molecules using state-of-the-art pretrained diffusion models. It offers three main components: diffusion pipelines for inference, interchangeable noise schedulers for balancing generation speed and quality, and pretrained models that can be used as building blocks for creating custom diffusion systems. The library is designed with a focus on usability over performance,", "retrieved_docs": ["## Diffusers\n\nDiffusers is a generative AI library for creating images and videos from text or images with diffusion models. LoRA is an especially popular training method for diffusion models because you can very quickly train and share diffusion models to generate images in new styles. To make it easier to use and try multiple LoRA models, Diffusers uses the PEFT library to help manage different adapters for inference.\n\nFor example, load a base model and then load the [artificialguybr/3DRedmond-V1](https://huggingface.co/artificialguybr/3DRedmond-V1) adapter for inference with the [`load_lora_weights`](https://huggingface.co/docs/diffusers/v0.24.0/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) method. The `adapter_name` argument in the loading method is enabled by PEFT and allows you to set a name for the adapter so it is easier to reference.\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline", "# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:", "\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\n\ud83e\udd17 Diffusers offers three core components:", "Using \ud83e\udde8 `diffusers` at Hugging Face\n\nDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\u2019re looking for a simple inference solution or want to train your own diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.\n\n## Exploring Diffusers in the Hub\n\nThere are over 10,000 `diffusers` compatible pipelines on the Hub which you can find by filtering at the left of [the models page](https://huggingface.co/models?library=diffusers&sort=downloads). Diffusion systems are typically composed of multiple components such as text encoder, UNet, VAE, and scheduler. Even though they are not standalone models, the pipeline abstraction makes it easy to use them for inference or training.\n\nYou can find diffusion pipelines for many different tasks:", "For large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable.\n**However**, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:", "For large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable.\n**However**, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:", "The library has three main components:\n\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in \ud83e\udd17 Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve.\n- Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality.\n- Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It accurately identifies the Diffusers library as a toolbox for generating various types of content using pretrained diffusion models. It also correctly mentions the components such as diffusion pipelines, noise schedulers, and pretrained models. However, the response includes additional details not present in the reference answer, such as the specific mention of images, audio, and 3D structures of molecules, and the focus on usability over performance. These details, while informative, are not necessary to fulfill the purpose described in the reference answer. Therefore, the response is not completely correct as per the reference answer but still provides a largely accurate description of the Diffusers library's purpose."}, {"question": "What method does the EulerAncestralDiscreteScheduler use for sampling?\n", "true_answer": "Ancestral sampling with Euler method steps.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/schedulers/euler_ancestral.md", "generated_answer": "The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps, as described in the original implementation of k-diffusion by Katherine Crowson. This is a fast scheduler that can generate high-quality outputs in around 20-30 steps.", "retrieved_docs": ["# EulerAncestralDiscreteScheduler\n\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n\n## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteSchedulerOutput", "# EulerDiscreteScheduler\n\nThe Euler scheduler (Algorithm 2) is from the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper by Karras et al. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n\n\n## EulerDiscreteScheduler\n[[autodoc]] EulerDiscreteScheduler\n\n## EulerDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_euler_discrete.EulerDiscreteSchedulerOutput", "Let's use the [`SchedulerMixin.from_pretrained`] method to replace the default [`PNDMScheduler`] with a more performant scheduler, [`EulerDiscreteScheduler`]. The `subfolder=\"scheduler\"` argument is required to load the scheduler configuration from the correct [subfolder](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main/scheduler) of the pipeline repository.\n\nThen you can pass the new [`EulerDiscreteScheduler`] instance to the `scheduler` argument in [`DiffusionPipeline`]:\n\n```python\nfrom diffusers import DiffusionPipeline, EulerDiscreteScheduler\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\nscheduler = EulerDiscreteScheduler.from_pretrained(repo_id, subfolder=\"scheduler\")\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id, scheduler=scheduler, use_safetensors=True)\n```\n\n### Safety checker", "Cool, lots of schedulers to look at. Feel free to have a look at their respective class definitions:\n\n- [`EulerDiscreteScheduler`],\n- [`LMSDiscreteScheduler`],\n- [`DDIMScheduler`],\n- [`DDPMScheduler`],\n- [`HeunDiscreteScheduler`],\n- [`DPMSolverMultistepScheduler`],\n- [`DEISMultistepScheduler`],\n- [`PNDMScheduler`],\n- [`EulerAncestralDiscreteScheduler`],\n- [`UniPCMultistepScheduler`],\n- [`KDPM2DiscreteScheduler`],\n- [`DPMSolverSinglestepScheduler`],\n- [`KDPM2AncestralDiscreteScheduler`].\n\nWe will now compare the input prompt with all other schedulers. To change the scheduler of the pipeline you can make use of the\nconvenient [`~ConfigMixin.config`] property in combination with the [`~ConfigMixin.from_config`] function.\n\n```python\npipeline.scheduler.config\n```\n\nreturns a dictionary of the configuration of the scheduler:", "# DPMSolverSinglestepScheduler\n\n`DPMSolverSinglestepScheduler` is a single step scheduler from [DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps](https://huggingface.co/papers/2206.00927) and [DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models](https://huggingface.co/papers/2211.01095) by Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.\n\nDPMSolver (and the improved version DPMSolver++) is a fast dedicated high-order solver for diffusion ODEs with convergence order guarantee. Empirically, DPMSolver sampling with only 20 steps can generate high-quality\nsamples, and it can generate quite good samples even in 10 steps.\n\nThe original implementation can be found at [LuChengTHU/dpm-solver](https://github.com/LuChengTHU/dpm-solver).\n\n## Tips\n\nIt is recommended to set `solver_order` to 2 for guide sampling, and `solver_order=3` for unconditional sampling.", "| Euler               | [`EulerDiscreteScheduler`]          |                                                                                                               |\n| Euler a             | [`EulerAncestralDiscreteScheduler`] |                                                                                                               |\n| Heun                | [`HeunDiscreteScheduler`]           |                                                                                                               |\n| LMS                 | [`LMSDiscreteScheduler`]            |                                                                                                               |\n| LMS Karras          | [`LMSDiscreteScheduler`]            | init with `use_karras_sigmas=True`                                                                            |", "```python\nfrom diffusers import LMSDiscreteScheduler\n\npipeline.scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator).images[0]\nimage\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_lms.png\" width=\"400\"/>\n    <br>\n</p>\n\n\n[`EulerDiscreteScheduler`] and [`EulerAncestralDiscreteScheduler`] can generate high quality results with as little as 30 steps.\n\n```python\nfrom diffusers import EulerDiscreteScheduler\n\npipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\nimage\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the method used by the EulerAncestralDiscreteScheduler as ancestral sampling with Euler method steps. It also provides additional context about the original implementation and the efficiency of the scheduler, which aligns with the reference answer. The response is completely correct, accurate, and factual."}, {"question": "What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n", "true_answer": "IDEFICS", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md", "generated_answer": "The name of the large multimodal model that can solve image-text tasks and is based on Flamingo is IDEFICS.", "retrieved_docs": ["-->\n\n# Image tasks with IDEFICS\n\n[[open-in-colab]]\n\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach \nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. \nFor instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. \nThis approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can \nsolve image-text tasks with a large multimodal model called IDEFICS.", "Another approach to leveraging pre-trained language models for multi-modal tasks is to directly fuse visual information into the layers of a language model decoder using a cross-attention mechanism instead of using images as additional prefixes to the language model. Models such as [VisualGPT](https://arxiv.org/abs/2102.10407), [VC-GPT](https://arxiv.org/abs/2201.12723), and [Flamingo](https://arxiv.org/abs/2204.14198) use this pre-training strategy and are trained on image captioning and visual question-answering tasks. The main goal of such models is to balance the mixture of text generation capacity and visual information efficiently, which is highly important in the absence of large multi-modal datasets.", "[IDEFICS](../model_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198), \na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image \nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content, \ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b) \nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the \ud83e\udd17 Hub. For each variant, you can also find fine-tuned instructed \nversions of the model adapted for conversational use cases.", ". The resulting InstructBLIP models achieve state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and the larger Flamingo. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA IMG). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models.*", "Finally, models such as [MAPL](https://arxiv.org/abs/2210.07179) and [Flamingo](https://arxiv.org/abs/2204.14198) keep both the pre-trained vision encoder and language model frozen. Flamingo sets a new state-of-the-art in few-shot learning on a wide range of open-ended vision and language tasks by adding Perceiver Resampler modules on top of the pre-trained frozen vision model and inserting new cross-attention layers between existing pre-trained and frozen LM layers to condition the LM on visual data.\n\nA nifty advantage of the Frozen PrefixLM pre-training objective is it enables training with limited aligned image-text data, which is particularly useful for domains where aligned multi-modal datasets are not available.\n### 3) Multi-modal Fusing with Cross Attention", "Try out the [demo](https://huggingface.co/spaces/HuggingFaceM4/idefics_playground) and the [models](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct) on the Hub!\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/idefics/woodstock_ai.png\" width=\"600\" alt=\"Screenshot of IDEFICS generation for HF Woodstock of AI\"/>\n</p>\n\n## What is IDEFICS?\n\nIDEFICS is an 80 billion parameters multimodal model that accepts sequences of images and texts as input and generates coherent text as output. It can answer questions about images, describe visual content, create stories grounded in multiple images, etc.\n\nIDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the original closed-source model across various image-text understanding benchmarks. It comes in two variants - 80 billion parameters and 9 billion parameters.", "## Multimodal\n\nMultimodal tasks require a model to process multiple data modalities (text, image, audio, video) to solve a particular problem. Image captioning is an example of a multimodal task where the model takes an image as input and outputs a sequence of text describing the image or some properties of the image. \n\nAlthough multimodal models work with different data types or modalities, internally, the preprocessing steps help the model convert all the data types into embeddings (vectors or list of numbers that holds meaningful information about the data). For a task like image captioning, the model learns relationships between image embeddings and text embeddings.\n\n### Document question answering"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the name of the large multimodal model as IDEFICS, which is in agreement with the reference answer. The information provided is accurate and factual, meeting the criteria for the highest score on the rubric."}, {"question": "What is the purpose of the `gradio.Blocks` API?\n", "true_answer": "The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/7.mdx", "generated_answer": "The `gradio.Blocks` API is a low-level approach for designing web apps with more flexible layouts and data flows in Gradio. It allows for things like controlling where components appear on the page, handling complex data flows (e.g. Outputs can serve as inputs to other functions), and updating properties/visibility of components based on user interaction, all still in Python. With Blocks, users can build very custom and complex applications using Gradio. Examples of popular applications", "retrieved_docs": ["#### Custom Demos with `gr.Blocks`\n\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction \u2014 still all in Python. \n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n#### The Gradio Python & JavaScript Ecosystem", "### Custom Demos with `gr.Blocks`\n\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction \u2014 still all in Python. \n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n### The Gradio Python & JavaScript Ecosystem", "```py\nwith gr.Blocks() as demo:\n    gr.Markdown(\"\u0633\u0644\u0627\u0645\", rtl=True)\ndemo.launch()\n```\n\n- The `get_api_info` method of `Blocks` now supports layout output components [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4871](https://github.com/gradio-app/gradio/pull/4871)\n\n- Added the support for the new command `gradio environment`to make it easier for people to file bug reports if we shipped an easy command to list the OS, gradio version, and versions of gradio/gradio-client dependencies. bu [@varshneydevansh](https://github.com/varshneydevansh) in [PR 4915](https://github.com/gradio-app/gradio/pull/4915).\n\n### Bug Fixes:", "```py\nwith gr.Blocks() as demo:\n    gr.Markdown(\"\u0633\u0644\u0627\u0645\", rtl=True)\ndemo.launch()\n```\n\n- The `get_api_info` method of `Blocks` now supports layout output components [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4871](https://github.com/gradio-app/gradio/pull/4871)\n\n- Added the support for the new command `gradio environment`to make it easier for people to file bug reports if we shipped an easy command to list the OS, gradio version, and versions of gradio/gradio-client dependencies. bu [@varshneydevansh](https://github.com/varshneydevansh) in [PR 4915](https://github.com/gradio-app/gradio/pull/4915).\n\n### Bug Fixes:", "### Contributors Shoutout:\n\n- [@nhankiet](https://github.com/nhankiet) made their first contribution in [PR 1819](https://github.com/gradio-app/gradio/pull/1819)\n\n## 3.0\n\n###### \ud83d\udd25 Gradio 3.0 is the biggest update to the library, ever.\n\n### New Features:\n\n###### 1. Blocks \ud83e\uddf1\n\nBlocks is a new, low-level API that allows you to have full control over the data flows and layout of your application. It allows you to build very complex, multi-step applications. For example, you might want to:\n\n- Group together related demos as multiple tabs in one web app\n- Change the layout of your demo instead of just having all of the inputs on the left and outputs on the right\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a Dropdown) or its visibility based on user input\n\nHere's a simple example that creates the demo below it:", "### Contributors Shoutout:\n\n- [@nhankiet](https://github.com/nhankiet) made their first contribution in [PR 1819](https://github.com/gradio-app/gradio/pull/1819)\n\n## 3.0\n\n###### \ud83d\udd25 Gradio 3.0 is the biggest update to the library, ever.\n\n### New Features:\n\n###### 1. Blocks \ud83e\uddf1\n\nBlocks is a new, low-level API that allows you to have full control over the data flows and layout of your application. It allows you to build very complex, multi-step applications. For example, you might want to:\n\n- Group together related demos as multiple tabs in one web app\n- Change the layout of your demo instead of just having all of the inputs on the left and outputs on the right\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a Dropdown) or its visibility based on user input\n\nHere's a simple example that creates the demo below it:", "Using Gradio Blocks Like Functions\n\nTags: TRANSLATION, HUB, SPACES\n\n**Prerequisite**: This Guide builds on the Blocks Introduction. Make sure to [read that guide first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control).\n\n## Introduction\n\nDid you know that apart from being a full-stack machine learning demo, a Gradio Blocks app is also a regular-old python function!?\n\nThis means that if you have a gradio Blocks (or Interface) app called `demo`, you can use `demo` like you would any python function.\n\nSo doing something like `output = demo(\"Hello\", \"friend\")` will run the first event defined in `demo` on the inputs \"Hello\" and \"friend\" and store it\nin the variable `output`.\n\nIf I put you to sleep \ud83e\udd71, please bear with me! By using apps like functions, you can seamlessly compose Gradio apps.\nThe following section will show how.\n\n## Treating Blocks like functions\n\nLet's say we have the following demo that translates english text to german text."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided aligns well with the reference answer, detailing the purpose of the `gradio.Blocks` API with accuracy. It correctly identifies the API as a tool for creating web apps with flexible layouts and data flows, and it mentions specific functionalities such as the ability to control component placement, handle complex data flows, and update component properties based on user interactions. The response also correctly notes that Blocks can be used to build complex applications, which is in line with the reference answer's emphasis on building multi-step applications. The response expands on the reference answer by providing examples of what can be achieved with the API, although it does not list specific examples as it trails off. Overall, the response is accurate and factual, providing a clear understanding of the `gradio.Blocks` API's purpose."}, {"question": "What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n", "true_answer": "The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unclip.md", "generated_answer": "The paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" proposes a two-stage model called Prior Transformer for predicting CLIP image embeddings from CLIP text embeddings through a denoising diffusion process. The purpose of this model is to generate high-quality images based on text prompts using the CLIP latent space, which allows for more efficient and accurate image generation compared to traditional generative models. The first stage of the model involves", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Prior Transformer\n\nThe Prior Transformer was originally introduced in [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process.\n\nThe abstract from the paper is:", "-->\n\n# CLIP\n\n## Overview\n\nThe CLIP model was proposed in [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. CLIP\n(Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be\ninstructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing\nfor the task, similarly to the zero-shot capabilities of GPT-2 and 3.\n\nThe abstract from the paper is the following:", "This model was contributed by [valhalla](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/openai/CLIP).\n\n## Usage tips and example\n\nCLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image\nclassification. CLIP uses a ViT like transformer to get visual features and a causal language model to get the text\nfeatures. Both the text and visual features are then projected to a latent space with identical dimension. The dot\nproduct between the projected image and text features is then used as a similar score.", ". Implemented on large-scale paired image-text data, UniDiffuser is able to perform image, text, text-to-image, image-to-text, and image-text pair generation by setting proper timesteps without additional overhead. In particular, UniDiffuser is able to produce perceptually realistic samples in all tasks and its quantitative results (e.g., the FID and CLIP score) are not only superior to existing general-purpose models but also comparable to the bespoken models (e.g., Stable Diffusion and DALL-E 2) in representative tasks (e.g., text-to-image generation).*", "-->\n\n# CLIPSeg\n\n## Overview\n\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo L\u00fcddecke\nand Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero- and one-shot image segmentation.\n\nThe abstract from the paper is the following:", ". Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference. Project Page: [this https URL](https://latent-consistency-models.github.io/).*", ". Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference. Project Page: [this https URL](https://latent-consistency-models.github.io/)*"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies the two-stage model as the Prior Transformer and mentions its function of predicting CLIP image embeddings from text embeddings, which aligns with the reference answer's mention of generating a CLIP image embedding from a text caption. The response also accurately states the purpose of generating high-quality images from text prompts using the CLIP latent space, which is a more detailed explanation of the reference answer's point about generating an image conditioned on the image embedding. However, the response is incomplete as it ends abruptly and does not mention the second stage of the model, which is part of the reference answer's explanation. Therefore, the response is mostly correct, accurate, and factual but not complete."}, {"question": "What command is used to install the requirements for a research project using \ud83e\udd17 Transformers?\n", "true_answer": "pip install -r requirements.txt", "source_doc": "huggingface/transformers/blob/main/examples/research_projects/README.md", "generated_answer": "The command to install the requirements for a research project using \ud83e\udd17 Transformers is:\n\n```bash\npip install -r requirements.txt\n```\n\nThis command should be run inside the folder of the chosen research project. If help is needed with any of the projects, the author(s) should be contacted, as indicated at the top of the README file in each folder. Note that some projects may require a specific version of Diffusers", "retrieved_docs": ["Research projects\n\nThis folder contains various research projects using \ud83e\udde8 Diffusers.\nThey are not really maintained by the core maintainers of this library and often require a specific version of Diffusers that is indicated in the requirements file of each folder.\nUpdating them to the most recent version of the library will require some work.\n\nTo use any of them, just run the command\n\n```\npip install -r requirements.txt\n```\ninside the folder of your choice.\n\nIf you need help with any of those, please open an issue where you directly ping the author(s), as indicated at the top of the README of each folder.", "Check if \ud83e\udd17 Transformers has been properly installed by running the following command:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"\n```\n\n## Editable install\n\nYou will need an editable install if you'd like to:\n\n* Use the `main` version of the source code.\n* Contribute to \ud83e\udd17 Transformers and need to test changes in the code.\n\nClone the repository and install \ud83e\udd17 Transformers with the following commands:\n\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python will also search the folder you cloned to: `~/transformers/`.\n\n<Tip warning={true}>", "Then switch your current clone of \ud83e\udd17 Transformers to a specific version, like v3.5.1 for example:\n\n```bash\ngit checkout tags/v3.5.1\n```\n\nAfter you've setup the correct library version, navigate to the example folder of your choice and install the example specific requirements:\n\n```bash\npip install -r requirements.txt\n```\n\n## Run a script\n\n<frameworkcontent>\n<pt>\nThe example script downloads and preprocesses a dataset from the \ud83e\udd17 [Datasets](https://huggingface.co/docs/datasets/) library. Then the script fine-tunes a dataset with the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) on an architecture that supports summarization. The following example shows how to fine-tune [T5-small](https://huggingface.co/t5-small) on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) dataset. The T5 model requires an additional `source_prefix` argument due to how it was trained. This prompt lets T5 know this is a summarization task.", "## How to fine-prune?\n\n### Setup\n\nThe code relies on the \ud83e\udd17 Transformers library. In addition to the dependencies listed in the [`examples`](https://github.com/huggingface/transformers/tree/main/examples) folder, you should install a few additional dependencies listed in the `requirements.txt` file: `pip install -r requirements.txt`.\n\nNote that we built our experiments on top of a stabilized version of the library (commit https://github.com/huggingface/transformers/commit/352d5472b0c1dec0f420d606d16747d851b4bda8): we do not guarantee that everything is still compatible with the latest version of the main branch.\n\n### Fine-pruning with movement pruning\n\nBelow, we detail how to reproduce the results reported in the paper. We use SQuAD as a running example. Commands (and scripts) can be easily adapted for other tasks.", "## Add new model like command\n\nUsing the `transformers-cli add-new-model-like` command requires to have all the `dev` dependencies installed. Let's\nfirst clone the repository and install it in our environment:\n\n```shell script\ngit clone https://github.com/huggingface/transformers\ncd transformers\npip install -e \".[dev]\"\n```\n\nDepending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\nfailure with this command. If that's the case make sure to install the Deep Learning framework you are working with\n(PyTorch, TensorFlow and/or Flax) then do:\n\n```bash\npip install -e \".[quality]\"\n```\n\nOnce the installation is done, you can use the CLI command `add-new-model-like` to generate your models:\n\n```shell script\ntransformers-cli add-new-model-like\n```\n\nThis will start a small questionnaire you have to fill.\n\n```\nWhat identifier would you like to use for the model type of this model?\n```", "To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice.\n\nIf you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder.", "```bash\npip install transformers\n```\n\n\u092f\u0926\u093f \u0906\u092a \u0909\u092a\u092f\u094b\u0917 \u0915\u0947 \u092e\u093e\u092e\u0932\u094b\u0902 \u0915\u094b \u0906\u091c\u093c\u092e\u093e\u0928\u093e \u091a\u093e\u0939\u0924\u0947 \u0939\u0948\u0902 \u092f\u093e \u0906\u0927\u093f\u0915\u093e\u0930\u093f\u0915 \u0930\u093f\u0932\u0940\u091c\u093c \u0938\u0947 \u092a\u0939\u0932\u0947 \u0928\u0935\u0940\u0928\u0924\u092e \u0907\u0928-\u0921\u0947\u0935\u0932\u092a\u092e\u0947\u0902\u091f \u0915\u094b\u0921 \u0915\u093e \u0909\u092a\u092f\u094b\u0917 \u0915\u0930\u0928\u093e \u091a\u093e\u0939\u0924\u0947 \u0939\u0948\u0902, \u0924\u094b \u0906\u092a\u0915\u094b [\u0938\u094b\u0930\u094d\u0938 \u0938\u0947 \u0907\u0902\u0938\u094d\u091f\u0949\u0932 \u0915\u0930\u0928\u093e \u0939\u094b\u0917\u093e](https://huggingface.co/docs/transformers/installation#installing-from-) \u0938\u094d\u0930\u094b\u0924\u0964\n\n### \u0915\u094b\u0902\u0921\u093e \u0915\u093e \u0909\u092a\u092f\u094b\u0917 \u0915\u0930\u0928\u093e\n\n\u091f\u094d\u0930\u093e\u0902\u0938\u092b\u0949\u0930\u094d\u092e\u0930 \u0938\u0902\u0938\u094d\u0915\u0930\u0923 4.0.0 \u0915\u0947 \u092c\u093e\u0926 \u0938\u0947, \u0939\u092e\u093e\u0930\u0947 \u092a\u093e\u0938 \u090f\u0915 \u0915\u094b\u0902\u0921\u093e \u091a\u0948\u0928\u0932 \u0939\u0948: `\u0939\u0917\u093f\u0902\u0917\u092b\u0947\u0938`\u0964\n\n\u091f\u094d\u0930\u093e\u0902\u0938\u092b\u0949\u0930\u094d\u092e\u0930 \u0915\u094b\u0902\u0921\u093e \u0915\u0947 \u092e\u093e\u0927\u094d\u092f\u092e \u0938\u0947 \u0928\u093f\u092e\u094d\u0928\u093e\u0928\u0941\u0938\u093e\u0930 \u0938\u094d\u0925\u093e\u092a\u093f\u0924 \u0915\u093f\u092f\u093e \u091c\u093e \u0938\u0915\u0924\u093e \u0939\u0948:\n\n```shell script\nconda install -c huggingface transformers\n```\n\n\u0915\u094b\u0902\u0921\u093e \u0915\u0947 \u092e\u093e\u0927\u094d\u092f\u092e \u0938\u0947 Flax, PyTorch, \u092f\u093e TensorFlow \u092e\u0947\u0902 \u0938\u0947 \u0915\u093f\u0938\u0940 \u090f\u0915 \u0915\u094b \u0938\u094d\u0925\u093e\u092a\u093f\u0924 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f, \u0928\u093f\u0930\u094d\u0926\u0947\u0936\u094b\u0902 \u0915\u0947 \u0932\u093f\u090f \u0909\u0928\u0915\u0947 \u0938\u0902\u092c\u0902\u0927\u093f\u0924 \u0938\u094d\u0925\u093e\u092a\u0928\u093e \u092a\u0943\u0937\u094d\u0920 \u0926\u0947\u0916\u0947\u0902\u0964\n\n## \u092e\u0949\u0921\u0932 \u0906\u0930\u094d\u0915\u093f\u091f\u0947\u0915\u094d\u091a\u0930\n[\u0909\u092a\u092f\u094b\u0917\u0915\u0930\u094d\u0924\u093e](https://huggingface.co/users) \u0914\u0930 [organization](https://huggingface.co) \u0926\u094d\u0935\u093e\u0930\u093e \u091f\u094d\u0930\u093e\u0902\u0938\u092b\u0949\u0930\u094d\u092e\u0930 \u0938\u092e\u0930\u094d\u0925\u093f\u0924 [**\u0938\u092d\u0940 \u092e\u0949\u0921\u0932 \u091a\u094c\u0915\u093f\u092f\u094b\u0902**](https://huggingface.co/models/users) \u0939\u0917\u093f\u0902\u0917\u092b\u0947\u0938.\u0915\u094b/\u0911\u0930\u094d\u0917\u0928\u093e\u0907\u091c\u0947\u0936\u0928), \u0938\u092d\u0940 \u0915\u094b \u092c\u093f\u0928\u093e \u0915\u093f\u0938\u0940 \u092c\u093e\u0927\u093e \u0915\u0947 \u0939\u0917\u093f\u0902\u0917\u092b\u0947\u0938.\u0915\u094b [\u092e\u0949\u0921\u0932 \u0939\u092c](https://huggingface.co) \u0915\u0947 \u0938\u093e\u0925 \u090f\u0915\u0940\u0915\u0943\u0924 \u0915\u093f\u092f\u093e \u0917\u092f\u093e \u0939\u0948\u0964"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer and includes additional useful information about running the command within the correct folder and contacting the authors if needed. This additional information does not detract from the correctness of the response."}, {"question": "What task does the `roberta-large-mnli` checkpoint perform?\n", "true_answer": "Text classification", "source_doc": "huggingface/course/blob/main/chapters/en/chapter1/10.mdx", "generated_answer": "The `roberta-large-mnli` checkpoint is for the task of natural language inference (NLI), specifically for the Multi-NLI (MNLI) dataset. It was pretrained on this task using the RoBERTa architecture.", "retrieved_docs": ["!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-chapter-quiz]]\n\n<CourseFloatingBanner\n    chapter={1}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThis chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood.\n\nFirst, though, let's test what you learned in this chapter!\n\n\n### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?", "There are total **3 checkpoints** available in **\ud83e\udd17Hub** (at the point of writing this article): [`bigbird-roberta-base`](https://huggingface.co/google/bigbird-roberta-base), [`bigbird-roberta-large`](https://huggingface.co/google/bigbird-roberta-large), [`bigbird-base-trivia-itc`](https://huggingface.co/google/bigbird-base-trivia-itc). The first two checkpoints come from pretraining `BigBirdForPretraining` with `masked_lm loss`; while the last one corresponds to the checkpoint after finetuning `BigBirdForQuestionAnswering` on `trivia-qa` dataset.\n\nLet's have a look at minimal code you can write (in case you like to use your PyTorch trainer), to use \ud83e\udd17's BigBird model for fine-tuning your tasks.\n\n```python\n# let's consider our task to be question-answering as an example\n\nfrom transformers import BigBirdForQuestionAnswering, BigBirdTokenizer\nimport torch\n\ndevice = torch.device(\"cpu\")\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")", "to implement BigBird.\n- The first goal should be to successfully run a forward pass using the RoBERTa checkpoint `bigbr_base/model.ckpt-0.data-00000-of-00001` and `bigbr_base/model.ckpt-0.index`.", "### Performance\n\nWe get the following results for [roberta-base](https://huggingface.co/roberta-base) and [roberta-large](https://huggingface.co/roberta-large)\nmixed precision training(fp16) on sst2 dataset under PyTorch and ONNX Runtime backends. A single Nvidia A100 card was used to run the\nexperiment for 3 epochs::\n\n| Model           | Backend      | Runtime(s) | Train samples(/s) |\n| --------------- |------------- | ---------- | ----------------- |\n| roberta-base    | PyTorch      | 752.3      | 268.6             |\n| roberta-base    | ONNX Runtime | 729.7      | 276.9             |\n| roberta-large   | PyTorch      | 3523.7     | 57.3              |\n| roberta-large   | ONNX Runtime | 2986.6     | 67.7              |\n\nWe observe the gain of ONNX Runtime compared to PyTorch as follow:\n\n| Model         | Latency | Throughput |\n| ------------- | ------- | ---------- |\n| roberta-base  | 2.99%   | 3.08%      |\n| roberta-large | 15.24%  | 17.98%     |", "Then, we set the training arguments:\n\n```python\nfrom transformers import TrainingArguments\n\nlr = 1e-4\nbatch_size = 8\nnum_epochs = 5\n\ntraining_args = TrainingArguments(\n    output_dir=\"roberta-large-lora-token-classification\",\n    learning_rate=lr,\n    lr_scheduler_type= \"constant\",\n    warmup_ratio= 0.1,\n    max_grad_norm= 0.3,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.001,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    report_to=\"wandb\",\n    fp16=False,\n    gradient_checkpointing=True,\n)\n```\n\nFinally, we define the RoBERTa trainer by providing the model, the training arguments and the tokenized datasets:", "At the time of writing, two Bark checkpoints are available, a [smaller](https://huggingface.co/suno/bark-small) and a [larger](https://huggingface.co/suno/bark) version.\n\n\n## Load the Model and its Processor\n\nThe pre-trained Bark small and large checkpoints can be loaded from the [pre-trained weights](https://huggingface.co/suno/bark) on the Hugging Face Hub. You can change the repo-id with the checkpoint size that you wish to use.\n\nWe'll default to the small checkpoint, to keep it fast. But you can try the large checkpoint by using `\"suno/bark\"` instead of `\"suno/bark-small\"`.\n\n```python\nfrom transformers import BarkModel\n\nmodel = BarkModel.from_pretrained(\"suno/bark-small\")\n```\n\nPlace the model to an accelerator device to get the most of the optimization techniques:\n\n```python\nimport torch\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\n```\n\nLoad the processor, which will take care of tokenization and optional speaker embeddings.", "Therefore you have two ways to take advantage of this very beneficial feature:\n\n1. If you want to use a HF Transformers models you can do `model.gradient_checkpointing_enable()` or use `--gradient_checkpointing` in the HF Trainer, which will automatically enable this for you. `torch.utils.checkpoint` is used there.\n2. If you write your own model and you want to use DeepSpeed's activation checkpointing you can use the [API prescribed there](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html). You can also take the HF Transformers modeling code and replace `torch.utils.checkpoint` with the DeepSpeed's API. The latter is more flexible since it allows you to offload the forward activations to the CPU memory instead of recalculating them.\n\n\n### Optimizer and Scheduler"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response accurately identifies the task associated with the `roberta-large-mnli` checkpoint as natural language inference (NLI) and correctly associates it with the Multi-NLI (MNLI) dataset. It also correctly mentions that it uses the RoBERTa architecture. However, the reference answer is \"Text classification,\" which is a broader category that NLI falls under. The response is more specific than the reference answer but is still factually correct as NLI is a form of text classification. Therefore, the response is mostly correct, accurate, and factual."}, {"question": "What service is replacing the Paid tier of the Inference API at Hugging Face?\n", "true_answer": "Inference Endpoints", "source_doc": "huggingface/blog/blob/main/pricing-update.md", "generated_answer": "The service replacing the Paid tier of the Inference API at Hugging Face is [Inference Endpoints](https://huggingface.co/inference-endpoints). The Inference API will still be available for free, but for faster and enterprise-grade inference, users are recommended to switch to Inference Endpoints. This new service allows users to deploy machine learning models directly from the Hugging Face hub to managed infrastructure on their preferred cloud in just a few clicks", "retrieved_docs": ["--\ntitle: Introducing our new pricing\nthumbnail: /blog/assets/114_pricing-update/thumbnail.png\nauthors:\n- user: sbrandeis\n- user: pierric\n---\n\n# Introducing our new pricing\n\n\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently.\n\nFirst of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints).", "## Demos with the Hugging Face Inference API\n\nHugging Face has a free service called the [Inference API](https://huggingface.co/inference-api), which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated [Inference Endpoints](https://huggingface.co/pricing) when you want to use it in production. Gradio integrates directly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name (e.g. `Helsinki-NLP/opus-mt-en-es`), like this:\n\n```python\nimport gradio as gr\n\ndemo = gr.load(\"Helsinki-NLP/opus-mt-en-es\", src=\"models\")\n\ndemo.launch()\n```\n\nFor any Hugging Face model supported in the Inference API, Gradio automatically infers the expected input and output and make the underlying server calls, so you don't have to worry about defining the prediction function.", "Hugging Face PRO users now have access to exclusive API endpoints for a curated list of powerful models that benefit from ultra-fast inference powered by [text-generation-inference](https://github.com/huggingface/text-generation-inference). This is a benefit on top of the free inference API, which is available to all Hugging Face users to facilitate testing and prototyping on 200,000+ models. PRO users enjoy higher rate limits on these models, as well as exclusive access to some of the best models available today.\n\n## Contents", "### Text Generation Inference\n\nResponse time and latency for concurrent users are a big challenge for serving these large models. To tackle this problem, Hugging Face has released [text-generation-inference](https://github.com/huggingface/text-generation-inference) (TGI), an open-source serving solution for large language models built on Rust, Python, and gRPc. TGI is integrated into inference solutions of Hugging Face, [Inference Endpoints](https://huggingface.co/inference-endpoints), and [Inference API](https://huggingface.co/inference-api), so you can directly create an endpoint with optimized inference with few clicks, or simply send a request to Hugging Face's Inference API to benefit from it, instead of integrating TGI to your platform. \n\n![Screenshot from HuggingChat](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/os_llms/huggingchat_ui.png)", "These services can be called with the [`InferenceClient`] object. It acts as a replacement for the legacy\n[`InferenceApi`] client, adding specific support for tasks and handling inference on both\n[Inference API](https://huggingface.co/docs/api-inference/index) and [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index).\nLearn how to migrate to the new client in the [Legacy InferenceAPI client](#legacy-inferenceapi-client) section.\n\n<Tip>\n\n[`InferenceClient`] is a Python client making HTTP calls to our APIs. If you want to make the HTTP calls directly using\nyour preferred tool (curl, postman,...), please refer to the [Inference API](https://huggingface.co/docs/api-inference/index)\nor to the [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) documentation pages.", "We strive to fix this problem with the newly launched Hugging Face [Inference Endpoints](https://huggingface.co/inference-endpoints). In the spirit of making machine learning ever simpler without compromising on state-of-the-art quality, we've built a service that lets you deploy machine learning models directly from the [Hugging Face hub](https://huggingface.co) to managed infrastructure on your favorite cloud in just a few clicks. Simple, secure, and scalable: you can have it all.\n\nLet me show you how this works!\n\n### Deploying a model on Inference Endpoints", "```yaml\ninference:\n  parameters:\n    temperature: 0.7\n``` \n\nThe Inference API allows you to send HTTP requests to models in the Hugging Face Hub, and it's 2x to 10x faster than the widgets! \u26a1\u26a1 Learn more about it by reading the [Inference API documentation](./models-inference). Finally, you can also deploy all those models to dedicated [Inference Endpoints](https://huggingface.co/docs/inference-endpoints)."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response accurately identifies \"Inference Endpoints\" as the service replacing the Paid tier of the Inference API at Hugging Face. The additional information provided about the availability of the Inference API for free and the recommendation for users to switch to Inference Endpoints for faster and enterprise-grade inference is correct and factual. The explanation that Inference Endpoints allows users to deploy machine learning models to managed infrastructure on their preferred cloud is also accurate. The response aligns with the reference answer and provides a correct and factual description of the new service."}, {"question": "What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n", "true_answer": "Grouped convolutions", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/squeezebert.md", "generated_answer": "SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers, as described in the paper \"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?\" by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer. This technique is also referred to as \"grouped self-attention\" in the paper.", "retrieved_docs": ["-->\n\n# SqueezeBERT\n\n## Overview\n\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers.\n\nThe abstract from the paper is the following:", "techniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\nset. The SqueezeBERT code will be released.*", "This model was contributed by [forresti](https://huggingface.co/forresti).\n\n## Usage tips\n\n- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\n  rather than the left.\n- SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\n  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\n  with a causal language modeling (CLM) objective are better in that regard.\n- For best results when finetuning on sequence classification tasks, it is recommended to start with the\n  *squeezebert/squeezebert-mnli-headless* checkpoint.\n\n## Resources", "1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\n1. **[SwiftFormer](https://huggingface.co/docs/transformers/model_doc/swiftformer)** (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.", "1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\n1. **[SwiftFormer](https://huggingface.co/docs/transformers/model_doc/swiftformer)** (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.", "<!--\nType: model-index\nCollections:\n- Name: Gloun SENet\n  Paper:\n    Title: Squeeze-and-Excitation Networks\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\nModels:\n- Name: gluon_senet154\n  In Collection: Gloun SENet\n  Metadata:\n    FLOPs: 26681705136\n    Parameters: 115090000\n    File Size: 461546622\n    Architecture:\n    - Convolution\n    - Dense Connections\n    - Global Average Pooling\n    - Max Pooling\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Data:\n    - ImageNet\n    ID: gluon_senet154\n    Crop Pct: '0.875'\n    Image Size: '224'\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/gluon_resnet.py#L239\n  Weights: https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_senet154-70a1a3c0.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:", "<!--\nType: model-index\nCollections:\n- Name: Gloun SENet\n  Paper:\n    Title: Squeeze-and-Excitation Networks\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\nModels:\n- Name: gluon_senet154\n  In Collection: Gloun SENet\n  Metadata:\n    FLOPs: 26681705136\n    Parameters: 115090000\n    File Size: 461546622\n    Architecture:\n    - Convolution\n    - Dense Connections\n    - Global Average Pooling\n    - Max Pooling\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Data:\n    - ImageNet\n    ID: gluon_senet154\n    Crop Pct: '0.875'\n    Image Size: '224'\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/gluon_resnet.py#L239\n  Weights: https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_senet154-70a1a3c0.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies that SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers. The additional information provided about the paper and the authors is accurate and relevant, although not necessary to answer the question. The term \"grouped self-attention\" is not directly related to the architectural feature in question but does not detract from the correctness of the response regarding the use of grouped convolutions. Based on the score rubric, the response is completely correct, accurate, and factual."}, {"question": "What type of license is the HuggingFace Team's software distributed under?\n", "true_answer": "Apache License, Version 2.0", "source_doc": "huggingface/simulate/blob/main/docs/source/tutorials/interaction.mdx", "generated_answer": "The HuggingFace Team's software is distributed under the Apache License, Version 2.0, with the following conditions: it may not be used except in compliance with the License, and software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. This information can be found in the copyright notices included in the extracted documents provided in the context.", "retrieved_docs": ["!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Overview", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Overview", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Overview", "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Overview", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# UNet1DModel", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# UNet2DModel", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ControlNet"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the license as the Apache License, Version 2.0, which is in agreement with the reference answer. The additional details provided about the conditions of the license are accurate and factual, although they are not required by the instruction. The response goes beyond the reference answer but does not contradict it, maintaining factual accuracy throughout."}, {"question": "What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n", "true_answer": "Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md", "generated_answer": "The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:\n\n1. Splitting the embedding matrix into two smaller matrices.\n2. Using repeating layers split among groups.\n\nThese techniques help to make the ALBERT model more parameter-efficient compared to BERT, with ALBERT-large having fewer parameters (~18M) and performing comparably to BERT-base (~108", "retrieved_docs": ["However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths", "The abstract from the paper is the following:\n\n*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.*", "-->\n\n# ALBERT\n\n<div class=\"flex flex-wrap space-x-1\">\n<a href=\"https://huggingface.co/models?filter=albert\">\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-albert-blueviolet\">\n</a>\n<a href=\"https://huggingface.co/spaces/docs-demos/albert-base-v2\">\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n</a>\n</div>\n\n## Overview\n\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\nRadu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n\n- Splitting the embedding matrix into two smaller matrices.\n- Using repeating layers split among groups.\n\nThe abstract from the paper is the following:", "We conducted this experiment with real volunteers from the Neuropark community and used openly available datasets (OSCAR and Wikipedia), because we wanted to have a fully reproducible example that might serve as an inspiration for other groups. Below, we describe the detailed setup of our training run and demonstrate its results.\n\n### Architecture\n\nFor our experiment, we chose ALBERT _(A Lite BERT)_ \u2014 a model for language representations that is pretrained with Masked Language Modeling (MLM) and Sentence Order Prediction (SOP) as objectives. We use this architecture because weight sharing makes it very parameter-efficient: for example, ALBERT-large has ~18M trainable parameters and performs comparably to BERT-base with ~108M weights on the GLUE benchmark. It means that there is less data to exchange between the peers, which is crucial in our setup, as it significantly speeds up each training iteration.", "So, if you reduce the precision, you reduce the memory each model parameter takes in storage, therefore reducing the model size! This also means that you reduce... the actual precision of the computations, which can reduce the model's performance. However, we found out that on bigger models, this performance degradation is actually very [limited](https://huggingface.co/blog/overview-quantization-transformers).\n\nTo go back to our above example, our 30B parameters model in `float16` requires a bit less than 66G of RAM, in `8bit` it only requires half that, so 33G of RAM, and it `4bit` we reach even half of this, so around 16G of RAM, making it considerably more accessible.", "## Training\n\nThe argument `placement` can be `cpu`, `auto`, `cuda`, with `cpu` the GPU RAM required can be minimized to 4GB but will deceleration, with `cuda` you can also reduce GPU memory by half but accelerated training\uff0c with `auto` a more balanced solution for speed and memory can be obtained\u3002\n\n**___Note: Change the `resolution` to 768 if you are using the [stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2) 768x768 model.___**\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"", "## Efficient training strategies\n\nEven training the smallest LLaMA model requires an enormous amount of memory. Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam optimizer (see the [performance docs](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer) in Transformers for more info). So a 7B parameter model would use `(2+8)*7B=70GB` just to fit in memory and would likely need more when you compute intermediate values such as attention scores. So you couldn\u2019t train the model even on a single 80GB A100 like that. You can use some tricks, like more efficient optimizers of half-precision training, to squeeze a bit more into memory, but you\u2019ll run out sooner or later.\n\nAnother option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques, such as the [`peft`](https://github.com/huggingface/peft) library, which can perform Low-Rank Adaptation (LoRA) on a model loaded in 8-bit."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided correctly identifies the two parameter-reduction techniques in the ALBERT model as splitting the embedding matrix into two smaller matrices and using repeating layers split among groups. This matches the reference answer and is accurate and factual. However, the response includes additional information about the parameter count of ALBERT-large and its comparison to BERT-base, which is not required by the instruction but does not detract from the correctness of the answer."}, {"question": "What are the three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library?\n", "true_answer": "1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter5/1.mdx", "generated_answer": "The three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library are:\n1. Load a dataset from the Hugging Face Hub using `Dataset.load_from_hub()`.\n2. Preprocess the data with `Dataset.map()` using the processor associated with the model being fine-tuned.\n3. Load and compute metrics using `Dataset.map()` and the `MetricForSequenceClassification` class from the", "retrieved_docs": ["Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={5}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nIn [Chapter 3](/course/chapter3) you got your first taste of the \ud83e\udd17 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n\n1. Load a dataset from the Hugging Face Hub.\n2. Preprocess the data with `Dataset.map()`.\n3. Load and compute metrics.\n\nBut this is just scratching the surface of what \ud83e\udd17 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\n* What do you do when your dataset is not on the Hub?\n* How can you slice and dice a dataset? (And what if you _really_ need to use Pandas?)\n* What do you do when your dataset is huge and will melt your laptop's RAM?\n* What the heck are \"memory mapping\" and Apache Arrow?\n* How can you create your own dataset and push it to the Hub?", "This concludes the introduction to fine-tuning using the `Trainer` API. An example of doing this for most common NLP tasks will be given in [Chapter 7](/course/chapter7), but for now let's look at how to do the same thing in pure PyTorch.\n\n<Tip>\n\n\u270f\ufe0f **Try it out!** Fine-tune a model on the GLUE SST-2 dataset, using the data processing you did in section 2.\n\n</Tip>", "With the dataset ready, you can now set up the model for fine-tuning. \n\n## Load a base model\n\nLoad the [\"microsoft/git-base\"](https://huggingface.co/microsoft/git-base) into a [`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM) object.\n\n\n```python\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\n```\n\n## Evaluate\n\nImage captioning models are typically evaluated with the [Rouge Score](https://huggingface.co/spaces/evaluate-metric/rouge) or [Word Error Rate](https://huggingface.co/spaces/evaluate-metric/wer). For this guide, you will use the Word Error Rate (WER). \n\nWe use the \ud83e\udd17 Evaluate library to do so. For potential limitations and other gotchas of the WER, refer to [this guide](https://huggingface.co/spaces/evaluate-metric/wer). \n\n\n```python\nfrom evaluate import load\nimport torch\n\nwer = load(\"wer\")", "FrameworkSwitchCourse {fw} />\n\n# Fine-tuning, Check![[fine-tuning-check]]\n\n<CourseFloatingBanner\n    chapter={3}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThat was fun! In the first two chapters you learned about models and tokenizers, and now you know how to fine-tune them for your own data. To recap, in this chapter you:\n\n{#if fw === 'pt'}\n* Learned about datasets in the [Hub](https://huggingface.co/datasets)\n* Learned how to load and preprocess datasets, including using dynamic padding and collators\n* Implemented your own fine-tuning and evaluation of a model\n* Implemented a lower-level training loop\n* Used \ud83e\udd17 Accelerate to easily adapt your training loop so it works for multiple GPUs or TPUs\n\n{:else}\n* Learned about datasets in the [Hub](https://huggingface.co/datasets)\n* Learned how to load and preprocess datasets\n* Learned how to fine-tune and evaluate a model with Keras\n* Implemented a custom metric\n\n{/if}", "## Preprocess the dataset\n\nSince the dataset has two modalities (image and text), the pre-processing pipeline will preprocess images and the captions.\n\nTo do so, load the processor class associated with the model you are about to fine-tune. \n\n```python\nfrom transformers import AutoProcessor\n\ncheckpoint = \"microsoft/git-base\"\nprocessor = AutoProcessor.from_pretrained(checkpoint)\n```\n\nThe processor will internally pre-process the image (which includes resizing, and pixel scaling) and tokenize the caption. \n\n```python\ndef transforms(example_batch):\n    images = [x for x in example_batch[\"image\"]]\n    captions = [x for x in example_batch[\"text\"]]\n    inputs = processor(images=images, text=captions, padding=\"max_length\")\n    inputs.update({\"labels\": inputs[\"input_ids\"]})\n    return inputs\n\n\ntrain_ds.set_transform(transforms)\ntest_ds.set_transform(transforms)\n```\n\nWith the dataset ready, you can now set up the model for fine-tuning. \n\n## Load a base model", "### Supervised Fine Tuning\n\nThe process as introduced above involves the supervised fine-tuning step using [QLoRA](https://arxiv.org/abs/2305.14314) on the 7B Llama v2 model on the SFT split of the data via TRL\u2019s `SFTTrainer`:\n\n```python\n# load the base model in 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    script_args.model_name,        # \"meta-llama/Llama-2-7b-hf\"\n    quantization_config=bnb_config,\n    device_map={\"\": 0},\n    trust_remote_code=True,\n    use_auth_token=True,\n)\nbase_model.config.use_cache = False", "| [How to fine-tune a model on language modeling](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a causal or masked LM task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)|"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It correctly identifies the three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library, which are loading a dataset, preprocessing the data, and loading and computing metrics. However, the response includes a specific method `Dataset.load_from_hub()` which is not necessary for the general process and is not mentioned in the reference answer. Additionally, the response specifies the use of a `MetricForSequenceClassification` class, which is not required for all fine-tuning scenarios and is not included in the reference answer. The response also ends abruptly, indicating that it may be incomplete. Despite these issues, the core elements of the fine-tuning process are present."}, {"question": "What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n", "true_answer": "+800%", "source_doc": "huggingface/blog/blob/main/infinity-cpu-performance.md", "generated_answer": "According to the conclusion provided in the context, Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers.", "retrieved_docs": ["---\n\n## Conclusion\n\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput.\n\nHugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.\n\nThe flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. \n\nIf you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) \n\n\n## Resources", "Leveraging the latest features of the Hugging Face libraries, we achieve a reliable 10x speed up compared to an out-of-box deployment for a given model/hardware pair. As new releases of Transformers and Tokenizers typically ship every month, our API customers do not need to constantly adapt to new optimization opportunities, their models just keep running faster.\n\n## Compilation FTW: the hard to get 10x\nNow this is where it gets really tricky. In order to get the best possible performance we will need to modify the model and compile it targeting the specific hardware for inference. The choice of hardware itself will depend on both the model (size in memory) and the demand profile (request batching). Even when serving predictions from the same model, some API customers may benefit more from Accelerated CPU inference, and others from Accelerated GPU inference, each with different optimization techniques and libraries applied.", "The main bottleneck is the latency of predictions which can make large deployments expensive to run and real-time use cases impractical. Solving this is a difficult engineering challenge for any Machine Learning Engineering team and requires the use of advanced techniques to optimize models all the way down to the hardware.\n\nWith [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\n\n\n## What is Hugging Face Infinity", "\u201c*We\u2019re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.*\u201d, says Wei Li, Intel Vice President & General Manager, AI and Analytics.\n\nIn recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs.", "At Hugging Face, we are on a mission to democratize state-of-the-art Machine Learning, and a critical part of our work is to make these state-of-the-art models as efficient as possible, to use less energy and memory at scale, and to be more affordable to run by companies of all sizes. \n\nOur collaboration with Intel through the \ud83e\udd17 [Hardware Partner Program](https://huggingface.co/hardware) enables us to make advanced efficiency and optimization techniques easily available to the community, through our new \ud83e\udd17 [Optimum open source library](https://github.com/huggingface/optimum) dedicated to production performance.\n\nFor companies looking to accelerate their Transformer models inference, our new \ud83e\udd17 [Infinity product offers a plug-and-play containerized solution](https://huggingface.co/infinity), achieving down to 1ms latency on GPU and 2ms on Intel Xeon Ice Lake CPUs.", "The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Let\u2019s take a deeper look at it.\n\n## Get Peak Transformers Performance with Optimum Intel\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices. Thanks to built-in optimization techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep tweaking for maximum performance.", "After completing the training of BLOOM-176B, we at HuggingFace and BigScience were looking for ways to make this big model easier to run on less GPUs. Through our BigScience community we were made aware of research on Int8 inference that does not degrade predictive performance of large models and reduces the memory footprint of large models by a factor or 2x. Soon we started collaboring on this research which ended with a full integration into Hugging Face `transformers`. With this blog post, we offer LLM.int8() integration for all Hugging Face models which we explain in more detail below. If you want to read more about our research, you can read our paper, [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339).\n\nThis article focuses on giving a high-level overview of this quantization technology, outlining the difficulties in incorporating it into the `transformers` library, and drawing up the long-term goals of this partnership."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided matches the reference answer in terms of the percentage improvement in throughput. The respondent has correctly identified the maximum improvement as 800%, which is in line with the reference answer that states \"+800%.\" Therefore, the response is completely correct, accurate, and factual."}, {"question": "What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n", "true_answer": "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl", "source_doc": "huggingface/blog/blob/main/spacy.md", "generated_answer": "The command to upload a spaCy pipeline to the Hugging Face Hub is `python -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]`. This command is part of the `spacy-huggingface-hub` library, which can be installed with `pip install spacy-huggingface-hub`. Before running this command, the user is", "retrieved_docs": ["Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\n\n### Finding models\n\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.\n\n\n### Widgets", "All spaCy canonical pipelines can now be found in the official spaCy [organization](https://huggingface.co/spacy), and any user can share their pipelines with a single command `python -m spacy huggingface-hub`. To read more about it, head to [https://huggingface.co/blog/spacy](https://huggingface.co/blog/spacy). You can try all canonical spaCy models directly in the Hub in the demo [Space](https://huggingface.co/spaces/spacy/pipeline-visualizer)!\n\n![spaCy NER example](assets/27_summer_at_huggingface/spacy_ner.jpeg)\n\nAnother exciting integration is Sentence Transformers. You can read more about it in the [blog announcement](https://huggingface.co/blog/sentence-transformers-in-the-hub): you can find over 200 [models](https://huggingface.co/models?library=sentence-transformers) in the Hub, easily share your models with the rest of the community and reuse models from the community.", "You can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`.\n\n```bash\nhuggingface-cli login\npython -m spacy package ./en_ner_fashion ./output --build wheel\ncd ./output/en_ner_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n```\n\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\n\nThe command will output two things:\n\n* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en_core_web_sm\n* And how to install the pipeline directly from the Hub!\n\n\n### From a Python script", "## Sharing your models\n\n### Using the spaCy CLI (recommended)\n\nThe `spacy-huggingface-hub` library extends `spaCy` native CLI so people can easily push their packaged models to the Hub.\n\nYou can install spacy-huggingface-hub from pip:\n\n```bash\npip install spacy-huggingface-hub\n```\n\nYou can then check if the command has been registered successfully\n\n```bash\npython -m spacy huggingface-hub --help\n```\n\nTo push with the CLI, you can use the `huggingface-hub push` command as seen below.\n\n```bash\npython -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\n```", "There is only one commandline flag argument that is required of the user to get things up and running. Additionally, the user is expected to have a [huggingface user access token](https://huggingface.co/docs/hub/security-tokens) that will be used to upload the model post finetuning to HuggingFace hub.\n\nThe following bash command gets things running:\n\n```python\npython stable_diffusion_tuning.py --hf_user_access_token <token>\n```\n\nThe following table contains key hyperparameters that are directly correlated with positive results:", "And for larger-scale use cases, you can click \"Deploy > Accelerated Inference\" and see how to do this with Python.\n\n\n### Sharing your models\n\nBut probably the coolest feature is that now you can very easily share your models with the `spacy-huggingface-hub` [library](https://github.com/explosion/spacy-huggingface-hub), which extends the `spaCy` CLI with a new command, `huggingface-hub push`. \n\n```bash\nhuggingface-cli login\npython -m spacy package ./en_ner_fashion ./output --build wheel\ncd ./output/en_ner_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n```\n\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\n\nTry it out and share your models with the community!\n\n## Would you like to integrate your library to the Hub?", "he Push to Hub API. Let's have a look at the push_to_hub API. You will need to be logged in with your Hugging Face account, which you can do by executing this first cell or typing huggingface-cli login in a terminal. Just enter your username and password and click login, which will store an authentication token in the cache of the machine you're using. Now, let's launch the fine-tuning of a BERT model on the GLUE COLA dataset. We won't go over the fine-tuning code because you can find it in any Transformers tutorial, or by looking at the videos linked below. What interests us here, is how we can leverage the Model Hub during training. This is done with the push_to_hub=True passed in your TrainingArguments . This will automatically upload your model to the Hub each time it is saved (so every epoch in our case), which allows you to resume training from a different machine if the current one gets interrupted"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provides a command that is structurally similar to the reference answer but includes placeholders and additional flags that were not specified in the reference. While the response does not provide a specific example of a .whl file, which is present in the reference answer, it does correctly identify the command structure and the associated library needed for the task. The response is mostly correct but lacks the specificity of the reference answer by not completing the command with an actual file path. Based on the rubric, the response is mostly correct, accurate, and factual, but not completely as it includes additional, unnecessary options and does not provide a concrete example."}, {"question": "What is the time and memory complexity of the Nystr\u00f6mformer's approximation of self-attention?\n", "true_answer": "O(n)", "source_doc": "huggingface/blog/blob/main/nystromformer.md", "generated_answer": "The Nystr\u00f6mformer's approximation of self-attention has a time and memory complexity of O(n) instead of the standard self-attention's O(n^2) complexity. This is achieved by using the Nystr\u00f6m method for matrix approximation, which allows for the approximation of a matrix by sampling some of its rows and columns. In the case of the Nystr\u00f6mformer, the softmax matrix in standard self-attention is approximated by", "retrieved_docs": ["--\ntitle: \"Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\"\nthumbnail: /blog/assets/86_nystromformer/thumbnail.png\nauthors:\n- user: asi\n  guest: true\n---\n\n# Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n## Introduction\n\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences.", "The [Nystr\u00f6mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nystr\u00f6mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nystr\u00f6m method and how it can be adapted to approximate self-attention.\n\n\n## Nystr\u00f6m method for matrix approximation\n\nAt the heart of Nystr\u00f6mformer is the Nystr\u00f6m method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let's consider a matrix \\\\(P^{n \\times n}\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nystr\u00f6m method. We start by sampling \\\\(m\\\\) rows and columns from \\\\(P\\\\). We can then arrange the sampled rows and columns as follows:", "The four papers above offer different ways to deal with the quadratic memory complexity of the self-attention mechanism, usually by reducing it to linear complexity. Linformer and Longformer both rely on the observation that the self-attention matrix does not contain \\\\(n \u00d7 n\\\\) worth of information (the attention matrix is low-rank and sparse). Performer gives a principled method to approximate the softmax-attention kernel (and any kernelizable attention mechanisms beyond softmax). Compressive Transformer offers an orthogonal approach to model long range dependencies based on recurrence.", "</div>\n\n## Conclusion\n\nNystr\u00f6mformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other linear self-attention schemes. In this blog post, we went over a high-level overview of the Nystr\u00f6m method and how it can be leveraged for self-attention. Readers interested in deploying or fine-tuning Nystr\u00f6mformer for downstream tasks can find the HuggingFace documentation [here](https://huggingface.co/docs/transformers/model_doc/nystromformer).", "(n * h)\t            (n * n)   (n * h)\n\nLinformer:\n\n$$\\text{LinAttention}(Q, K, V) = \\text{softmax}(Q * K * W^K) * W^V * V$$\n\n                  (n * h)\t            (n * d)   (d * n)   (n * h)\n\n#### Main findings\n\n* The self-attention matrix is low-rank which implies that most of its information can be recovered by its first few highest eigenvalues and can be approximated by a low-rank matrix.\n* Lot of works focus on reducing the dimensionality of the hidden states. This paper shows that reducing the sequence length with learned projections can be a strong alternative while shrinking the memory complexity of the self-attention from quadratic to linear.\n* Increasing the sequence length doesn\u2019t affect the inference speed (time-clock) of Linformer, when transformers have a linear increase. Moreover, the convergence speed (number of updates) is not impacted by Linformer's self-attention.", "### [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768)\n\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma\n\nThe goal is to reduce the complexity of the self-attention with respect to the sequence length \\\\(n\\\\)) from quadratic to linear. This paper makes the observation that the attention matrices are low rank (i.e. they don\u2019t contain \\\\(n \u00d7 n\\\\) worth of information) and explores the possibility of using high-dimensional data compression techniques to build more memory efficient transformers.", "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Nystr\u00f6m approximation of P\" src=\"assets/86_nystromformer/p_hat.png\"></medium-zoom>\n  <figcaption>Nystr\u00f6m approximation of P</figcaption>\n</figure>\n\nAs shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices. The reason for doing so will become clear later.\n\n\n## Can we approximate self-attention with the Nystr\u00f6m method?\n\nOur goal is to ultimately approximate the softmax matrix in standard self attention: S = softmax \\\\( \\frac{QK^T}{\\sqrt{d}} \\\\)\n\nHere, \\\\(Q\\\\) and \\\\(K\\\\) denote the queries and keys respectively. Following the procedure discussed above, we would sample \\\\(m\\\\) rows and columns from \\\\(S\\\\), form four submatrices, and obtain \\\\(\\hat{S}\\\\):"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies that the Nystr\u00f6mformer's approximation of self-attention has a reduced complexity compared to the standard self-attention's O(n^2) complexity. However, the response fails to specify the exact time and memory complexity of the Nystr\u00f6mformer, which should be O(n). The explanation about the Nystr\u00f6m method for matrix approximation is accurate but incomplete as it does not finish the explanation of how the softmax matrix is approximated. Based on the score rubric, the response is mostly correct, as it correctly states the improvement in complexity but does not provide the exact complexity as per the reference answer."}, {"question": "What is the goal of the Named Entity Recognition task in token classification?\n", "true_answer": "The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/tasks.md", "generated_answer": "The goal of the Named Entity Recognition task in token classification is to find the entities (such as persons, locations, or organizations) in a sentence and assign a label to each token accordingly. This can be formulated as having one class per entity and one class for \"no entity.\" The task is commonly used in biomedical settings to label genes, proteins, and drug names, and can also be used to extract important information from invoices, such as dates, organization", "retrieved_docs": ["Welcome to the Hugging Face tasks series! In this video we\u2019ll take a look at the token classification task.\nToken classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.\nLet\u2019s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.\nAnother token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech.", "To evaluate the performance of sahajBERT, we finetuned it on two downstream tasks in Bengali:\n\n-   Named entity recognition (NER) on the Bengali split of [WikiANN](https://aclanthology.org/P17-1178/). The goal of this task is to classify each token in the input text into one of the following categories: person, organization, location, or none of them.\n-   News Category Classification (NCC) on the Soham articles dataset from [IndicGLUE](https://aclanthology.org/2020.findings-emnlp.445/). The goal of this task is to predict the category to which belong the input text.\n\nWe evaluated it during training on the NER task to check that everything was going well; as you can see on the following plot, this was indeed the case!", "Named-Entity Recognition\n\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert_ner_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial_Analyst_AI\nTags: NER, TEXT, HIGHLIGHT\n\n## Introduction\n\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \"token\") into different categories, such as names of people or names of locations, or different parts of speech.\n\nFor example, given the sentence:\n\n> Does Chicago have any Pakistani restaurants?\n\nA named-entity recognition algorithm may identify:\n\n- \"Chicago\" as a **location**\n- \"Pakistani\" as an **ethnicity**\n\nand so on.\n\nUsing `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team.\n\nHere is an example of a demo that you'll be able to build:\n\n$demo_ner_pipeline", "-->\n\n# Token classification\n\n[[open-in-colab]]\n\n<Youtube id=\"wVHdVlPScxA\"/>\n\nToken classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization.\n\nThis guide will show you how to:\n\n1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the [WNUT 17](https://huggingface.co/datasets/wnut_17) dataset to detect new entities.\n2. Use your finetuned model for inference.\n\n<Tip>\nThe task illustrated in this tutorial is supported by the following model architectures:\n\n<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->", "{/if}\n\nThe first application we'll explore is token classification. This generic task encompasses any problem that can be formulated as \"attributing a label to each token in a sentence,\" such as:\n\n- **Named entity recognition (NER)**: Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for \"no entity.\"\n- **Part-of-speech tagging (POS)**: Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.).\n- **Chunking**: Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually `B-`) to any tokens that are at the beginning of a chunk, another label (usually `I-`) to tokens that are inside a chunk, and a third label (usually `O`) to tokens that don't belong to any chunk.\n\n<Youtube id=\"wVHdVlPScxA\"/>", "### Token classification\n\nIn any NLP task, text is preprocessed by separating the sequence of text into individual words or subwords. These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. \n\nTwo common types of token classification are:\n\n* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names.\n* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb).\n\n```py\n>>> from transformers import pipeline", "Token classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score. Then we calculate it for every class and take the overall average to evaluate our model.\nAn example dataset used for this task is ConLL2003. Here, each token belongs to a certain named entity class, denoted as the indices of the list containing the labels.\nYou can extract important information from invoices using named entity recognition models, such as date, organization name or address.\nFor more information about the Token classification task, check out the Hugging Face course."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It accurately identifies the goal of Named Entity Recognition (NER) as being to find and label entities such as persons, locations, or organizations within a sentence. The response goes beyond the reference answer by giving examples of where NER can be applied, such as in biomedical settings and for extracting information from invoices. However, the response does not mention the aspect of assigning a label to each token, which is a part of the NER task, but this is a minor omission and does not significantly detract from the overall accuracy of the response."}, {"question": "What is the resolution of images used by the CLIPSeg model?\n", "true_answer": "352 x 352 pixels", "source_doc": "huggingface/blog/blob/main/clipseg-zero-shot.md", "generated_answer": "The resolution of images used by the CLIPSeg model is 352 x 352 pixels, as stated in the provided context. (Source: Extracted document 0)", "retrieved_docs": ["Currently, CLIPSeg still has its limitations. For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg). Before we describe how to do that, let\u2019s first take a look at how CLIPSeg works.\n\n## CLIP: the magic model behind CLIPSeg", "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png\"\nalt=\"drawing\" width=\"600\"/> \n\n<small> CLIPSeg overview. Taken from the <a href=\"https://arxiv.org/abs/2112.10003\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/timojl/clipseg).\n\n## Usage tips\n\n- [`CLIPSegForImageSegmentation`] adds a decoder on top of [`CLIPSegModel`]. The latter is identical to [`CLIPModel`].\n- [`CLIPSegForImageSegmentation`] can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text\n(provided to the model as `input_ids`) or an image (provided to the model as `conditional_pixel_values`). One can also provide custom\nconditional embeddings (provided to the model as `conditional_embeddings`).\n\n## Resources", "### Training\n\n#### Dataset\n\nWe fine-tuned the CLIP model primarily with the [RSICD dataset](https://github.com/201528014227051/RSICD_optimal). This dataset consists of about 10,000 images collected from Google Earth, Baidu Map, MapABC, and Tianditu. It is provided freely to the research community to advance remote sensing captioning via [Exploring Models and Data for Remote Sensing Image Caption Generation](https://arxiv.org/abs/1712.0783) (Lu et al, 2017). The images are (224, 224) RGB images at various resolutions, and each image has up to 5 captions associated with it.\n\n<img src=\"/blog/assets/30_clip_rsicd/rsicd-images-sampling.png\"/>\n<center><i>Some examples of images from the RSICD dataset</i></center>", "## CLIPSeg: image segmentation with CLIP\n\n[CLIPSeg](https://arxiv.org/abs/2112.10003) is a model that uses CLIP representations to create image segmentation masks. It was published by Timo L\u00fcddecke and Alexander Ecker. They achieved zero-shot image segmentation by training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesn\u2019t only use the final CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of CLIP.", "-->\n\n# CLIPSeg\n\n## Overview\n\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo L\u00fcddecke\nand Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero- and one-shot image segmentation.\n\nThe abstract from the paper is the following:", "**This guide shows how you can use [CLIPSeg](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg), a zero-shot image segmentation model, using [`\ud83e\udd17 transformers`](https://huggingface.co/transformers). CLIPSeg creates rough segmentation masks that can be used for robot perception, image inpainting, and many other tasks. If you need more precise segmentation masks, we\u2019ll show how you can refine the results of CLIPSeg on [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg).**", "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clipseg-overview.png\"></medium-zoom>\n  <figcaption><a href=\"https://arxiv.org/abs/2112.10003\">Source</a></figcaption>\n</figure>\n\nThe decoder is trained on the [PhraseCut dataset](https://arxiv.org/abs/2008.01187), which contains over 340,000 phrases with corresponding image segmentation masks. The authors also experimented with various augmentations to expand the size of the dataset. The goal here is not only to be able to segment the categories that are present in the dataset, but also to segment unseen categories. Experiments indeed show that the decoder can generalize to unseen categories."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual as it matches the reference answer exactly."}, {"question": "What can you use Gradio for?\n", "true_answer": "Create a demo for your machine learning model, share your machine learning model with others, and debug your model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/9.mdx", "generated_answer": "Gradio is an open-source Python package that allows you to easily build and share demos or web applications for your machine learning models, APIs, or any Python function. It requires Python 3.8 or higher and can be installed using pip. Gradio is not limited to NLP models, as it supports pretty much any data type, although it does supply developers with a library of pre-built components for various data types. Gradio's privacy and security features enhance user data", "retrieved_docs": ["`gradio_client`: Use a Gradio app as an API -- in 3 lines of Python\n\nThis directory contains the source code for `gradio_client`, a lightweight Python library that makes it very easy to use any Gradio app as an API.\n\nAs an example, consider this [Hugging Face Space that transcribes audio files](https://huggingface.co/spaces/abidlabs/whisper) that are recorded from the microphone.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/whisper-screenshot.jpg)\n\nUsing the `gradio_client` library, we can easily use the Gradio as an API to transcribe audio files programmatically.\n\nHere's the entire code to do it:\n\n```python\nfrom gradio_client import Client\n\nclient = Client(\"abidlabs/whisper\")\nclient.predict(\"audio_sample.wav\")\n\n>> \"This is a test of the whisper speech recognition model.\"\n```\n\nThe Gradio client works with any Gradio Space, whether it be an image generator, a stateful chatbot, or a tax calculator.\n\n## Installation", ">> \"This is a test of the whisper speech recognition model.\"\n```\n\nThe Gradio client works with any hosted Gradio app, whether it be an image generator, a text summarizer, a stateful chatbot, a tax calculator, or anything else! The Gradio Client is mostly used with apps hosted on [Hugging Face Spaces](https://hf.space), but your app can be hosted anywhere, such as your own server.\n\n**Prerequisites**: To use the Gradio client, you do _not_ need to know the `gradio` library in great detail. However, it is helpful to have general familiarity with Gradio's concepts of input and output components.\n\n## Installation\n\nIf you already have a recent version of `gradio`, then the `gradio_client` is included as a dependency.\n\nOtherwise, the lightweight `gradio_client` package can be installed from pip (or pip3) and is tested to work with Python versions 3.9 or higher:\n\n```bash\n$ pip install gradio_client\n```\n\n## Connecting to a running Gradio App", "Quickstart\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/lcm-screenshot-3.gif\" style=\"padding-bottom: 10px\">\n\nIt just takes a few lines of Python to create a beautiful demo like the one above, so let's get started \ud83d\udcab\n\n## Installation\n\n**Prerequisite**: Gradio requires [Python 3.8 or higher](https://www.python.org/downloads/)\n\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n```bash\npip install gradio\n```", "### 4. Gradio is designed primarily for NLP models\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: \"True\",\n\t\t\texplain: \"Gradio works with pretty much any data type, not just NLP.\"\n        },\n        {\n\t\t\ttext: \"False\",\n\t\t\texplain: \"Gradio supplies developers with a library of pre-built components for pretty much all data types.\",\n            correct: true\n        }\n\t]}\n/>\n\n### 5. Which of the following features are supported by Gradio?", "## Advanced Usage\n\nFor more ways to use the Gradio Python Client, check out our dedicated Guide on the Python client, available here: https://www.gradio.app/guides/getting-started-with-the-python-client", "### What's Next?\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features).\n\nOr, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/).\n\n\n\n\n## Questions?\n\nIf you'd like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help.\n\nIf you like Gradio, please leave us a \u2b50 on GitHub!\n\n## Open Source Stack\n\nGradio is built on top of many wonderful open-source libraries!", "### 3. Privacy and Security\nSince all processing occurs within the user's browser, `@gradio/lite` enhances privacy and security. User data remains on their device, providing peace of mind regarding data handling.\n\n### Limitations\n\n* Currently, the biggest limitation in using `@gradio/lite` is that your Gradio apps will generally take more time (usually 5-15 seconds) to load initially in the browser. This is because the browser needs to load the Pyodide runtime before it can render Python code. \n\n* Not every Python package is supported by Pyodide. While `gradio` and many other popular packages (including `numpy`, `scikit-learn`, and `transformers-js`) can be installed in Pyodide, if your app has many dependencies, its worth checking whether whether the dependencies are included in Pyodide, or can be [installed with `micropip`](https://micropip.pyodide.org/en/v0.2.2/project/api.html#micropip.install).\n\n## Try it out!"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct, accurate, and factual. It gives a comprehensive explanation of what Gradio is used for, including building and sharing demos or web applications for machine learning models, APIs, or any Python function. It also correctly states that Gradio supports various data types and mentions the privacy and security features, which, while not explicitly stated in the reference answer, are relevant to the use of Gradio. The response, however, does not directly mention debugging models, which is part of the reference answer. Therefore, it is not completely aligned with the reference answer but still provides a substantial amount of correct information."}, {"question": "What TensorFlow API function is used to load a saved tensor file?\n", "true_answer": "safetensors.tensorflow.load_file", "source_doc": "huggingface/safetensors/blob/main/docs/source/api/tensorflow.mdx", "generated_answer": "The TensorFlow API function used to load a saved tensor file is `tf.keras.models.load_model()` or `tf.saved_model.load()` from the `safetensors` package. However, it is recommended to save the model weights as a `h5` file extension with `model.save_weights()` and then reload the model with `TFPreTrainedModel.from_pretrained()` to avoid issues with saving and", "retrieved_docs": ["Tensorflow API\n\n[[autodoc]] safetensors.tensorflow.load_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save_file\n[[autodoc]] safetensors.tensorflow.save", "</Tip>\n\n## Unable to load a saved TensorFlow model\n\nTensorFlow's [model.save](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) method will save the entire model - architecture, weights, training configuration - in a single file. However, when you load the model file again, you may run into an error because \ud83e\udd17 Transformers may not load all the TensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we recommend you:\n\n- Save the model weights as a `h5` file extension with [`model.save_weights`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) and then reload the model with [`~TFPreTrainedModel.from_pretrained`]:\n\n```py\n>>> from transformers import TFPreTrainedModel\n>>> from tensorflow import keras\n\n>>> model.save_weights(\"some_folder/tf_model.h5\")\n>>> model = TFPreTrainedModel.from_pretrained(\"some_folder\")\n```", "# return the formated output\n        return self.serving_output(output)\n\n# Instantiate the model with the new serving method\nmodel = MyOwnModel.from_pretrained(\"bert-base-cased\")\n# save it with saved_model=True in order to have a SavedModel version along with the h5 weights.\nmodel.save_pretrained(\"my_model\", saved_model=True)\n```\n\nThe serving method has to be overridden by the new `input_signature` argument of the `tf.function` decorator. See the [official documentation](https://www.tensorflow.org/api_docs/python/tf/function#args_1) to know more about the `input_signature` argument. The `serving` method is used to define how will behave a SavedModel when deployed with TensorFlow Serving. Now the SavedModel looks like as expected, see the new `inputs_embeds` input:", "Thankfully, TensorFlow provides seamless support for reading files from a GCS bucket:\n\n```python\ntraining_records = tf.io.gfile.glob(os.path.join(args.train_dataset, \"*.tfrecord\"))\n```\n\nIf `args.dataset` contains the `gs://` identifier, TensorFlow will understand that it needs to look into a GCS bucket. Loading locally is as easy as removing the `gs://` identifier. For the rest of the data pipeline-related code, you can refer to [this section](https://github.com/huggingface/transformers/blob/474bf508dfe0d46fc38585a1bb793e5ba74fddfd/examples/tensorflow/language-modeling-tpu/run_mlm.py#L186-#L201) in the training script.", "### Step 1\n\nCreate a SavedModel. To create a SavedModel, the Transformers library lets you load a PyTorch model called `nateraw/bert-base-uncased-imdb` trained on the IMDB dataset and convert it to a TensorFlow Keras model for you:\n\n```python\nfrom transformers import TFBertForSequenceClassification\n\nmodel = TFBertForSequenceClassification.from_pretrained(\"nateraw/bert-base-uncased-imdb\", from_pt=True)\n# the saved_model parameter is a flag to create a SavedModel version of the model in same time than the h5 weights\nmodel.save_pretrained(\"my_model\", saved_model=True)\n```\n\n### Step 2\n\nCreate a Docker container with the SavedModel and run it. First, pull the TensorFlow Serving Docker image for CPU (for GPU replace serving by serving:latest-gpu):\n``` \ndocker pull tensorflow/serving\n```\n\nNext, run a serving image as a daemon named serving_base:\n```\ndocker run -d --name serving_base tensorflow/serving\n```", "- Safe: Can I use a file randomly downloaded and expect not to run arbitrary code ?\n- Zero-copy: Does reading the file require more memory than the original file ?\n- Lazy loading: Can I inspect the file without loading everything ? And loading only\nsome tensors in it without scanning the whole file (distributed setting) ?\n- Layout control: Lazy loading, is not necessarily enough since if the information about tensors is spread out in your file, then even if the information is lazily accessible you might have to access most of your file to read the available tensors (incurring many DISK -> RAM copies). Controlling the layout to keep fast access to single tensors is important.\n- No file size limit: Is there a limit to the file size ?\n- Flexibility: Can I save custom code in the format and be able to use it later with zero extra code ? (~ means we can store more than pure tensors, but no custom code)", "- Safe: Can I use a file randomly downloaded and expect not to run arbitrary code ?\n- Zero-copy: Does reading the file require more memory than the original file ?\n- Lazy loading: Can I inspect the file without loading everything ? And loading only\nsome tensors in it without scanning the whole file (distributed setting) ?\n- Layout control: Lazy loading, is not necessarily enough since if the information about tensors is spread out in your file, then even if the information is lazily accessible you might have to access most of your file to read the available tensors (incurring many DISK -> RAM copies). Controlling the layout to keep fast access to single tensors is important.\n- No file size limit: Is there a limit to the file size ?\n- Flexibility: Can I save custom code in the format and be able to use it later with zero extra code ? (~ means we can store more than pure tensors, but no custom code)"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response incorrectly identifies `tf.keras.models.load_model()` and `tf.saved_model.load()` as the functions used to load a saved tensor file, which are actually used to load a full model, not a tensor file. The mention of `safetensors` package is somewhat aligned with the reference answer, but the function name provided does not match the correct function `safetensors.tensorflow.load_file` from the reference answer. Additionally, the response includes recommendations for saving and loading model weights with unrelated functions, which is not relevant to the question about loading a tensor file. Therefore, the response is somewhat correct but includes significant inaccuracies."}, {"question": "Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n", "true_answer": "In the \"Logs\" tab of your Endpoint through the UI.", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/logs.mdx", "generated_answer": "Answer: You can access the logs of your Endpoints in Hugging Face Endpoints through the \"Logs\" tab in the UI of your Endpoint. Both the build logs of your Image artifacts and the Container Logs during inference are available. If your Endpoint creation is in the \"Failed\" state, you can also check the Build Logs to see the reason. (Source: Extracted document 0)", "retrieved_docs": ["Access and read Logs\n\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the \u201cLogs\u201d tab of your Endpoint. \n\nYou will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\" alt=\"select logs\" />\n\nThe Container Logs are only available when your Endpoint is in the \u201cRunning\u201d state. \n\n_Note: If your Endpoint creation is in the \u201cFailed\u201d state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\n\n**Build Logs:**\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png\" alt=\"build logs\" />\n\n**Container Logs:**\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png\" alt=\"container logs\" />", "Access and view Metrics\n\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the \u201cAnalytics\u201d tab of your Endpoints. \n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png\" alt=\"metric dashboard\" />\n\n## Access Metrics via API\n\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints.", "You should provide a MongoDB endpoint where your chats will be written. If you leave this section blank, your logs will be persisted to a database inside the Space. Note that Hugging Face does not have access to your chats. You can configure the name and the theme of the Space by providing the application name and application color parameters.\nBelow this, you can select the Hugging Face Hub ID of the model you wish to serve. You can also change the generation hyperparameters in the dictionary below in JSON format. \n\n_Note_: If you'd like to deploy a model with gated access or a model in a private repository, you can simply provide `HUGGING_FACE_HUB_TOKEN` in repository secrets. You need to set its value to an access token you can get from [here](https://huggingface.co/settings/tokens).\n\n<a href=\"Parameters\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/huggingface.co_spaces_docker_chatui_params.png\" />\n</a>", "Security & Compliance\n\n\ud83e\udd17 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place.\n\n## Data Security/Privacy\n\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\n\nWe also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet.\n\nHugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan.\n\n## Model Security/Privacy:", "## Custom Handler Examples\n\nThere are already several public examples on the [Hugging Face Hub](https://huggingface.co/models?other=endpoints-template) where you can take insipiration or directly use them. The repositories are tagged with `endpoints-template` and can be found under this [link](https://huggingface.co/models?other=endpoints-template).\n\nIncluded examples are for:", "The base URL for the HTTP endpoints above is `https://huggingface.co`.\n\nThose endpoints are not officially supported in `huggingface_hub` or `huggingface.js` yet but [this code snippet](https://github.com/huggingface/huggingface_hub/issues/1535#issuecomment-1614693412) (in Python) might help you getting started.\nNEW! There's an [open PR](https://github.com/huggingface/huggingface_hub/pull/1905) in `huggingface_hub` to add official support from our Python library.\n\n### Download access report\n\nYou can download a report of all access requests for a gated model with the **download user access report** button. Click on it to download a json file with a list of users. For each entry, you have:\n- **user**: the user id. Example: *julien-c*.\n- **fullname**: name of the user on the Hub. Example: *Julien Chaumond*.\n- **status**: status of the request. Either `\"pending\"`, `\"accepted\"` or `\"rejected\"`.\n- **email**: email of the user.\n- **time**: datetime when the user initially made the request.", "The base URL for the HTTP endpoints above is `https://huggingface.co`.\n\nThose endpoints are not officially supported in `huggingface_hub` or `huggingface.js` yet but [this code snippet](https://github.com/huggingface/huggingface_hub/issues/1535#issuecomment-1614693412) (in Python) might help you getting started.\n\n### Download access report\n\nYou can download a report of all access requests for a gated datasets with the **download user access report** button. Click on it to download a json file with a list of users. For each entry, you have:\n- **user**: the user id. Example: *julien-c*.\n- **fullname**: name of the user on the Hub. Example: *Julien Chaumond*.\n- **status**: status of the request. Either `\"pending\"`, `\"accepted\"` or `\"rejected\"`.\n- **email**: email of the user.\n- **time**: datetime when the user initially made the request.\n\n<a id=\"modifying-the-prompt\"></a> <!-- backward compatible anchor -->\n\n### Customize requested information"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns with the reference answer by indicating that the logs can be accessed through the \"Logs\" tab in the UI of the Endpoint. Additionally, the response goes into further detail by explaining the types of logs available, such as build logs of Image artifacts and Container Logs during inference, and what to do if the Endpoint creation is in the \"Failed\" state. This additional information supports the correctness and factual nature of the response without deviating from the accuracy of the reference answer."}, {"question": "What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n", "true_answer": "Image Classification", "source_doc": "huggingface/blog/blob/main/autotrain-image-classification.md", "generated_answer": "The latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification, as announced in the blog post \"The State of Computer Vision at Hugging Face \ud83e\udd17\". This allows users to train models for image classification with zero configuration needed, and AutoTrain will experiment with different model architectures to find the best one for the user's project. Other vision tasks supported by AutoTrain include text classification, text summarization, named entity recognition, and more", "retrieved_docs": ["[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with **zero** configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer \ud83e\udd2f We\u2019ve been expanding the number of tasks that we support, and we\u2019re proud to announce that **you can now use AutoTrain for Computer Vision**! Image Classification is the latest task we\u2019ve added, with more on the way. But what does this mean for you?", "## \ud83e\udd17 AutoTrain\n\n[AutoTrain](https://huggingface.co/autotrain) provides a \u201cno-code\u201d solution to train state-of-the-art Machine Learning models for tasks like text classification, text summarization, named entity recognition, and more. For Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage.\n\nAutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate \ud83e\udd17 Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details.\n\n## The technical philosophy", "--\ntitle: The State of Computer Vision at Hugging Face \ud83e\udd17\nthumbnail: /blog/assets/cv_state/thumbnail.png\nauthors:\n- user: sayakpaul\n---\n\n# The State of Computer Vision at Hugging Face \ud83e\udd17\n\n\nAt Hugging Face, we pride ourselves on democratizing the field of artificial intelligence together with the community. As a part of that mission, we began focusing our efforts on computer vision over the last year. What started as a [PR for having Vision Transformers (ViT) in \ud83e\udd17 Transformers](https://github.com/huggingface/transformers/pull/10950) has now grown into something much bigger \u2013 8 core vision tasks, over 3000 models, and over 100 datasets on the Hugging Face Hub.\n\nA lot of exciting things have happened since ViTs joined the Hub. In this blog post, we\u2019ll summarize what went down and what\u2019s coming to support the continuous progress of Computer Vision from the \ud83e\udd17 ecosystem.\n\nHere is a list of things we\u2019ll cover:", "## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive.", "## How can you train your own image classifier?\n\nIf you haven\u2019t [created a Hugging Face account](https://huggingface.co/join) yet, now\u2019s the time! Following that, make your way over to the [AutoTrain homepage](https://huggingface.co/autotrain) and click on \u201cCreate new project\u201d to get started. You\u2019ll be asked to fill in some basic info about your project. In the screenshot below you\u2019ll see that I created a project named `butterflies-classification`, and I chose the \u201cImage Classification\u201d task. I\u2019ve also chosen the \u201cAutomatic\u201d model option, since I want to let AutoTrain do the work of finding the best model architectures for my project.", "At Hugging Face, we\u2019ve been quietly working to pave the way for this inclusive future. Our suite of tools, including services like Spaces, AutoTrain, and Inference Endpoints, are designed to make the world of machine learning accessible to everyone.\n\nTo showcase just how accessible this democratized future is, this tutorial will show you how to use [Spaces](https://huggingface.co/Spaces), [AutoTrain](https://huggingface.co/autotrain) and [ChatUI](https://huggingface.co/inference-endpoints) to build the chat app. All in just three simple steps, sans a single line of code. For context I\u2019m also not an ML engineer, but a member of the Hugging Face GTM team. If I can do this then you can too! Let's dive in!\n\n## Introduction to Spaces", "Here is a list of things we\u2019ll cover:\n\n- [Supported vision tasks and Pipelines](#support-for-pipelines)\n- [Training your own vision models](#training-your-own-models)\n- [Integration with `timm`](#\ud83e\udd17-\ud83e\udd1d-timm)\n- [Diffusers](#\ud83e\udde8-diffusers)\n- [Support for third-party libraries](#support-for-third-party-libraries)\n- [Deployment](#deployment)\n- and much more!\n\n## Enabling the community: One task at a time \ud83d\udc41\n\nThe Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:\n\n- Image classification\n- Image segmentation\n- (Zero-shot) object detection\n- Video classification\n- Depth estimation\n- Image-to-image synthesis\n- Unconditional image generation\n- Zero-shot image classification"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies \"Image Classification\" as the latest task added to Hugging Face AutoTrain for Computer Vision, which aligns with the reference answer. However, the response includes additional information that is not required by the instruction, such as the announcement in a blog post and details about the AutoTrain's capabilities and other supported tasks. While this additional information is related to Hugging Face AutoTrain, it is not necessary to answer the question posed in the instruction. The score rubric requires the response to be correct, accurate, and factual based on the reference answer. Since the response does include the correct task but also contains superfluous information, it is not completely aligned with the reference answer. Therefore, the response is mostly correct, accurate, and factual."}, {"question": "What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n", "true_answer": "model", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md", "generated_answer": "The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository, as stated in the context provided in extract 0. If you want to create a different type of repository, such as a dataset or a space, you can specify the `repo_type` parameter when calling the `create_repo` function.", "retrieved_docs": ["By default, [`create_repo`] creates a model repository. But you can use the `repo_type` parameter to specify another repository type. For example, if you want to create a dataset repository:\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-dataset\", repo_type=\"dataset\")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n```\n\nWhen you create a repository, you can set your repository visibility with the `private` parameter.\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-private\", private=True)\n```\n\nIf you want to change the repository visibility at a later time, you can use the [`update_repo_visibility`] function.\n\n### Delete a repository\n\nDelete a repository with [`delete_repo`]. Make sure you want to delete a repository because this is an irreversible process!\n\nSpecify the `repo_id` of the repository you want to delete:\n\n```py\n>>> delete_repo(repo_id=\"lysandre/my-corrupted-dataset\", repo_type=\"dataset\")\n```", "### `create_repo`\n\nThe `create_repo` method creates a repository on the Hub. Use the `name` parameter to provide a name for your repository:\n\n```python\n>>> from huggingface_hub import create_repo\n>>> create_repo(repo_id=\"test-model\")\n'https://huggingface.co/lysandre/test-model'\n```\n\nWhen you check your Hugging Face account, you should now see a `test-model` repository under your namespace.\n\n### `upload_file`\n\nThe `upload_file` method uploads files to the Hub. This method requires the following:\n\n- A path to the file to upload.\n- The final path in the repository.\n- The repository you wish to push the files to.\n\nFor example:\n\n```python\n>>> from huggingface_hub import upload_file\n>>> upload_file(\n...    path_or_fileobj=\"/home/lysandre/dummy-test/README.md\", \n...    path_in_repo=\"README.md\", \n...    repo_id=\"lysandre/test-model\"\n... )\n'https://huggingface.co/lysandre/test-model/blob/main/README.md'\n```", "```python\n# if the repository doesn't exist remotely on the Hugging Face Hub, it will be created when we set create_remote to True\nrepo_id = \"skops-user/my-awesome-model\"\nhub_utils.push(\n    repo_id=repo_id,\n    source=local_repo,\n    token=token,\n    commit_message=\"pushing files to the repo from the example!\",\n    create_remote=True,\n)\n```\n\nOnce we push the model to the Hub, anyone can use it unless the repository is private. You can download the models using `download`. Apart from the model file, the repository contains the model configuration and the environment requirements.\n\n```python\ndownload_repo = \"downloaded-model\"\nhub_utils.download(repo_id=repo_id, dst=download_repo)\n```\n\nThe inference widget is enabled to make predictions in the repository.\n\n![Hosted Inference Widget](assets/94_skops/skops_widget.png)\n\nIf the requirements of your project have changed, you can use `update_env` to update the environment.", "```py\nfrom huggingface_hub import create_repo\n\ncreate_repo(\"dummy-model\")\n```\n\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\n\n```py\nfrom huggingface_hub import create_repo\n\ncreate_repo(\"dummy-model\", organization=\"huggingface\")\n```\n\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization.\nOther arguments which may be useful are:\n\n- `private`, in order to specify if the repository should be visible from others or not.\n- `token`, if you would like to override the token stored in your cache by a given token.\n- `repo_type`, if you would like to create a `dataset` or a `space` instead of a model. Accepted values are `\"dataset\"` and `\"space\"`.\n\nOnce the repository is created, we should add files to it! Jump to the next section to see the three ways this can be handled.", "Lastly, to push our model to the Hub, we will need to create a `Repository` object in a working folder. First log in to the Hugging Face Hub, if you aren't logged in already. We'll determine the repository name from the model ID we want to give our model (feel free to replace the `repo_name` with your own choice; it just needs to contain your username, which is what the function `get_full_repo_name()` does):\n\n```py\nfrom huggingface_hub import Repository, get_full_repo_name\n\nmodel_name = \"codeparrot-ds-accelerate\"\nrepo_name = get_full_repo_name(model_name)\nrepo_name\n```\n\n```python out\n'sgugger/codeparrot-ds-accelerate'\n```\n\nThen we can clone that repository in a local folder. If it already exists, this local folder should be an existing clone of the repository we are working with:\n\n```py\noutput_dir = \"codeparrot-ds-accelerate\"\nrepo = Repository(output_dir, clone_from=repo_name)\n```", "Payload:\n\n```js\npayload = {\n    \"type\":\"model\",\n    \"name\":\"name\",\n    \"organization\": \"organization\",\n    \"private\":\"private\",\n    \"sdk\": \"sdk\"\n}\n```\n\nThis is equivalent to `huggingface_hub.create_repo()`.\n\n### DELETE /api/repos/delete\n\nDelete a repository. It's a model repo by default.\n\nParameters:\n- `type`: Type of repo (dataset or space; model by default).\n- `name`: Name of repo.\n- `organization`: Name of organization (optional).\n\nPayload:\n\n```js\npayload = {\n    \"type\": \"model\",\n    \"name\": \"name\",\n    \"organization\": \"organization\",\n}\n```\n\nThis is equivalent to `huggingface_hub.delete_repo()`.\n\n### PUT /api/repos/{repo_type}/{repo_id}/settings\n\nUpdate repo visibility.\n\nPayload:\n\n```js\npayload = {\n    \"private\": \"private\",\n}\n```\n\nThis is equivalent to `huggingface_hub.update_repo_visibility()`.\n\n### POST /api/repos/move\n\nMove a repository (rename within the same namespace or transfer from user to organization).", "```bash\nhuggingface-cli login\n```\n\nThe `huggingface_hub` package offers several methods and classes which are useful for our purpose. Firstly, there are a few methods to manage repository creation, deletion, and others:\n\n```python no-format\nfrom huggingface_hub import (\n    # User management\n    login,\n    logout,\n    whoami,\n\n    # Repository creation and management\n    create_repo,\n    delete_repo,\n    update_repo_visibility,\n\n    # And some methods to retrieve/change information about the content\n    list_models,\n    list_datasets,\n    list_metrics,\n    list_repo_files,\n    upload_file,\n    delete_file,\n)\n```\n\n\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them.\n\nThe `create_repo` method can be used to create a new repository on the hub:\n\n```py\nfrom huggingface_hub import create_repo\n\ncreate_repo(\"dummy-model\")\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that the default repository type is a model repository. Additionally, the response includes extra information about how to specify a different repository type, which is accurate and relevant to the instruction but not necessary for a full score."}, {"question": "How many splits does the \"duorc\" dataset have?\n", "true_answer": "Six", "source_doc": "huggingface/datasets-server/blob/main/docs/source/splits.mdx", "generated_answer": "The \"duorc\" dataset has six splits, as shown in the provided JSON endpoint response (extract 0). The splits are listed as follows:\n\n```json\n{\n  \"splits\": [\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"train\" },\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"", "retrieved_docs": ["The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\n\n```json\n{\n  \"splits\": [\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"train\" },\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"validation\" },\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"test\" },\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"train\" },\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"validation\" },\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"test\" }\n  ],\n  \"pending\": [],\n  \"failed\": []\n}\n```", "The endpoint response is a JSON containing a list of the dataset's files URLs in the Parquet format for each split and configuration. For example, the [`duorc`](https://huggingface.co/datasets/duorc) dataset has one Parquet file for the train split of the \"ParaphraseRC\" configuration (see the [List splits and configurations](./splits) guide for more details about splits and configurations).", "},\n    {\n      \"dataset\": \"duorc\",\n      \"config\": \"SelfRC\",\n      \"split\": \"test\",\n      \"url\": \"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/SelfRC/test/0000.parquet\",\n      \"filename\": \"0000.parquet\",\n      \"size\": 3035735\n    },\n    {\n      \"dataset\": \"duorc\",\n      \"config\": \"SelfRC\",\n      \"split\": \"train\",\n      \"url\": \"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/SelfRC/train/0000.parquet\",\n      \"filename\": \"0000.parquet\",\n      \"size\": 14851719\n    },\n    {\n      \"dataset\": \"duorc\",\n      \"config\": \"SelfRC\",\n      \"split\": \"validation\",\n      \"url\": \"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/SelfRC/validation/0000.parquet\",\n      \"filename\": \"0000.parquet\",\n      \"size\": 3114389\n    }\n  ]\n}\n```", "```json\n{\n    \"dataset_info\": {\n        \"description\": \"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\n\",\n        \"citation\": \"@inproceedings{DuoRC,\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\nyear = {2018}\\n}\\n\",\n        \"homepage\": \"https://duorc.github.io/\",\n        \"license\": \"https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\",\n        \"features\": {\n            \"plot_id\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"plot\": {\n                \"dtype\": \"string\",\n                \"_type\": \"Value\"\n            },\n            \"title\": {", "},\n        \"splits\": {\n            \"train\": {\n                \"name\": \"train\",\n                \"num_bytes\": 239852729,\n                \"num_examples\": 60721,\n                \"dataset_name\": \"duorc\"\n            },\n            \"validation\": {\n                \"name\": \"validation\",\n                \"num_bytes\": 51662519,\n                \"num_examples\": 12961,\n                \"dataset_name\": \"duorc\"\n            },\n            \"test\": {\n                \"name\": \"test\",\n                \"num_bytes\": 49142710,\n                \"num_examples\": 12559,\n                \"dataset_name\": \"duorc\"\n            }\n        },\n        \"download_checksums\": {\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_train.json\": {\n                \"num_bytes\": 24388192,\n                \"checksum\": null\n            },\n            \"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC_dev.json\": {\n                \"num_bytes\": 5051240,\n                \"checksum\": null", "\"num_bytes_memory\": 496682909,\n        \"num_rows\": 69524,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"ParaphraseRC\",\n        \"split\": \"validation\",\n        \"num_bytes_parquet_files\": 5566868,\n        \"num_bytes_memory\": 106510489,\n        \"num_rows\": 15591,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"ParaphraseRC\",\n        \"split\": \"test\",\n        \"num_bytes_parquet_files\": 6136591,\n        \"num_bytes_memory\": 115215760,\n        \"num_rows\": 15857,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"SelfRC\",\n        \"split\": \"train\",\n        \"num_bytes_parquet_files\": 14851720,\n        \"num_bytes_memory\": 239852729,\n        \"num_rows\": 60721,\n        \"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"SelfRC\",\n        \"split\": \"validation\",\n        \"num_bytes_parquet_files\": 3114390,\n        \"num_bytes_memory\": 51662519,", "split = \"train\"  # \"valid\"\nfilters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n\ndata = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\nfiltered_data = filter_streaming_dataset(data, filters)\n```\n\n```python out\n3.26% of data after filtering.\n```\n\nThis leaves us with about 3% of the original dataset, which is still quite sizable -- the resulting dataset is 6 GB and consists of 600,000 Python scripts!\n\nFiltering the full dataset can take 2-3h depending on your machine and bandwidth. If you don't want to go through this lengthy process yourself, we provide the filtered dataset on the Hub for you to download:\n\n```py\nfrom datasets import load_dataset, DatasetDict\n\nds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\nds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies the number of splits in the \"duorc\" dataset as six, which aligns with the reference answer. However, the response is incomplete as it does not list all the splits but instead provides a JSON snippet that is cut off. This does not affect the factual accuracy of the number of splits but does impact the completeness of the information provided. Based on the rubric, the response is mostly correct, accurate, and factual, but not complete."}, {"question": "What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n", "true_answer": "FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.", "source_doc": "huggingface/peft/blob/main/docs/source/accelerate/fsdp.md", "generated_answer": "Fully Sharded Data Parallel (FSDP) in distributed training is a data parallel method developed for scaling training of large pretrained models up to 1T parameters. It shards the model parameters, gradients, and optimizer states across data parallel processes and can also offload sharded model parameters to a CPU. This reduces memory usage and allows for scaling training to larger batch or model sizes. In FSDP, during the forward pass, each FSDP unit performs an", "retrieved_docs": ["-->\n\n# Fully Sharded Data Parallel\n\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or *rank*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class.\n\nBefore you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer.\n\n```bash\npip install accelerate\n```\n\n## FSDP configuration", "Fully Sharded Data Parallelism (FSDP) is a paradigm in which the optimizer states, gradients and parameters are sharded across devices. During the forward pass, each FSDP unit performs an _all-gather operation_ to get the complete weights, computation is performed followed by discarding the shards from other devices. After the forward pass, the loss is computed followed by the backward pass. In the backward pass, each FSDP unit performs an all-gather operation to get the complete weights, with computation performed to get the local gradients. These local gradients are averaged and sharded across the devices via a _reduce-scatter operation_ so that each device can update the parameters of its shard. For more information on what PyTorch FSDP is, please refer to this blog post: [Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel](https://huggingface.co/blog/pytorch-fsdp).\n\n![FSDP Workflow](./assets/62_pytorch_fsdp/FSDP_workflow.png)", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Fully Sharded Data Parallel\n\n[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.\n\n<Tip warning={true}>\n\nCurrently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training. You can track this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any updates.\n\n</Tip>", "NEFTune is disabled after training to restore the original embedding layer to avoid any unexpected behavior.\n\n## Accelerate and Trainer\n\nThe [`Trainer`] class is powered by [Accelerate](https://hf.co/docs/accelerate), a library for easily training PyTorch models in distributed environments with support for integrations such as [FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) and [DeepSpeed](https://www.deepspeed.ai/).\n\n<Tip>\n\nLearn more about FSDP sharding strategies, CPU offloading, and more with the [`Trainer`] in the [Fully Sharded Data Parallel](fsdp) guide.\n\n</Tip>", "--\ntitle: \"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\"\nthumbnail: /blog/assets/62_pytorch_fsdp/fsdp-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n\n# Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\n\n\nIn this post we will look at how we can leverage **[Accelerate](https://github.com/huggingface/accelerate)** Library for training large models which enables users to leverage the latest features of **[PyTorch FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**.\n\n# Motivation \ud83e\udd17", "In this post we will look at Data Parallelism using ZeRO and more specifically the latest PyTorch feature **[FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**.  **[DeepSpeed](https://github.com/microsoft/deepspeed)** and **[FairScale](https://github.com/facebookresearch/fairscale/)** have implemented the core ideas of the ZERO paper. These have already been integrated in `transformers` Trainer and accompanied by great blog [Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/zero-deepspeed-fairscale) [10]. PyTorch recently upstreamed the Fairscale FSDP into PyTorch Distributed with additional optimizations.\n\n# Accelerate \ud83d\ude80:  Leverage PyTorch FSDP without any code\u00a0changes\n\nWe will look at the task of Causal Language Modelling using GPT-2 Large (762M) and XL (1.5B) model variants.", "(Source:\u00a0[link](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/))\n\nThe above workflow gives an overview of what happens behind the scenes when FSDP is activated. Let's first understand how DDP works and how FSDP improves it. In DDP, each worker/accelerator/GPU has a replica of the entire model parameters, gradients and optimizer states. Each worker gets a different batch of data, it goes through the forwards pass, a loss is computed followed by the backward pass to generate gradients. Now, an all-reduce operation is performed wherein each worker gets the gradients from the remaining workers and averaging is done. In this way, each worker now has the same global gradients which are used by the optimizer to update the model parameters. We can see that having full replicas consume a lot of redundant memory on each GPU, which limits the batch size as well as the size of the models."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct, accurate, and factual. It correctly identifies FSDP as a data parallel method used in distributed training and mentions its capability to scale training of large pretrained models up to 1T parameters. The response also accurately describes the sharding of model parameters, gradients, and optimizer states across data parallel processes, which is a key feature of FSDP. Additionally, it mentions the ability of FSDP to offload sharded model parameters to a CPU, which is a relevant detail in the context of reducing memory usage and scaling training. However, the response is incomplete as it ends abruptly without finishing the explanation of the forward pass process. Despite this, the information provided aligns well with the reference answer and the core purpose of FSDP is conveyed. Therefore, the response is mostly correct, accurate, and factual."}, {"question": "What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n", "true_answer": "`.safetensors`", "source_doc": "huggingface/safetensors/blob/main/docs/source/convert-weights.md", "generated_answer": "The more secure file format used to save and store PyTorch model weights is `.safetensors`. This format is recommended over saving weights as `.bin` files with Python's `pickle` utility, as it provides a more secure way to store model weights. To convert existing PyTorch weights to `.safetensors`, the easiest way is to use the Convert Space provided by Hugging Face, or by running the `convert.py` script locally.", "retrieved_docs": ["Convert weights to safetensors\n\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n\nThe easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\n\n<Tip warning={true}>\n\nFor larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights.", "This is why large models on the Hugging Face Hub are not saved and shared with one big file containing all the weights, but **several** of them. If you go to the [BLOOM model page](https://huggingface.co/bigscience/bloom/tree/main) for instance, you will see there is 72 files named `pytorch_model_xxxxx-of-00072.bin`, which each contain part of the model weights. Using this format, we can load one part of the state dict in memory, put the weights inside the model, move them on the right device, then discard this state dict part before going to the next. Instead of requiring to have enough RAM to accommodate the whole model, we only need enough RAM to get the biggest checkpoint part, which we call a **shard**, so 7.19GB in the case of BLOOM.", "## Sharding state dicts\n\nTraditionally, PyTorch models are saved in a whole file containing a map from parameter name to weight. This map is often called a `state_dict`. Here is an excerpt from the [PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html) on saving on loading:\n\n```python\n# Save the model weights\ntorch.save(my_model.state_dict(), 'model_weights.pth')\n\n# Reload them\nnew_model = ModelClass()\nnew_model.load_state_dict(torch.load('model_weights.pth'))\n```\n\nThis works pretty well for models with less than 1 billion parameters, but for larger models, this is very taxing in RAM. The BLOOM model has 176 billions parameters; even with the weights saved in bfloat16 to save space, it still represents 352GB as a whole. While the super computer that trained this model might have this amount of memory available, requiring this for inference is unrealistic.", "{#if fw === 'pt'}\nThe *pytorch_model.bin* file is known as the *state dictionary*; it contains all your model's weights. The two files go hand in hand; the configuration is necessary to know your model's architecture, while the model weights are your model's parameters.\n\n{:else}\nThe *tf_model.h5* file is known as the *state dictionary*; it contains all your model's weights. The two files go hand in hand; the configuration is necessary to know your model's architecture, while the model weights are your model's parameters.\n\n{/if}\n\n## Using a Transformer model for inference[[using-a-transformer-model-for-inference]]\n\nNow that you know how to load and save a model, let's try using it to make some predictions. Transformer models can only process numbers \u2014 numbers that the tokenizer generates. But before we discuss tokenizers, let's explore what inputs the model accepts.", "```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```", "```bash\npython zero_to_fp32.py . pytorch_model.bin\n```\n\nThis is it. `pytorch_model.bin` will now contain the full fp32 model weights consolidated from multiple GPUs.\n\nThe script will automatically be able to handle either a ZeRO-2 or ZeRO-3 checkpoint.\n\n`python zero_to_fp32.py -h` will give you usage details.\n\nThe script will auto-discover the deepspeed sub-folder using the contents of the file `latest`, which in the current\nexample will contain `global_step1`.\n\nNote: currently the script requires 2x general RAM of the final fp32 model weights.\n\n\n### ZeRO-3 and Infinity Nuances\n\nZeRO-3 is quite different from ZeRO-2 because of its param sharding feature.\n\nZeRO-Infinity further extends ZeRO-3 to support NVMe memory and multiple other speed and scalability improvements.\n\nWhile all the efforts were made for things to just work without needing any special changes to your models, in certain\ncircumstances you may find the following information to be needed.", "```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies `.safetensors` as the more secure file format for saving and storing PyTorch model weights, which aligns with the reference answer. The additional information provided about the format being recommended over `.bin` files and the methods to convert existing weights to `.safetensors` is accurate and factual, although not required by the instruction. The response fully meets the criteria of being correct, accurate, and factual."}, {"question": "What type of security certification does Hugging Face have?\n", "true_answer": "SOC2 Type 2 certified", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/security.md", "generated_answer": "Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses. (Source: Extract 0)", "retrieved_docs": ["Security\n\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.\n\nHugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). \n\nHugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.", "## Model Security/Privacy:\n\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.\n\n## Inference Endpoints and Hub Security\n\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\n## Inference Endpoint Security level\n\nWe currently offer three types of endpoints, in order or increasing security level:", "We are happy to announce that Hugging Face has been selected by the [CNIL](https://www.cnil.fr/en/home) (French Data Protection Authority) to benefit from its [Enhanced Support program](https://www.cnil.fr/en/enhanced-support-cnil-selects-3-digital-companies-strong-potential)!\nThis new program picked three companies with \u201cstrong potential for economic development\u201d out of over 40 candidates, who will receive support in understanding and implementing their duties with respect to data protection - \na daunting and necessary endeavor in the context of the rapidly evolving field of Artificial Intelligence.", "[Explore existing organizations](https://huggingface.co/organizations), create a new organization [here](https://huggingface.co/organizations/new), and then visit the [**Organizations documentation**](./organizations) to learn more.\n\n## Security\n\nThe Hugging Face Hub supports security and access control features to give you the peace of mind that your code, models, and data are safe. Visit the [**Security**](./security) section in these docs to learn about:\n\n- User Access Tokens\n- Access Control for Organizations\n- Signing commits with GPG\n- Malware scanning\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">", "In the initial set up phase of SafeCoder, the Hugging Face team provides containers, scripts and examples to work hand in hand with the customer to select, extract, prepare, duplicate, deidentify internal codebase data into a training dataset to be used in a Hugging Face provided training container configured to the hardware infrastructure available to the customer.\n\nIn the deployment phase of SafeCoder, the customer deploys containers provided by Hugging Face on their own infrastructure to expose internal private endpoints within their VPC. These containers are configured to the exact hardware configuration available to the customer, including NVIDIA GPUs, AMD Instinct GPUs, Intel Xeon CPUs, AWS Inferentia2 or Habana Gaudi accelerators.\n\n## Compliance as a Core Principle\n\nAs the regulation framework around machine learning models and datasets is still being written across the world, global companies need to make sure the solutions they use minimize legal risks.", "## FAQ[[faq]]\n\nHere are some answers to frequently asked questions:\n\n- **Does taking this course lead to a certification?**\nCurrently we do not have any certification for this course. However, we are working on a certification program for the Hugging Face ecosystem -- stay tuned!\n\n- **How much time should I spend on this course?**\nEach chapter in this course is designed to be completed in 1 week, with approximately 6-8 hours of work per week. However, you can take as much time as you need to complete the course.\n\n- **Where can I ask a question if I have one?**\nIf you have a question about any section of the course, just click on the \"*Ask a question*\" banner at the top of the page to be automatically redirected to the right section of the [Hugging Face forums](https://discuss.huggingface.co/):\n\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/forum-button.png\" alt=\"Link to the Hugging Face forums\" width=\"75%\">", "\ud83c\udf81 Merchandise and assets. \n\n\u2728 Being officially recognized as a Hugging Face\u2019s Ambassador\n\n<br />\n\n**Eligibility Requirements for Students**\n\n- Validate your student status \n- Have taken at least one machine learning/data science course (online courses are considered as well)\n- Be enrolled in an accredited college or university\n- Be a user of the Hugging Face Hub and/or the Hugging Face\u2019s libraries\n- Acknowledge the [Code of Conduct](https://huggingface2.notion.site/huggingface2/Hugging-Face-Code-of-Conduct-45eeeafa9ef44c5e888a2952619fdfa8). Community is at the center of the Hugging Face ecosystem. Because of that, we strictly adhere to our [Code of conduct](https://huggingface2.notion.site/huggingface2/Hugging-Face-Code-of-Conduct-45eeeafa9ef44c5e888a2952619fdfa8). If any ambassador infringes it or behaves inadequately, they will be excluded from the Program."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies Hugging Face as SOC2 Type 2 certified, which is in agreement with the reference answer. The additional information provided about the meaning of SOC2 Type 2 certification and the company's practices regarding security monitoring and patching does not detract from the factual accuracy of the certification type."}, {"question": "What do RAG models combine to generate outputs?\n", "true_answer": "Pretrained dense retrieval (DPR) and sequence-to-sequence models.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/rag.md", "generated_answer": "RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models to generate outputs. They retrieve contextual documents from an external dataset as part of their execution, pass them to a seq2seq model, and then marginalize to generate outputs. Both the retrieval and seq2seq modules are initialized from pretrained models and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.", "retrieved_docs": ["## Overview\n\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and\nsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate\noutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\nboth retrieval and generation to adapt to downstream tasks.\n\nIt is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, Douwe Kiela.\n\nThe abstract from the paper is the following:", "This model was contributed by [ola13](https://huggingface.co/ola13).\n\n## Usage tips\n\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. \nRAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq \nmodules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt \nto downstream tasks.\n\n## RagConfig\n\n[[autodoc]] RagConfig\n\n## RagTokenizer\n\n[[autodoc]] RagTokenizer\n\n## Rag specific outputs\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput\n\n## RagRetriever\n\n[[autodoc]] RagRetriever\n\n<frameworkcontent>\n<pt>\n\n## RagModel\n\n[[autodoc]] RagModel\n    - forward\n\n## RagSequenceForGeneration\n\n[[autodoc]] RagSequenceForGeneration\n    - forward\n    - generate\n\n## RagTokenForGeneration\n\n[[autodoc]] RagTokenForGeneration\n    - forward\n    - generate", "_An overview of RAG. The model retrieves contextual documents from an external dataset as part of its execution. These contextual documents are used in conjunction with the original input to produce an output. The  GIF is taken from [Facebook's original blog post](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models)._\n\n\n\nRecently, [Huggingface](https://huggingface.co/) partnered with [Facebook AI](https://ai.facebook.com/) to introduce the [RAG](https://twitter.com/huggingface/status/1310597560906780680) model as part of its Transformers library.", "Intro\n\nAuthors: @patrickvonplaten and @lhoestq\n\nAimed at tackling the knowledge-intensive NLP tasks (think tasks a human wouldn't be expected to solve without access to external knowledge sources), RAG models are seq2seq models with access to a retrieval mechanism providing relevant context documents at training and evaluation time.\n\nA RAG model encapsulates two core components: a question encoder and a generator.\nDuring a forward pass, we encode the input with the question encoder and pass it\nto the retriever to extract relevant context documents. The documents are then prepended to the input.\nSuch contextualized inputs are passed to the generator.\n\nRead more about RAG  at https://arxiv.org/abs/2005.11401.\n\n# Note\n\n\u26a0\ufe0f This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability\n\n# Finetuning", "```bash\npython examples/research_projects/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8\n```\nWe publish two `base` models which can serve as a starting point for finetuning on downstream tasks (use them as `model_name_or_path`):\n- [`facebook/rag-sequence-base`](https://huggingface.co/facebook/rag-sequence-base) - a base for finetuning `RagSequenceForGeneration` models,\n- [`facebook/rag-token-base`](https://huggingface.co/facebook/rag-token-base) - a base for finetuning `RagTokenForGeneration` models.\n\nThe `base` models initialize the question encoder with [`facebook/dpr-question_encoder-single-nq-base`](https://huggingface.co/facebook/dpr-question_encoder-single-nq-base) and the generator with [`facebook/bart-large`](https://huggingface.co/facebook/bart-large).", "[RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) acts just like any other [seq2seq model](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html). However, [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) has an intermediate component that retrieves contextual documents from an external knowledge base (like a Wikipedia text corpus). These documents are then used in conjunction with the input sequence and passed into the underlying seq2seq [generator](https://huggingface.co/blog/how-to-generate).", "If you would like to initialize finetuning with a base model using different question encoder and generator architectures, you can build it with a consolidation script, e.g.:\n```\npython examples/research_projects/rag/consolidate_rag_checkpoint.py \\\n    --model_type rag_sequence \\\n    --generator_name_or_path facebook/bart-large-cnn \\\n    --question_encoder_name_or_path facebook/dpr-question_encoder-single-nq-base \\\n    --dest path/to/checkpoint\n```\nYou will then be able to pass `path/to/checkpoint` as `model_name_or_path` to the `finetune_rag.py` script.\n\n## Document Retrieval\nWhen running distributed fine-tuning, each training worker needs to retrieve contextual documents\nfor its input by querying a index loaded into memory. RAG provides two implementations for document retrieval,\none with [`torch.distributed`](https://pytorch.org/docs/stable/distributed.html) communication package and the other\nwith [`Ray`](https://docs.ray.io/en/master/)."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response accurately identifies that RAG models combine pretrained dense retrieval (DPR) and sequence-to-sequence models to generate outputs, which is in line with the reference answer. It also provides additional correct details about the process, such as the retrieval of contextual documents and the joint fine-tuning of the retrieval and seq2seq modules. The response is completely correct, accurate, and factual."}, {"question": "What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n", "true_answer": "Beautiful Soup", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md", "generated_answer": "The library used by MarkupLMFeatureExtractor to extract data from HTML and XML files is Beautiful Soup, as mentioned in the context provided. This information can be found in the usage instructions for MarkupLMProcessor, specifically in the documentation for the `MarkupLMFeatureExtractor` class. However, it should be noted that while using Beautiful Soup is the easiest way to prepare data for the model, it is not a requirement. The context also explains that users can still provide their", "retrieved_docs": ["In short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.\nOptionally, one can provide node labels to the processor, which are turned into token-level `labels`.\n\n[`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].", "## Usage: MarkupLMProcessor\n\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks.\n\n```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\n\nfeature_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from_pretrained(\"microsoft/markuplm-base\")\nprocessor = MarkupLMProcessor(feature_extractor, tokenizer)\n```", "In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs).\n\n**Use case 1: web page classification (training, inference) + token classification (inference), parse_html = True**\n\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n\n>>> html_string = \"\"\"\n...  <!DOCTYPE html>\n...  <html>\n...  <head>\n...  <title>Hello world</title>\n...  </head>\n...  <body>\n...  <h1>Welcome</h1>\n...  <p>Here is my website.</p>\n...  </body>\n...  </html>\"\"\"", "-->\n\n# MarkupLM\n\n## Overview\n\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm).\n\nThe model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction), a dataset\nfor information extraction from web pages (basically named-entity recogntion on web pages)", "```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n\n>>> html_string = \"\"\"\n...  <!DOCTYPE html>\n...  <html>\n...  <head>\n...  <title>Hello world</title>\n...  </head>\n...  <body>\n...  <h1>Welcome</h1>\n...  <p>My name is Niels.</p>\n...  </body>\n...  </html>\"\"\"\n\n>>> question = \"What's his name?\"\n>>> encoding = processor(html_string, questions=question, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n**Use case 5: web page question answering (inference), parse_html=False**\n\nFor question answering tasks (such as WebSRC), you can provide a question to the processor. If you have extracted\nall nodes and xpaths yourself, you can provide them directly to the processor. Make sure to set `parse_html` to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor", "```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False\n\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> question = \"What's his name?\"\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, questions=question, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n## Resources\n\n- [Demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n\n## MarkupLMConfig\n\n[[autodoc]] MarkupLMConfig\n    - all\n\n## MarkupLMFeatureExtractor", ">>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\n>>> encoding = processor(html_string, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n\n**Use case 2: web page classification (training, inference) + token classification (inference), parse_html=False**\n\nIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse_html` to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response provided is incorrect. The reference answer states that the correct library is \"Beautiful Soup,\" but the response inaccurately attributes the functionality to MarkupLMFeatureExtractor, which is not a library but a class within a library. The response does not directly answer the question about which library is used, instead, it discusses the usage of the MarkupLMFeatureExtractor class and mentions Beautiful Soup in an incorrect context. Based on the score rubric, the response is not factual as it confuses the class with the library and does not correctly identify the library used."}, {"question": "What is the file size limit for syncing to HF Spaces without using Git-LFS?\n", "true_answer": "10MB", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md", "generated_answer": "The file size limit for syncing to HF Spaces without using Git-LFS is 10MB, as mentioned in the context provided. If files exceed this limit, it is recommended to use Git-LFS for larger files, or to review and remove large files using a tool like BFG Repo-Cleaner before syncing to Spaces.", "retrieved_docs": ["Managing Spaces with Github Actions\n\nYou can keep your app in sync with your GitHub repository with **Github Actions**. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\n\nFirst, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.\n\n```bash\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n```\n\nThen force push to sync everything for the first time:\n\n```bash\ngit push --force space main\n```\n\nNext, set up a GitHub Action to push your main branch to Spaces. In the example below:", "Finally, create an Action that automatically checks the file size of any new pull request:\n\n\n```yaml\nname: Check file size\non:               # or directly `on: [push]` to run the action on every push on any branch\n  pull_request:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check large files\n        uses: ActionsDesk/lfs-warning@v2.0\n        with:\n          filesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\n```", "<Tip>\n\nProgress bars are supported in `hf_transfer` starting from version `0.1.4`. Consider upgrading (`pip install -U hf-transfer`) if you plan to enable faster uploads.\n\n</Tip>\n\n## (legacy) Upload files with Git LFS\n\nAll the methods described above use the Hub's API to upload files. This is the recommended way to upload files to the Hub.\nHowever, we also provide [`Repository`], a wrapper around the git tool to manage a local repository.\n\n<Tip warning={true}>\n\nAlthough [`Repository`] is not formally deprecated, we recommend using the HTTP-based methods described above instead.\nFor more details about this recommendation, please have a look at [this guide](../concepts/git_vs_http) explaining the\ncore differences between HTTP-based and Git-based approaches.\n\n</Tip>\n\nGit LFS automatically handles files larger than 10MB. But for very large files (>5GB), you need to install a custom transfer agent for Git LFS:\n\n```bash\nhuggingface-cli lfs-enable-largefiles\n```", "Text file extensions are not tracked by Git LFS by default, and if they're greater than 10MB, they will not be committed and uploaded. Take a look at the `.gitattributes` file in your repository for a complete list of tracked file extensions. For this tutorial, you can use the following sample `.csv` files since they're small: <a href=\"https://huggingface.co/datasets/stevhliu/demo/raw/main/train.csv\" download>train.csv</a>, <a href=\"https://huggingface.co/datasets/stevhliu/demo/raw/main/test.csv\" download>test.csv</a>.\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/upload_files.png\"/>\n</div>\n\n2. Drag and drop your dataset files and add a brief descriptive commit message.\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/commit_files.png\"/>\n</div>", "## Step 10: Enable Git-LFS for Large File Storage\n\nNavigate to your repository. Use the following commands to track large build files.\n\n```\ngit lfs install\ngit lfs track Build/* \n```\n\n## Step 11: Push your Changes\n\nFinally, use the following Git commands to push your changes:\n\n```\ngit add .\ngit commit -m \"Add Unity WebGL build files\"\ngit push\n```\n\n## Done!\n\nCongratulations! Refresh your Space. You should now be able to play your game in a Hugging Face Space.\n\nWe hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!", "9 directories, 34 files\n```\n\nNice, we reduced the *n-gram* by more than half to less than 2GB now. In\nthe final step, let's upload all files.\n\n```python\nrepo.push_to_hub(commit_message=\"Upload lm-boosted decoder\")\n```\n\n**Output:**\n```bash\n    Git LFS: (1 of 1 files) 1.85 GB / 1.85 GB\n    Counting objects: 9, done.\n    Delta compression using up to 2 threads.\n    Compressing objects: 100% (9/9), done.\n    Writing objects: 100% (9/9), 1.23 MiB | 1.92 MiB/s, done.\n    Total 9 (delta 3), reused 0 (delta 0)\n    To https://huggingface.co/hf-test/xls-r-300m-sv\n       27d0c57..5a191e2  main -> main\n```\n\nThat's it. Now you should be able to use the *5gram* for LM-boosted\ndecoding as shown in Section 1.\n\nAs can be seen on [`xls-r-300m-sv`'s model\ncard](https://huggingface.co/hf-test/xls-r-300m-sv#inference-with-lm)\nour *5gram* LM-boosted decoder yields a WER of 18.85% on Common Voice's 7\ntest set which is a relative performance of *ca.* 30% \ud83d\udd25.", "2. Clone your fork repository:\n\n```\ngit clone git@hf.co:me/myfork\n```\n\n3. Fetch non-LFS files:\n\n```\ncd myfork\ngit lfs install --skip-smudge --local # affects only this clone\ngit remote add upstream git@hf.co:friend/upstream\ngit fetch upstream\n```\n\n4. Fetch large files. This can take some time depending on your download bandwidth:\n\n```\ngit lfs fetch --all upstream # this can take time depending on your download bandwidth\n```\n\n4.a. If you want to completely override the fork history (which should only have an initial commit), run:\n\n```\ngit reset --hard upstream/main\n```\n\n4.b. If you want to rebase instead of overriding, run the following command and resolve any conflicts:\n\n```\ngit rebase upstream/main\n```\n\n5. Prepare your LFS files to push:\n\n```\ngit lfs install --force --local # this reinstalls the LFS hooks\nhuggingface-cli lfs-enable-largefiles . # needed if some files are bigger than 5GB\n```\n\n6. And finally push:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating the file size limit is 10MB, which is the correct information. Additionally, the response offers further advice on what to do if the file size exceeds the limit, which, while not required, does not detract from the accuracy of the answer to the question posed."}, {"question": "What is the title of the paper introducing the ByT5 model?\n", "true_answer": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md", "generated_answer": "The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models\" (arXiv:2105.13626).", "retrieved_docs": ["-->\n\n# ByT5\n\n## Overview\n\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\nKale, Adam Roberts, Colin Raffel.\n\nThe abstract from the paper is the following:", "-->\n\n# UL2\n\n## Overview\n\nThe T5 model was presented in [Unifying Language Learning Paradigms](https://arxiv.org/pdf/2205.05131v1.pdf) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler.\n\nThe abstract from the paper is the following:", "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.", "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.", "-->\n\n# SpeechT5\n\n## Overview\n\nThe SpeechT5 model was proposed in [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n\nThe abstract from the paper is the following:", "-->\n\n# LongT5\n\n## Overview\n\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention.\n\n\nThe abstract from the paper is the following:", "- **byT5**: byT5 is a T5 model pre-trained on byte sequences rather than SentencePiece subword token sequences. Refer\n  to the documentation of byT5 which can be found [here](byt5).\n\n- **UL2**: UL2 is a T5 like model pretrained on various denoising objectives\n\n- **Flan-T5**: Flan is a pretraining methods that is based on prompting. The Flan-T5 are T5 models trained on the Flan collection of \n    datasets which include: `taskmaster2`, `djaym7/wiki_dialog`, `deepmind/code_contests`, `lambada`, `gsm8k`, `aqua_rat`, `esnli`, `quasc` and `qed`.\n\n- **FLan-UL2** : the UL2 model finetuned using the \"Flan\" prompt tuning and dataset collection.\n\n- **UMT5**: UmT5 is a multilingual T5 model trained on an improved and refreshed mC4 multilingual corpus,  29 trillion characters across 107 language, using a new sampling method, UniMax. Refer to\n the documentation of mT5 which can be found [here](umt5).\n\n## Training"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly and also includes the arXiv identifier, which adds to the factual accuracy of the response."}, {"question": "What is the dimension of the feature vector for the base BERT model?\n", "true_answer": "768", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter1/05_encoders.md", "generated_answer": "The dimension of the feature vector for the base BERT model is 768, as stated in the first extract provided. This contextualized vector holds the \"meaning\" of a word in a text, taking into account the words around it, thanks to the self-attention mechanism. This information can be found in the first extract provided, specifically in the text that says \"Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined", "retrieved_docs": ["Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \"to\", isn't the representation of only the \"to\" word. It also takes into account the words around it, which we call the \u201ccontext\u201d.As in, it looks to the left context, the word on the left of the one we're studying (here the word \"Welcome\") and the context on the right (here the word \"NYC\") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \"meaning\" of that word in the text. How it does this is thanks to the self-attention mechanism", "The result is a matrix of shape (number of examples, hidden size). The hidden size is the number of dimensions in the hidden representation. For BERT, the hidden size is 768. The hidden representation is a vector of numbers that represents the text that can be used for many different tasks. In this case, we will use it for classification with [XGBoost](https://github.com/dmlc/xgboost) afterwards.\n\n```python\nimport numpy as np\nimport tqdm\n# Function that transforms a list of texts to their representation\n# learned by the transformer.\ndef text_to_tensor(\n   list_text_X_train: list,\n   transformer_model: AutoModelForSequenceClassification,\n   tokenizer: AutoTokenizer,\n   device: str,\n) -> np.ndarray:\n   # Tokenize each text in the list one by one\n   tokenized_text_X_train_split = []\n   tokenized_text_X_train_split = [\n       tokenizer.encode(text_x_train, return_tensors=\"pt\")\n       for text_x_train in list_text_X_train\n   ]", "---\n \\\\( {}^1 \\\\) For a simpler explanation, the layer norm layer which is normally applied to \\\\( \\mathbf{\\overline{Z}} \\\\) before being processed by the feed forward layers is omitted for now.\n\n \\\\( {}^2 \\\\) In `bert-base-uncased`, *e.g.* the intermediate dimension \\\\( d_{f} \\\\) is with 3072 four times larger than the output dimension \\\\( d_{h} \\\\).\n\n \\\\( {}^3 \\\\) As a reminder, the output `config.num_attention_heads` is assumed to be 1 for the sake of clarity and illustration in this notebook, so that the output of the self-attention layers can be assumed to be of size `config.hidden_size`.\n\nMore information on chunked linear / feed forward layers can also be found [here](https://huggingface.co/transformers/glossary.html#feed-forward-chunking) on the \ud83e\udd17Transformers docs.\n\n\n### Benchmark\n\nLet's test how much memory can be saved by using chunked feed forward layers.", "Cool! Next, let\\'s get a sense of the length of input data and labels.\n\nAs models compute length in *token-length*, we will make use of the\n`bert-base-uncased` tokenizer to compute the article and summary length.\n\nFirst, we load the tokenizer.\n\n```python\nfrom transformers import BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n```\n\nNext, we make use of `.map()` to compute the length of the article and\nits summary. Since we know that the maximum length that\n`bert-base-uncased` can process amounts to 512, we are also interested\nin the percentage of input samples being longer than the maximum length.\nSimilarly, we compute the percentage of summaries that are longer than\n64, and 128 respectively.\n\nWe can define the `.map()` function as follows.", "The following table shows a complete list of all investigated model\nvariants including the number of randomly initialized weights, *i.e.*\n\\\"random\\\", and the number of weights initialized from the respective\npre-trained checkpoints, *i.e.* \\\"leveraged\\\". All models are based on a\n12-layer architecture with 768-dim hidden size embeddings, corresponding\nto the `bert-base-cased`, `bert-base-uncased`, `roberta-base`, and\n`gpt2` checkpoints in the \ud83e\udd17Transformers model hub.\n\n  |Model          |random   |leveraged   |total\n  |-------------- |:------- |----------  |-------\n  |Rnd2Rnd        |221M     |0           |221M\n  |Rnd2BERT       |112M     |109M        |221M\n  |BERT2Rnd       |112M     |109M        |221M\n  |Rnd2GPT2       |114M     |125M        |238M\n  |BERT2BERT      |26M      |195M        |221M\n  |BERTShare      |26M      |109M        |135M\n  |RoBERTaShare   |26M      |126M        |152M\n  |BERT2GPT2      |26M      |234M        |260M\n  |RoBERTa2GPT2   |26M      |250M        |276M", "##### Base models fine-tuning\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\ntorchrun --nproc_per_node=8 ./examples/question-answering/run_squad.py \\\n    --model_name_or_path zhiheng-huang/bert-base-uncased-embedding-relative-key-query \\\n    --dataset_name squad \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate 3e-5 \\\n    --num_train_epochs 2 \\\n    --max_seq_length 512 \\\n    --doc_stride 128 \\\n    --output_dir relative_squad \\\n    --per_device_eval_batch_size=60 \\\n    --per_device_train_batch_size=6\n```\nTraining with the above command leads to the following results. It boosts the BERT default from f1 score of 88.52 to 90.54.\n\n```bash\n'exact': 83.6802270577105, 'f1': 90.54772098174814\n```", "Regarding the summary length, we can see that a length of 128 already\nincludes all of the summary labels. 128 is easily within the limits of\n`bert-base-cased`, so we decide to limit the generation to 128.\n\nAgain, we will make use of the `.map()` function - this time to\ntransform each training batch into a batch of model inputs.\n\n`\"article\"` and `\"highlights\"` are tokenized and prepared as the\nEncoder\\'s `\"input_ids\"` and Decoder\\'s `\"decoder_input_ids\"`\nrespectively.\n\n`\"labels\"` are shifted automatically to the left for language modeling\ntraining.\n\nLastly, it is very important to remember to ignore the loss of the\npadded labels. In \ud83e\udd17Transformers this can be done by setting the label to\n-100. Great, let\\'s write down our mapping function then.\n\n```python\nencoder_max_length=512\ndecoder_max_length=128"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies the dimension of the feature vector for the base BERT model as 768, which is in line with the reference answer. The additional explanation provided about the contextualized vector and the self-attention mechanism, while not required, does not detract from the accuracy of the response regarding the dimension of the feature vector. Therefore, the response meets the criteria for a score of 5 as it is completely correct, accurate, and factual."}, {"question": "What special identifier does the WordPiece Model use for continuing subwords?\n", "true_answer": "##", "source_doc": "huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx", "generated_answer": "The WordPiece Model uses the special identifier \"##\" for continuing subwords. This identifier is added as a prefix to all the characters inside a word during the initial splitting process. As the model learns merge rules, it progressively learns to split words into smaller subwords, and the \"##\" prefix is used to identify tokens that are part of a word (i.e., not starting a word). The final vocabulary includes only the subwords, not the merge rules learned", "retrieved_docs": ["<a id='wordpiece'></a>\n\n### WordPiece\n\nWordPiece is the subword tokenization algorithm used for [BERT](model_doc/bert), [DistilBERT](model_doc/distilbert), and [Electra](model_doc/electra). The algorithm was outlined in [Japanese and Korean\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to\nBPE. WordPiece first initializes the vocabulary to include every character present in the training data and\nprogressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\nsymbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.", "If you used a model that added special characters to represent subtokens\nof a given \"word\" (like the `\"##\"` in\nWordPiece) you will need to customize the `decoder` to treat\nthem properly. If we take our previous `bert_tokenizer` for instance the\ndefault decoding will give:", "</Tip>\n\n## Training algorithm[[training-algorithm]]\n\n<Tip warning={true}>\n\n\u26a0\ufe0f Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate.\n\n</Tip>\n\nLike BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\"word\"` gets split like this:\n\n```\nw ##o ##r ##d\n```\n\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix.", "<a id='unigram'></a>\n\n### Unigram\n\nUnigram is a subword tokenization algorithm introduced in [Subword Regularization: Improving Neural Network Translation\nModels with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf). In contrast to BPE or\nWordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each\nsymbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and\nthe most common substrings. Unigram is not used directly for any of the models in the transformers, but it's used in\nconjunction with [SentencePiece](#sentencepiece).", "| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words don\u2019t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word).  |\n| Unigram | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |", "</Tip>\n\n## Tokenization algorithm[[tokenization-algorithm]]\n\nTokenization differs in WordPiece and BPE in that WordPiece only saves the final vocabulary, not the merge rules learned. Starting from the word to tokenize, WordPiece finds the longest subword that is in the vocabulary, then splits on it. For instance, if we use the vocabulary learned in the example above, for the word `\"hugs\"` the longest subword starting from the beginning that is inside the vocabulary is `\"hug\"`, so we split there and get `[\"hug\", \"##s\"]`. We then continue with `\"##s\"`, which is in the vocabulary, so the tokenization of `\"hugs\"` is `[\"hug\", \"##s\"]`.\n\nWith BPE, we would have applied the merges learned in order and tokenized this as `[\"hu\", \"##gs\"]`, so the encoding is different.", "explain: \"No, that's how another tokenization algorithm works.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text.\",\n\t\t\texplain: \"Yes, this is how WordPiece proceeds for the encoding.\",\n\t\t\tcorrect: true\n\t\t},\n\t]}\n/>"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies \"##\" as the special identifier used by the WordPiece Model for continuing subwords. The explanation provided is accurate and factual, aligning well with the reference answer. The response goes beyond the reference answer by explaining the function of the \"##\" identifier in the context of the WordPiece Model's operation, which is not required but does not detract from the correctness of the answer."}, {"question": "What is the purpose of the \ud83e\udde8 Diffusers tutorials?\n", "true_answer": "To provide a gentle introduction to diffusion models and help understand the library fundamentals.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/tutorials/tutorial_overview.md", "generated_answer": "The purpose of the \ud83e\udde8 Diffusers tutorials is to serve as examples on how to use Diffusers for training, with a focus on simplicity and educational value rather than creating state-of-the-art diffusion models. The tutorials should have all the necessary code found in a single Python file and be runnable from the command line with the `python <your-example>.py --args` command. This philosophy aims to make it easier for users to reproduce known training", "retrieved_docs": ["# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:", "Using \ud83e\udde8 `diffusers` at Hugging Face\n\nDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\u2019re looking for a simple inference solution or want to train your own diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.\n\n## Exploring Diffusers in the Hub\n\nThere are over 10,000 `diffusers` compatible pipelines on the Hub which you can find by filtering at the left of [the models page](https://huggingface.co/models?library=diffusers&sort=downloads). Diffusion systems are typically composed of multiple components such as text encoder, UNet, VAE, and scheduler. Even though they are not standalone models, the pipeline abstraction makes it easy to use them for inference or training.\n\nYou can find diffusion pipelines for many different tasks:", "Great, now you should have generally understood why \ud83e\udde8 Diffusers is designed the way it is \ud83e\udd17.\nWe try to apply these design principles consistently across the library. Nevertheless, there are some minor exceptions to the philosophy or some unlucky design choices. If you have feedback regarding the design, we would \u2764\ufe0f  to hear it [directly on GitHub](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feedback.md&title=).\n\n## Design Philosophy in Details\n\nNow, let's look a bit into the nitty-gritty details of the design philosophy. Diffusers essentially consists of three major classes: [pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines), [models](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models), and [schedulers](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers).\nLet's walk through more in-detail design decisions for each class.\n\n### Pipelines", "Great, now you should have generally understood why \ud83e\udde8 Diffusers is designed the way it is \ud83e\udd17.\nWe try to apply these design principles consistently across the library. Nevertheless, there are some minor exceptions to the philosophy or some unlucky design choices. If you have feedback regarding the design, we would \u2764\ufe0f  to hear it [directly on GitHub](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feedback.md&title=).\n\n## Design Philosophy in Details\n\nNow, let's look a bit into the nitty-gritty details of the design philosophy. Diffusers essentially consists of three major classes: [pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines), [models](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models), and [schedulers](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers).\nLet's walk through more detailed design decisions for each class.\n\n### Pipelines", "Training examples of the Diffusers library should adhere to the following philosophy:\n- All the code necessary to run the examples should be found in a single Python file.\n- One should be able to run the example from the command line with `python <your-example>.py --args`.\n- Examples should be kept simple and serve as **an example** on how to use Diffusers for training. The purpose of example scripts is **not** to create state-of-the-art diffusion models, but rather to reproduce known training schemes without adding too much custom logic. As a byproduct of this point, our examples also strive to serve as good educational materials.", "Training examples of the Diffusers library should adhere to the following philosophy:\n- All the code necessary to run the examples should be found in a single Python file.\n- One should be able to run the example from the command line with `python <your-example>.py --args`.\n- Examples should be kept simple and serve as **an example** on how to use Diffusers for training. The purpose of example scripts is **not** to create state-of-the-art diffusion models, but rather to reproduce known training schemes without adding too much custom logic. As a byproduct of this point, our examples also strive to serve as good educational materials.", "## Diffusers\n\nDiffusers is a generative AI library for creating images and videos from text or images with diffusion models. LoRA is an especially popular training method for diffusion models because you can very quickly train and share diffusion models to generate images in new styles. To make it easier to use and try multiple LoRA models, Diffusers uses the PEFT library to help manage different adapters for inference.\n\nFor example, load a base model and then load the [artificialguybr/3DRedmond-V1](https://huggingface.co/artificialguybr/3DRedmond-V1) adapter for inference with the [`load_lora_weights`](https://huggingface.co/docs/diffusers/v0.24.0/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) method. The `adapter_name` argument in the loading method is enabled by PEFT and allows you to set a name for the adapter so it is easier to reference.\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided aligns well with the reference answer, indicating that the purpose of the \ud83e\udde8 Diffusers tutorials is to guide users on how to use the Diffusers library effectively. It emphasizes the tutorials' focus on simplicity and educational value, which is in line with the reference answer's mention of providing a gentle introduction and helping understand the library fundamentals. The response also adds specific details about the tutorials being contained in a single Python file and being executable from the command line, which, while not mentioned in the reference answer, do not contradict the overall purpose and could be seen as supporting details. Therefore, the response is mostly correct, accurate, and factual."}, {"question": "What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n", "true_answer": "\"manual\"", "source_doc": "gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/using-flagging.md", "generated_answer": "The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `\"manual\"`. This means that by default, underneath the output components, there is a button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file by default. If the demo involves images,", "retrieved_docs": ["## The **Flag** button in `gradio.Interface`\n\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.\n\nThere are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.", "- `allow_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`.\n  - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n  - `auto`: users will not see a button to flag, but every sample will be flagged automatically.\n  - `never`: users will not see a button to flag, and no sample will be flagged.\n- `flagging_options`: this parameter can be either `None` (default) or a list of strings.\n  - If `None`, then the user simply clicks on the **Flag** button and no additional options are shown.\n  - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled **Flag as Incorrect** and **Flag as Ambiguous** appear. This only applies if `allow_flagging` is `\"manual\"`.\n  - The chosen option is then logged along with the input and output.", "iface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    description=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"],\n    flagging_callback=hf_writer\n)\n\niface.launch()\n```\n\nNotice that we define our own\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set `allow_flagging=\"manual\"`\nbecause on Hugging Face Spaces, `allow_flagging` is set to `\"never\"` by default. Here's our demo:\n\n<gradio-app space=\"gradio/calculator-flagging-crowdsourced/\"></gradio-app>", "If you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV.\n\nIf we go back to the calculator example, the following code will create the interface embedded below it.\n\n```python\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\n)\n\niface.launch()\n```\n\n<gradio-app space=\"gradio/calculator-flagging-options/\"></gradio-app>\n\nWhen users click the flag button, the csv file will now include a column indicating the selected option.\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n```", "\u5728 `gradio.Interface` \u4e2d\u6709[\u56db\u4e2a\u53c2\u6570](https://gradio.app/docs/#interface-header)\u63a7\u5236\u6807\u8bb0\u7684\u5de5\u4f5c\u65b9\u5f0f\u3002\u6211\u4eec\u5c06\u8be6\u7ec6\u4ecb\u7ecd\u5b83\u4eec\u3002\n\n- `allow_flagging`\uff1a\u6b64\u53c2\u6570\u53ef\u4ee5\u8bbe\u7f6e\u4e3a `\"manual\"`\uff08\u9ed8\u8ba4\u503c\uff09\uff0c`\"auto\"` \u6216 `\"never\"`\u3002\n  - `manual`\uff1a\u7528\u6237\u5c06\u770b\u5230\u4e00\u4e2a\u6807\u8bb0\u6309\u94ae\uff0c\u53ea\u6709\u5728\u70b9\u51fb\u6309\u94ae\u65f6\u6837\u672c\u624d\u4f1a\u88ab\u6807\u8bb0\u3002\n  - `auto`\uff1a\u7528\u6237\u5c06\u4e0d\u4f1a\u770b\u5230\u4e00\u4e2a\u6807\u8bb0\u6309\u94ae\uff0c\u4f46\u6bcf\u4e2a\u6837\u672c\u90fd\u4f1a\u81ea\u52a8\u88ab\u6807\u8bb0\u3002\n  - `never`\uff1a\u7528\u6237\u5c06\u4e0d\u4f1a\u770b\u5230\u4e00\u4e2a\u6807\u8bb0\u6309\u94ae\uff0c\u5e76\u4e14\u4e0d\u4f1a\u6807\u8bb0\u4efb\u4f55\u6837\u672c\u3002\n- `flagging_options`\uff1a\u6b64\u53c2\u6570\u53ef\u4ee5\u662f `None`\uff08\u9ed8\u8ba4\u503c\uff09\u6216\u5b57\u7b26\u4e32\u5217\u8868\u3002\n  - \u5982\u679c\u662f `None`\uff0c\u5219\u7528\u6237\u53ea\u9700\u70b9\u51fb**\u6807\u8bb0**\u6309\u94ae\uff0c\u4e0d\u4f1a\u663e\u793a\u5176\u4ed6\u9009\u9879\u3002\n  - \u5982\u679c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u5219\u7528\u6237\u4f1a\u770b\u5230\u591a\u4e2a\u6309\u94ae\uff0c\u5bf9\u5e94\u4e8e\u63d0\u4f9b\u7684\u6bcf\u4e2a\u5b57\u7b26\u4e32\u3002\u4f8b\u5982\uff0c\u5982\u679c\u6b64\u53c2\u6570\u7684\u503c\u4e3a`[\" \u9519\u8bef \", \" \u6a21\u7cca \"]`\uff0c\u5219\u4f1a\u663e\u793a\u6807\u8bb0\u4e3a**\u6807\u8bb0\u4e3a\u9519\u8bef**\u548c**\u6807\u8bb0\u4e3a\u6a21\u7cca**\u7684\u6309\u94ae\u3002\u8fd9\u4ec5\u9002\u7528\u4e8e `allow_flagging` \u4e3a `\"manual\"` \u7684\u60c5\u51b5\u3002\n  - \u6240\u9009\u9009\u9879\u5c06\u4e0e\u8f93\u5165\u548c\u8f93\u51fa\u4e00\u8d77\u8bb0\u5f55\u3002\n- `flagging_dir`\uff1a\u6b64\u53c2\u6570\u63a5\u53d7\u4e00\u4e2a\u5b57\u7b26\u4e32\u3002\n  - \u5b83\u8868\u793a\u6807\u8bb0\u6570\u636e\u5b58\u50a8\u7684\u76ee\u5f55\u540d\u79f0\u3002\n- `flagging_callback`\uff1a\u6b64\u53c2\u6570\u63a5\u53d7 `FlaggingCallback` \u7c7b\u7684\u5b50\u7c7b\u7684\u5b9e\u4f8b\n  - \u4f7f\u7528\u6b64\u53c2\u6570\u5141\u8bb8\u60a8\u7f16\u5199\u5728\u70b9\u51fb\u6807\u8bb0\u6309\u94ae\u65f6\u8fd0\u884c\u7684\u81ea\u5b9a\u4e49\u4ee3\u7801\n  - \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5b83\u8bbe\u7f6e\u4e3a `gr.CSVLogger` \u7684\u4e00\u4e2a\u5b9e\u4f8b\n  - \u4e00\u4e2a\u793a\u4f8b\u662f\u5c06\u5176\u8bbe\u7f6e\u4e3a `gr.HuggingFaceDatasetSaver` \u7684\u4e00\u4e2a\u5b9e\u4f8b\uff0c\u8fd9\u6837\u60a8\u53ef\u4ee5\u5c06\u4efb\u4f55\u6807\u8bb0\u7684\u6570\u636e\u5bfc\u5165\u5230 HuggingFace \u6570\u636e\u96c6\u4e2d\uff08\u53c2\u89c1\u4e0b\u6587\uff09\u3002\n\n## \u6807\u8bb0\u7684\u6570\u636e\u4f1a\u53d1\u751f\u4ec0\u4e48\uff1f\n\n\u5728 `flagging_dir` \u53c2\u6570\u63d0\u4f9b\u7684\u76ee\u5f55\u4e2d\uff0c\u5c06\u8bb0\u5f55\u6807\u8bb0\u7684\u6570\u636e\u7684 CSV \u6587\u4ef6\u3002\n\n\u4ee5\u4e0b\u662f\u4e00\u4e2a\u793a\u4f8b\uff1a\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e86\u5d4c\u5165\u5176\u4e2d\u7684\u8ba1\u7b97\u5668\u754c\u9762\uff1a\n\n```python\nimport gradio as gr", "![flagging callback hf](/assets/guides/flagging-callback-hf.png)\n\n\u6211\u4eec\u521b\u5efa\u4e86 `gradio.HuggingFaceDatasetSaver` \u7c7b\uff0c\u4f46\u53ea\u8981\u5b83\u7ee7\u627f\u81ea[\u6b64\u6587\u4ef6](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py)\u4e2d\u5b9a\u4e49\u7684 `FlaggingCallback`\uff0c\u60a8\u53ef\u4ee5\u4f20\u9012\u81ea\u5df1\u7684\u81ea\u5b9a\u4e49\u7c7b\u3002\u5982\u679c\u60a8\u521b\u5efa\u4e86\u4e00\u4e2a\u5f88\u68d2\u7684\u56de\u8c03\uff0c\u8bf7\u5c06\u5176\u8d21\u732e\u7ed9\u8be5\u5b58\u50a8\u5e93\uff01\n\n## \u4f7f\u7528 Blocks \u8fdb\u884c\u6807\u8bb0\n\n\u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528 `gradio.Blocks`\uff0c\u53c8\u8be5\u600e\u4e48\u529e\u5462\uff1f\u4e00\u65b9\u9762\uff0c\u4f7f\u7528 Blocks \u60a8\u62e5\u6709\u66f4\u591a\u7684\u7075\u6d3b\u6027\n--\u60a8\u53ef\u4ee5\u7f16\u5199\u4efb\u4f55\u60a8\u60f3\u5728\u6309\u94ae\u88ab\u70b9\u51fb\u65f6\u8fd0\u884c\u7684 Python \u4ee3\u7801\uff0c\n\u5e76\u4f7f\u7528 Blocks \u4e2d\u7684\u5185\u7f6e\u4e8b\u4ef6\u5206\u914d\u5b83\u3002\n\n\u540c\u65f6\uff0c\u60a8\u53ef\u80fd\u5e0c\u671b\u4f7f\u7528\u73b0\u6709\u7684 `FlaggingCallback` \u6765\u907f\u514d\u7f16\u5199\u989d\u5916\u7684\u4ee3\u7801\u3002\n\u8fd9\u9700\u8981\u4e24\u4e2a\u6b65\u9aa4\uff1a\n\n1. \u60a8\u5fc5\u987b\u5728\u4ee3\u7801\u4e2d\u7684\u67d0\u4e2a\u4f4d\u7f6e\u8fd0\u884c\u60a8\u7684\u56de\u8c03\u7684 `.setup()` \u65b9\u6cd5\n   \u5728\u7b2c\u4e00\u6b21\u6807\u8bb0\u6570\u636e\u4e4b\u524d\n2. \u5f53\u70b9\u51fb\u6807\u8bb0\u6309\u94ae\u65f6\uff0c\u60a8\u89e6\u53d1\u56de\u8c03\u7684 `.flag()` \u65b9\u6cd5\uff0c\n   \u786e\u4fdd\u6b63\u786e\u6536\u96c6\u53c2\u6570\u5e76\u7981\u7528\u901a\u5e38\u7684\u9884\u5904\u7406\u3002\n\n\u4e0b\u9762\u662f\u4e00\u4e2a\u4f7f\u7528\u9ed8\u8ba4\u7684 `CSVLogger` \u6807\u8bb0\u56fe\u50cf\u6000\u65e7\u6ee4\u955c Blocks \u6f14\u793a\u7684\u793a\u4f8b\uff1a\ndata using the default `CSVLogger`:\n\n$code_blocks_flag\n$demo_blocks_flag\n\n## \u9690\u79c1\n\n\u91cd\u8981\u63d0\u793a\uff1a\u8bf7\u786e\u4fdd\u7528\u6237\u4e86\u89e3\u4ed6\u4eec\u63d0\u4ea4\u7684\u6570\u636e\u4f55\u65f6\u88ab\u4fdd\u5b58\u4ee5\u53ca\u60a8\u8ba1\u5212\u5982\u4f55\u5904\u7406\u5b83\u3002\u5f53\u60a8\u4f7f\u7528 `allow_flagging=auto`\uff08\u5f53\u901a\u8fc7\u6f14\u793a\u63d0\u4ea4\u7684\u6240\u6709\u6570\u636e\u90fd\u88ab\u6807\u8bb0\u65f6\uff09\uff0c\u8fd9\u4e00\u70b9\u5c24\u4e3a\u91cd\u8981\n\n### \u8fd9\u5c31\u662f\u5168\u90e8\uff01\u795d\u60a8\u5efa\u8bbe\u6109\u5feb :)", "<gradio-app space=\"gradio/calculator-flagging-crowdsourced/\"></gradio-app>\n\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo).\n\n![flagging callback hf](https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true)\n\nWe created the `gradio.HuggingFaceDatasetSaver` class, but you can pass your own custom class as long as it inherits from `FLaggingCallback` defined in [this file](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py). If you create a cool callback, contribute it to the repo!\n\n## Flagging with Blocks\n\nWhat about if you are using `gradio.Blocks`? On one hand, you have even more flexibility\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\nand assign that using the built-in events in Blocks."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the default setting for the `allow_flagging` parameter as \"manual,\" which is in agreement with the reference answer. The additional explanation provided about the functionality of the flag button and the logging of data is accurate and enhances the understanding of what the \"manual\" setting entails. The response is factual and aligns with the reference answer."}, {"question": "Where can the full code for the Stable Diffusion demo be found?\n", "true_answer": "https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "source_doc": "gradio-app/gradio/blob/main/demo/stable-diffusion/DESCRIPTION.md", "generated_answer": "The full code for the Stable Diffusion demo can be found at the following link: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main. This was mentioned in the context provided, specifically in the first extract. The author also provided a simplified version of the code needed to create the demo, which can be found in the second extract. Additionally, the author mentioned other resources for the Stable Diffusion demo, such as the GitHub", "retrieved_docs": ["ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "The original codebase can be found at [Xiang-cd/DiffEdit-stable-diffusion](https://github.com/Xiang-cd/DiffEdit-stable-diffusion), and you can try it out in this [demo](https://blog.problemsolversguild.com/technical/research/2022/11/02/DiffEdit-Implementation.html).\n\nThis pipeline was contributed by [clarencechen](https://github.com/clarencechen). \u2764\ufe0f\n\n## Tips", "Gradio Demo: stable-diffusion\n### Note: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n        \n\n\n```\n!pip install -q gradio diffusers transformers nvidia-ml-py3 ftfy torch\n```\n\n\n```\nimport gradio as gr\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\nimport os\n\nauth_token = os.getenv(\"auth_token\")\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cpu\"\npipe = StableDiffusionPipeline.from_pretrained(\n    model_id, use_auth_token=auth_token, revision=\"fp16\", torch_dtype=torch.float16\n)\npipe = pipe.to(device)", "The original codebase can be found at [LambdaLabsML/lambda-diffusers](https://github.com/LambdaLabsML/lambda-diffusers#stable-diffusion-image-variations) and additional official checkpoints for image variation can be found at [lambdalabs/sd-image-variations-diffusers](https://huggingface.co/lambdalabs/sd-image-variations-diffusers).\n\n<Tip>\n\nMake sure to check out the Stable Diffusion [Tips](./overview#tips) section to learn how to explore the tradeoff between scheduler speed and quality, and how to reuse pipeline components efficiently!\n\n</Tip>\n\n## StableDiffusionImageVariationPipeline\n\n[[autodoc]] StableDiffusionImageVariationPipeline\n\t- all\n\t- __call__\n\t- enable_attention_slicing\n\t- disable_attention_slicing\n\t- enable_xformers_memory_efficient_attention\n\t- disable_xformers_memory_efficient_attention\n\n## StableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput", "</Tip>\n\nNow that you know what you need for the Stable Diffusion pipeline, load all these components with the [`~ModelMixin.from_pretrained`] method. You can find them in the pretrained [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, and each component is stored in a separate subfolder:\n\n```py\n>>> from PIL import Image\n>>> import torch\n>>> from transformers import CLIPTextModel, CLIPTokenizer\n>>> from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler", "And much more! Check out the repo.\n\n### Reusable Seeds\n\nOne of the most impressive early demos of Stable Diffusion was the reuse of seeds to tweak images. The idea is to use the seed of an image of interest to generate a new image, with a different prompt. This yields some cool results! Check out the [Colab](https://colab.research.google.com/github/pcuenca/diffusers-examples/blob/main/notebooks/stable-diffusion-seeds.ipynb) \n\n\n## Thanks for reading!\n\nI hope you enjoy reading this! Remember to give a Star in our [GitHub Repository](https://github.com/huggingface/diffusers) and join the [Hugging Face Discord Server](https://hf.co/join/discord), where we have a category of channels just for Diffusion models. Over there the latest news in the library are shared! \n\nFeel free to open issues with feature requests and bug reports! Everything that has been achieved couldn't have been done without such an amazing community.", "And that's it!\n\nOnce you have created your tool, open a pull request to the `gradio_tools` repo! We welcome all contributions.\n\n## Example tool - Stable Diffusion\n\nHere is the code for the StableDiffusion tool as an example:\n\n```python\nfrom gradio_tool import GradioTool\nimport os\n\nclass StableDiffusionTool(GradioTool):\n    \"\"\"Tool for calling stable diffusion from llm\"\"\"\n\n    def __init__(\n        self,\n        name=\"StableDiffusion\",\n        description=(\n            \"An image generator. Use this to generate images based on \"\n            \"text input. Input should be a description of what the image should \"\n            \"look like. The output will be a path to an image file.\"\n        ),\n        src=\"gradio-client-demos/stable-diffusion\",\n        hf_token=None,\n    ) -> None:\n        super().__init__(name, description, src, hf_token)\n\n    def create_job(self, query: str) -> Job:\n        return self.client.submit(query, \"\", 9, fn_index=1)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided the correct link to the full code for the Stable Diffusion demo, which matches the reference answer. It also includes additional context and information about the location of the code and other resources, which is not required but does not detract from the accuracy of the response. The response is completely correct, accurate, and factual."}, {"question": "What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n", "true_answer": "Fourier transform", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/fnet.md", "generated_answer": "The FNet model replaces the self-attention layer in a BERT model with a fourier transform that returns only the real parts of the transform. (Source: 1)", "retrieved_docs": ["-->\n\n# FNet\n\n## Overview\n\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:", "*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all", "2.  Second, BERT\\'s *bi-directional* self-attention layers have to be\n    changed to *uni-directional* self-attention layers to comply with\n    auto-regressive generation. Because both the bi-directional and the\n    uni-directional self-attention layer are based on the same *key*,\n    *query* and *value* projection weights, the decoder\\'s\n    self-attention layer weights can be initialized with BERT\\'s\n    self-attention layer weights. *E.g.* the query, key and value weight\n    parameters of the decoder\\'s uni-directional self-attention layer\n    are initialized with those of BERT\\'s bi-directional self-attention", "*Pre-trained language models like BERT and its variants have recently achieved impressive performance in various\nnatural language understanding tasks. However, BERT heavily relies on the global self-attention block and thus suffers\nlarge memory footprint and computation cost. Although all its attention heads query on the whole input sequence for\ngenerating the attention map from a global perspective, we observe some heads only need to learn local dependencies,\nwhich means the existence of computation redundancy. We therefore propose a novel span-based dynamic convolution to\nreplace these self-attention heads to directly model local dependencies. The novel convolution heads, together with the\nrest self-attention heads, form a new mixed attention block that is more efficient at both global and local context\nlearning. We equip BERT with this mixed attention design and build a ConvBERT model. Experiments have shown that", "techniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\nset. The SqueezeBERT code will be released.*", "*Recent progress in pre-trained neural language models has significantly improved the performance of many natural\nlanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with\ndisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the\ndisentangled attention mechanism, where each word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed using disentangled matrices on their\ncontents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to\npredict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency\nof model pretraining and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of", "*Recent progress in pre-trained neural language models has significantly improved the performance of many natural\nlanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with\ndisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the\ndisentangled attention mechanism, where each word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed using disentangled matrices on their\ncontents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to\npredict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency\nof model pretraining and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the Fourier transform as the replacement for the self-attention layer in the FNet model, which aligns with the reference answer. Additionally, the response provides extra detail by mentioning that only the real parts of the transform are returned, which is an accurate and factual extension of the reference answer."}, {"question": "What type of test should typically accompany a bug fix in Gradio's testing strategy?\n", "true_answer": "Dynamic code test", "source_doc": "gradio-app/gradio/blob/main/test-strategy.md", "generated_answer": "According to the testing strategy outlined in source document 0, every bug fix in Gradio should be accompanied by a test that failed before the fix and passes afterwards. This test should typically be a dynamic code test, but it could also be a linting rule or a new type of test if appropriate. However, exceptions should be carefully considered before ignoring this rule. (Source document: 0)", "retrieved_docs": ["## Managing defects\n\nAs we formalise our testing strategy and bring / keep our test up to standard, it is important that we have some principles on managing defects as they occur/ are reported. For now we can have one very simple rule:\n\n- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should _typically_ be a dynamic code test but it could be a linting rule or new type if that is appropriate. There are always exceptions but we should think very carefully before ignoring this rule.", "We should not focus on code coverage but on test coverage following the below criteria:\n\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\n\n## Types of testing", "Test Strategy\n\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to.\n\nThis document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md).\n\n## Objectives\n\nThe purposes of all testing activities on Gradio fit one of the following objectives:\n\n1. Ensure that the Gradio library functions as we expect it to.\n2. Enable the maintenance team to quickly identify both the presence and source of defects.\n3. Prevent regressions, i.e. if we fix something it should stay fixed.\n4. Improve the quality of the codebase in order to ease maintenance efforts.\n5. Reduce the amount of manual testing required.\n\n## Scope", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.3.0-beta.6\n\n### Features", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests.  Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.1.0-beta.6\n\n### Features", "## 4.9.1\n\n### Features\n\n- [#6781](https://github.com/gradio-app/gradio/pull/6781) [`a807ede`](https://github.com/gradio-app/gradio/commit/a807ede818e0690949aca41020e75a96f0110ece) - Fix backend tests on Windows.  Thanks [@abidlabs](https://github.com/abidlabs)!\n\n### Fixes", "## 4.9.1\n\n### Features\n\n- [#6781](https://github.com/gradio-app/gradio/pull/6781) [`a807ede`](https://github.com/gradio-app/gradio/commit/a807ede818e0690949aca41020e75a96f0110ece) - Fix backend tests on Windows.  Thanks [@abidlabs](https://github.com/abidlabs)!\n\n### Fixes"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies that a test accompanying a bug fix in Gradio should be one that failed before the fix and passes afterwards, which aligns with the reference answer that specifies a dynamic code test. The response goes beyond the reference answer by mentioning the possibility of using a linting rule or a new type of test, which is not incorrect but provides additional context not present in the reference answer. The mention of exceptions to the rule is also not incorrect but is additional information not required by the reference answer. Overall, the response is mostly correct, accurate, and factual, but includes extra information not necessary to answer the question directly."}, {"question": "How can you force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate?\n", "true_answer": "By passing `fp16=True` to the Accelerator init.", "source_doc": "huggingface/blog/blob/main/accelerate-library.md", "generated_answer": "To force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate, you can set the `mixed_precision` parameter to either \"fp16\" or \"bf16\" when calling the `prepare` method on your training objects. Here's an example:\n\n```python\nfrom accelerate import Accelerator\n\naccelerator = Accelerator(mixed_precision=\"fp16\")\n", "retrieved_docs": ["To enable mixed precision training, set the `fp16` flag to `True`:\n\n```py\ntraining_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\n```\n\nIf you prefer to use \ud83e\udd17 Accelerate, find the \ud83e\udd17 Accelerate example [further in this guide](#using-accelerate). \n\n### BF16\n\nIf you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While \nbf16 has a worse precision than fp16, it has a much bigger dynamic range. In fp16 the biggest number you can have \nis `65535` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which \nis about the same as fp32 - because both have 8-bits used for the numerical range.\n\nYou can enable BF16 in the \ud83e\udd17 Trainer with:\n\n```python\ntraining_args = TrainingArguments(bf16=True, **default_args)\n```\n\n### TF32", "<Tip>\n\nNote: when using mixed precision with a small model and a large batch size, there will be some memory savings but with a \nlarge model and a small batch size, the memory use will be larger.\n\n</Tip>\n\nYou can combine the above methods to get a cumulative effect. These techniques are available to you whether you are \ntraining your model with [`Trainer`] or writing a pure PyTorch loop, in which case you can [configure these optimizations \nwith \ud83e\udd17 Accelerate](#using-accelerate).\n\nIf these methods do not result in sufficient gains, you can explore the following options: \n* [Look into building your own custom Docker container with efficient softare prebuilds](#efficient-software-prebuilds)\n* [Consider a model that uses Mixture of Experts (MoE)](#mixture-of-experts)\n* [Convert your model to BetterTransformer to leverage PyTorch native attention](#using-pytorch-native-attention)", "For example, to speedup training with mixed precision using the bf16 format, add the `--mixed_precision` parameter to the training command:\n\n```bash\naccelerate launch train_unconditional.py \\\n  --mixed_precision=\"bf16\"\n```\n\nSome basic and important parameters to specify include:\n\n- `--dataset_name`: the name of the dataset on the Hub or a local path to the dataset to train on\n- `--output_dir`: where to save the trained model\n- `--push_to_hub`: whether to push the trained model to the Hub\n- `--checkpointing_steps`: frequency of saving a checkpoint as the model trains; this is useful if training is interrupted, you can continue training from that checkpoint by adding `--resume_from_checkpoint` to your training command\n\nBring your dataset, and let the training script handle everything else!\n\n## Training script", "For example, to speedup training with mixed precision using the bf16 format, add the `--mixed_precision` parameter to the training command:\n\n```bash\naccelerate launch train_text_to_image_sdxl.py \\\n  --mixed_precision=\"bf16\"\n```\n\nMost of the parameters are identical to the parameters in the [Text-to-image](text2image#script-parameters) training guide, so you'll focus on the parameters that are relevant to training SDXL in this guide.", "For example, to speedup training with mixed precision using the fp16 format, add the `--mixed_precision` parameter to the training command:\n\n```bash\naccelerate launch train_text_to_image.py \\\n  --mixed_precision=\"fp16\"\n```\n\nSome basic and important parameters include:\n\n- `--pretrained_model_name_or_path`: the name of the model on the Hub or a local path to the pretrained model\n- `--dataset_name`: the name of the dataset on the Hub or a local path to the dataset to train on\n- `--image_column`: the name of the image column in the dataset to train on\n- `--caption_column`: the name of the text column in the dataset to train on\n- `--output_dir`: where to save the trained model\n- `--push_to_hub`: whether to push the trained model to the Hub\n- `--checkpointing_steps`: frequency of saving a checkpoint as the model trains; this is useful if for some reason training is interrupted, you can continue training from that checkpoint by adding `--resume_from_checkpoint` to your training command", "```py\n>>> from accelerate import Accelerator\n\n>>> accelerator = Accelerator()\n```\n\n## Prepare to accelerate\n\nThe next step is to pass all the relevant training objects to the [`~accelerate.Accelerator.prepare`] method. This includes your training and evaluation DataLoaders, a model and an optimizer:\n\n```py\n>>> train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n...     train_dataloader, eval_dataloader, model, optimizer\n... )\n```\n\n## Backward\n\nThe last addition is to replace the typical `loss.backward()` in your training loop with \ud83e\udd17 Accelerate's [`~accelerate.Accelerator.backward`]method:\n\n```py\n>>> for epoch in range(num_epochs):\n...     for batch in train_dataloader:\n...         outputs = model(**batch)\n...         loss = outputs.loss\n...         accelerator.backward(loss)\n\n...         optimizer.step()\n...         lr_scheduler.step()\n...         optimizer.zero_grad()\n...         progress_bar.update(1)\n```", "Now, we can launch training:\n\n```bash\naccelerate launch --mixed_precision=\"fp16\" train_instruct_pix2pix.py \\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\n    --dataset_name=$DATASET_ID \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=256 --random_flip \\\n    --train_batch_size=4 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n    --max_train_steps=15000 \\\n    --checkpointing_steps=5000 --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 --max_grad_norm=1 --lr_warmup_steps=0 \\\n    --conditioning_dropout_prob=0.05 \\\n    --mixed_precision=fp16 \\\n    --seed=42 \\\n    --push_to_hub\n```\n\nAdditionally, we support performing validation inference to monitor training progress\nwith Weights and Biases. You can enable this feature with `report_to=\"wandb\"`:"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "The response provided is somewhat correct. The user correctly identifies that mixed precision training can be set to \"fp16\" or \"bf16\", which is accurate. However, the response incorrectly states that the `mixed_precision` parameter is set when calling the `prepare` method, while the reference answer indicates that mixed precision is enabled by passing `fp16=True` to the Accelerator initialization. This suggests that the user's response may not accurately reflect the current API of \ud83e\udd17 Accelerate or may be based on a misunderstanding of the initialization process. Therefore, the response is only partially correct and does not fully align with the reference answer."}, {"question": "What is the range of parameters for the LLaMA models?\n", "true_answer": "7B to 65B parameters", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/llama.md", "generated_answer": "Answer: The LLaMA models range from 7B to 65B parameters, as stated in the overview section of the provided context.", "retrieved_docs": ["-->\n\n# LLaMA\n\n## Overview\n\nThe LLaMA model was proposed in [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. It is a collection of foundation language models ranging from 7B to 65B parameters.\n\nThe abstract from the paper is the following:", "## The LLaMA model\n\nWhen doing RLHF, it is important to start with a capable model: the RLHF step is only a fine-tuning step to align the model with how we want to interact with it and how we expect it to respond.  Therefore, we choose to use the recently introduced and performant [LLaMA models](https://arxiv.org/abs/2302.13971). The LLaMA models are the latest large language models developed by Meta AI. They come in sizes ranging from 7B to 65B parameters and were trained on between 1T and 1.4T tokens, making them very capable. We use the 7B model as the base for all the following steps!\nTo access the model, use the [form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform) from Meta AI.\n\n## Stack Exchange dataset", "*we\u2019re currently running evaluation of the Llama 2 70B (non chatty version). This table will be updated with the results.\n\n\n## Demo\n\nYou can easily try the Big Llama 2 Model (70 billion parameters!) in [this Space](https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI) or in the playground embedded below:\n\n<script type=\"module\" src=\"https://gradio.s3-us-west-2.amazonaws.com/3.37.0/gradio.js\"> </script>\n<gradio-app theme_mode=\"light\" space=\"ysharma/Explore_llamav2_with_TGI\"></gradio-app>\n\nUnder the hood, this playground uses Hugging Face's [Text Generation Inference](https://github.com/huggingface/text-generation-inference), the same technology that powers [HuggingChat](https://huggingface.co/chat/), and which we'll share more in the following sections.", "The first model family in this series was the [LLaMA](https://huggingface.co/papers/2302.13971) family, released by Meta AI. The explicit objective of the researchers was to train a set of models of various sizes with the best possible performances for a given computing budget. For one of the first times, the research team explicitly decided to consider not only the training budget but also the inference cost (for a given performance objective, how much does it cost to run inference with the model). In this perspective, they decided to train smaller models on even more data and for more steps than was usually done, thereby reaching higher performances at a smaller model size (the trade-off being training compute efficiency). The biggest model in the Llama 1 family is a 65B parameters model trained on 1.4T tokens, while the smaller models (resp. 6 and 13B parameters) were trained on 1T tokens", "## Efficient training strategies\n\nEven training the smallest LLaMA model requires an enormous amount of memory. Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam optimizer (see the [performance docs](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer) in Transformers for more info). So a 7B parameter model would use `(2+8)*7B=70GB` just to fit in memory and would likely need more when you compute intermediate values such as attention scores. So you couldn\u2019t train the model even on a single 80GB A100 like that. You can use some tricks, like more efficient optimizers of half-precision training, to squeeze a bit more into memory, but you\u2019ll run out sooner or later.\n\nAnother option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques, such as the [`peft`](https://github.com/huggingface/peft) library, which can perform Low-Rank Adaptation (LoRA) on a model loaded in 8-bit.", "Increasing Llama 2\u2019s 4k context window to Code Llama\u2019s 16k (that can extrapolate up to 100k) was possible due to recent developments in RoPE scaling. The community found that Llama\u2019s position embeddings can be interpolated linearly or in the frequency domain, which eases the transition to a larger context window through fine-tuning. In the case of Code Llama, the frequency domain scaling is done with a slack: the fine-tuning length is a fraction of the scaled pretrained length, giving the model powerful extrapolation capabilities. \n\n![Training Process](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/160_codellama/training-process.jpg \"Training Process\")\n\nAll models were initially trained with 500 billion tokens on a near-deduplicated dataset of publicly available code. The dataset also contains some natural language datasets, such as discussions about code and code snippets. Unfortunately, there is not more information about the dataset.", "*Note: all models are compiled with a maximum sequence length of 2048.*\n\nThe `llama2 7B` \"budget\" model is meant to be deployed on `inf2.xlarge` instance that has only one neuron device, and enough `cpu` memory to load the model.\n\nAll other models are compiled to use the full extent of cores available on the `inf2.48xlarge` instance.\n\n*Note: please refer to the [inferentia2 product page](https://aws.amazon.com/ec2/instance-types/inf2/) for details on the available instances.*\n\nWe created two \"latency\" oriented configurations for the `llama2 7B` and `llama2 13B` models that can serve only one request at a time, but at full speed.\n\nWe also created two \"throughput\" oriented configurations to serve up to four requests in parallel.\n\nTo evaluate the models, we generate tokens up to a total sequence length of 1024, starting from\n256 input tokens (i.e. we generate 256, 512 and 768 tokens).\n\n*Note: the \"budget\" model numbers are reported but not included in the graphs for better readability.*"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating the range of parameters for the LLaMA models as being from 7B to 65B."}, {"question": "What is the purpose of tokenizers in the NLP pipeline?\n", "true_answer": "To translate text into data that can be processed by the model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter2/4.mdx", "generated_answer": "The purpose of tokenizers in the NLP pipeline is to translate text inputs into numerical data that can be processed by machine learning models. Models can only process numbers, so tokenizers need to convert text inputs into a meaningful and smallest representation that can be understood by the model. This is achieved through various algorithms and models, such as BPE, Unigram, WordLevel, and WordPiece, which split text into tokens and map them to their corresponding IDs in the vocabulary of", "retrieved_docs": ["{/if}\n\n<Youtube id=\"VFp38yj8h3A\"/>\n\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. \n\nIn NLP tasks, the data that is generally processed is raw text. Here's an example of such text:\n\n```\nJim Henson was a puppeteer\n```\n\nHowever, models can only process numbers, so we need to find a way to convert the raw text to numbers. That's what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation \u2014 that is, the one that makes the most sense to the model \u2014 and, if possible, the smallest representation.\n\nLet's take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization.", "## Model\n\nOnce the input texts are normalized and pre-tokenized, the\n`Tokenizer` applies the model on the\npre-tokens. This is the part of the pipeline that needs training on your\ncorpus (or that has been trained if you are using a pretrained\ntokenizer).\n\nThe role of the model is to split your \"words\" into tokens, using the\nrules it has learned. It's also responsible for mapping those tokens to\ntheir corresponding IDs in the vocabulary of the model.\n\nThis model is passed along when intializing the\n`Tokenizer` so you already know how to\ncustomize this part. Currently, the \ud83e\udd17 Tokenizers library supports:\n\n-   `models.BPE`\n-   `models.Unigram`\n-   `models.WordLevel`\n-   `models.WordPiece`\n\nFor more details about each model and its behavior, you can check\n[here](components#models)\n\n## Post-Processing\n\nPost-processing is the last step of the tokenization pipeline, to\nperform any additional transformation to the\n`Encoding` before it's returned, like\nadding potential special tokens.", "This chapter will begin with an end-to-end example where we use a model and a tokenizer together to replicate the `pipeline()` function introduced in [Chapter 1](/course/chapter1). Next, we'll discuss the model API: we'll dive into the model and configuration classes, and show you how to load a model and how it processes numerical inputs to output predictions. \n\nThen we'll look at the tokenizer API, which is the other main component of the `pipeline()` function. Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. Finally, we'll show you how to handle sending multiple sentences through a model in a prepared batch, then wrap it all up with a closer look at the high-level `tokenizer()` function.", "{/if}\n\nIn this section we will take a closer look at the capabilities of the tokenizers in \ud83e\udd17 Transformers. Up to now we have only used them to tokenize inputs or decode IDs back into text, but tokenizers -- especially those backed by the \ud83e\udd17 Tokenizers library -- can do a lot more. To illustrate these additional features, we will explore how to reproduce the results of the `token-classification` (that we called `ner`) and `question-answering` pipelines that we first encountered in [Chapter 1](/course/chapter1).\n\n<Youtube id=\"g8quOxoqhHQ\"/>", "In this chapter, you will learn how to train a brand new tokenizer on a corpus of texts, so it can then be used to pretrain a language model. This will all be done with the help of the [\ud83e\udd17 Tokenizers](https://github.com/huggingface/tokenizers) library, which provides the \"fast\" tokenizers in the [\ud83e\udd17 Transformers](https://github.com/huggingface/transformers) library. We'll take a close look at the features that this library provides, and explore how the fast tokenizers differ from the \"slow\" versions.\n\nTopics we will cover include:\n\n* How to train a new tokenizer similar to the one used by a given checkpoint on a new corpus of texts\n* The special features of fast tokenizers\n* The differences between the three main subword tokenization algorithms used in NLP today\n* How to build a tokenizer from scratch with the \ud83e\udd17 Tokenizers library and train it on some data", "he fast tokenizers of the Transformers library are fast, but they also implement features that will be super useful for data pre-processing and post-processing. Let's have a look at them! First, let's have a look at the usual output of a tokenizer. We get input IDs that correspond to tokens, but we lose a lot of information in the process. For instance, here the tokenization is the same for the two sentences, even if one has several more spaces than the other. Just having the input IDs is thus not enough if we want to match some tokens with a span of text (something we will need to do when tackling question answering for instance). It's also difficult to know when two tokens belong to the same word or not: it looks easy when you just look at the output of a BERT tokenizer, we just need to look for the ##. But other tokenizers have different ways to tokenize parts of words", "There are a few general tokenization algorithms, and a lot of different normalization and pre-processing steps that are crucial to using the model effectively. The transformers library made the decision to abstract all those operations in the same library (`tokenizers`), and represent the decisions as configuration files that are stored in the Hub alongside the model. For example, this is an excerpt from the configuration of the Llama 2 tokenizer that describes _just the normalization step_:\n\n```\n  \"normalizer\": {\n    \"type\": \"Sequence\",\n    \"normalizers\": [\n      {\n        \"type\": \"Prepend\",\n        \"prepend\": \"\u2581\"\n      },\n      {\n        \"type\": \"Replace\",\n        \"pattern\": {\n          \"String\": \" \"\n        },\n        \"content\": \"\u2581\"\n      }\n    ]\n  },\n```"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct and factual. It accurately states that tokenizers are used to translate text inputs into numerical data, which is essential for processing by machine learning models. The response goes into detail about how tokenizers work, mentioning various algorithms and models like BPE, Unigram, WordLevel, and WordPiece, which are indeed used to split text into tokens. However, the response could be more concise to match the reference answer's brevity and directness. The reference answer summarizes the purpose of tokenizers without going into the specifics of the algorithms used. Therefore, the response could be improved by focusing more on the core purpose of tokenizers rather than the methods they use."}, {"question": "What is the purpose of the Safety Checker in the Diffusers library?\n", "true_answer": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md", "generated_answer": "The purpose of the Safety Checker in the Diffusers library is to check generated outputs against known hardcoded NSFW content and flag inappropriate content during inference. This component, implemented in the `safety_checker` argument of the `DiffusionPipeline` class, helps users interact with diffusion models responsibly and ethically by mitigating the potential risks and dangers associated with the generation of harmful and NSFW content. The Safety Checker is optional, and users", "retrieved_docs": ["### Safety checker\n\nDiffusion models like Stable Diffusion can generate harmful content, which is why \ud83e\udde8 Diffusers has a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) to check generated outputs against known hardcoded NSFW content. If you'd like to disable the safety checker for whatever reason, pass `None` to the `safety_checker` argument:\n\n```python\nfrom diffusers import DiffusionPipeline", "- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repository\u2019s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.", "- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.", "## Ethics and safety\n\nGenerative models are cool, but they also have the ability to produce harmful and NSFW content. To help users interact with these models responsibly and ethically, we\u2019ve added a [`safety_checker`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) component that flags inappropriate content generated during inference. Model creators can choose to incorporate this component into their models if they want.", "```python\nfrom diffusers import DiffusionPipeline\n\nrepo_id = \"runwayml/stable-diffusion-v1-5\"\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id, safety_checker=None, use_safetensors=True)\n\"\"\"\nYou have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide by the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend keeping the safety filter enabled in all public-facing circumstances, disabling it only for use cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n\"\"\"\n```\n\n### Reuse components across pipelines", "# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:", "## Tips\n\nUse the `safety_concept` property of [`StableDiffusionPipelineSafe`] to check and edit the current safety concept:\n\n```python\n>>> from diffusers import StableDiffusionPipelineSafe\n\n>>> pipeline = StableDiffusionPipelineSafe.from_pretrained(\"AIML-TUDA/stable-diffusion-safe\")\n>>> pipeline.safety_concept\n'an image showing hate, harassment, violence, suffering, humiliation, harm, suicide, sexual, nudity, bodily fluids, blood, obscene gestures, illegal activity, drug use, theft, vandalism, weapons, child abuse, brutality, cruelty'\n```\nFor each image generation the active concept is also contained in [`StableDiffusionSafePipelineOutput`].\n\nThere are 4 configurations (`SafetyConfig.WEAK`, `SafetyConfig.MEDIUM`, `SafetyConfig.STRONG`, and `SafetyConfig.MAX`) that can be applied:\n\n```python\n>>> from diffusers import StableDiffusionPipelineSafe\n>>> from diffusers.pipelines.stable_diffusion_safe import SafetyConfig"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided captures the essence of the Safety Checker's purpose in the Diffusers library, which is to flag inappropriate content and help users interact with diffusion models responsibly. It correctly identifies that the Safety Checker operates during inference and is concerned with mitigating risks associated with harmful content generation. However, the response does not mention the specific mechanism of comparing class probabilities of harmful concepts in the embedding space against an image, as detailed in the reference answer. This omission means the response is not completely accurate in describing the technical workings of the Safety Checker. Therefore, the response is mostly correct but lacks some factual details provided in the reference answer."}, {"question": "What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n", "true_answer": "HfApi", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/community.md", "generated_answer": "The Python class that allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub is `HfApi`. This class is part of the `huggingface_hub` library and provides a Python interface to interact with Pull Requests and Discussions on the Hub. You can use the `get_repo_discussions` method of this class to retrieve a list of Discussions, and you can use the `get_dis", "retrieved_docs": ["!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Interact with Discussions and Pull Requests \n\nThe `huggingface_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub.\nVisit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood.\n\n## Retrieve Discussions and Pull Requests from the Hub\n\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(repo_id=\"bigscience/bloom\"):\n...     print(f\"{discussion.num} - {discussion.title}, pr: {discussion.is_pull_request}\")", "![Discussions on the Hugging Face Hub](assets/76_community_update/new-discussion.png)\n\n[Discussions](https://huggingface.co/gpt2/discussions?type=discussion) allow community members ask and answer questions as well as share their ideas and suggestions directly with the repository owners and the community. Anyone can create and participate in discussions in the community tab of a repository.\n\n## Pull requests\n\n![Pull requests on the Hugging Face Hub](assets/76_community_update/new-pr.png)\n\n[Pull requests](https://huggingface.co/gpt2/discussions?type=pull_request) allow community members open, comment, merge, or close pull requests directly from the website. The easiest way to open a pull request is to use the \"Collaborate\" button in the \"Files and versions\" tab. It will let you do single file contributions very easily.", "## What should I use ? And when ?\n\nOverall, the **HTTP-based approach is the recommended way to use** `huggingface_hub`\nin all cases. [`HfApi`] allows to pull and push changes, work with PRs, tags and branches, interact with discussions and much more. Since the `0.16` release, the http-based methods can also run in the background, which was the last major advantage of the [`Repository`] class.\n\nHowever, not all git commands are available through [`HfApi`]. Some may never be implemented, but we are always trying to improve and close the gap. If you don't see your use case covered, please open [an issue on Github](https://github.com/huggingface/huggingface_hub)! We welcome feedback to help build the \ud83e\udd17 ecosystem with and for our users.", "```python\n>>> from huggingface_hub import get_repo_discussions\n>>> discussions_list = list(get_repo_discussions(repo_id=\"bert-base-uncased\"))\n```\n\nThe [`Discussion`] object returned by [`HfApi.get_repo_discussions`] contains high-level overview of the\nDiscussion or Pull Request. You can also get more detailed information using [`HfApi.get_discussion_details`]:\n\n```python\n>>> from huggingface_hub import get_discussion_details", ">>> create_pull_request(\n...     repo_id=\"username/repo-name\",\n...     title=\"Hi from the huggingface_hub library!\",\n...     token=\"<insert your access token here>\",\n... )\nDiscussionWithDetails(..., is_pull_request=True)\n```\n\nManaging Pull Requests and Discussions can be done entirely with the [`HfApi`] class. For example:\n\n    * [`comment_discussion`] to add comments\n    * [`edit_discussion_comment`] to edit comments\n    * [`rename_discussion`] to rename a Discussion or Pull Request \n    * [`change_discussion_status`] to open or close a Discussion / Pull Request \n    * [`merge_pull_request`] to merge a Pull Request \n\n\nVisit the [`HfApi`] documentation page for an exhaustive reference of all available methods.\n\n## Push changes to a Pull Request\n\n*Coming soon !*\n\n## See also\n\nFor a more detailed reference, visit the [Discussions and Pull Requests](../package_reference/community) and the [hf_api](../package_reference/hf_api) documentation page.", "--\ntitle: Introducing Pull Requests and Discussions \ud83e\udd73\nthumbnail: /blog/assets/76_community_update/thumbnail.png\n---\n\n# Introducing Pull Requests and Discussions \ud83e\udd73\n\n![Pull requests and discussions on the hub](assets/76_community_update/community-update.png)\n\nWe are thrilled to announce the release of our latest collaborative features: pull requests and discussions on the Hugging Face Hub!\n\nPull requests and discussions are available today under the [community tab](https://huggingface.co/gpt2/discussions) for all repository types: models, datasets, and Spaces. Any member of the community can create and participate in discussions and pull requests, facilitating collaborations not only within teams, but also with everyone else in the community!\n\nIt's the biggest update ever done to the Hub, and we can't wait to see the community members start collaborating with it \ud83e\udd29.", "## Repository: the historical git-based approach\n\nAt first, `huggingface_hub` was mostly built around the [`Repository`] class. It provides\nPython wrappers for common `git` commands such as `\"git add\"`, `\"git commit\"`, `\"git push\"`,\n`\"git tag\"`, `\"git checkout\"`, etc.\n\nThe library also helps with setting credentials and tracking large files, which are often\nused in machine learning repositories. Additionally, the library allows you to execute its\nmethods in the background, making it useful for uploading data during training."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies `HfApi` as the Python class used to interact with the Hugging Face Hub, which is in line with the reference answer. The additional information provided about the `huggingface_hub` library and the specific methods `get_repo_discussions` and `get_dis` (although incomplete) does not contradict the reference answer and can be seen as an elaboration on how to use the `HfApi` class. Therefore, the response is completely correct, accurate, and factual."}, {"question": "What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n", "true_answer": "Skops", "source_doc": "huggingface/blog/blob/main/skops.md", "generated_answer": "The name of the new library introduced by Hugging Face for hosting scikit-learn models is Skops. This information can be found in the context provided, specifically in the section titled \"Introducing Skops\" where it is stated that \"With Skops, you can host your scikit-learn models on the Hugging Face Hub...\".", "retrieved_docs": ["--\ntitle: \"Introducing Skops\"\nthumbnail: /blog/assets/94_skops/introducing_skops.png\nauthors:\n- user: merve\n- user: adrin\n- user: BenjaminB\n---\n\n# Introducing Skops\n\n\n##\u00a0Introducing Skops\n\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others.\n\nLet's go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production.\n\n```python\n# let's import the libraries first\nimport sklearn\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split", "Using Keras at Hugging Face\n\n`keras` is an open-source machine learning library that uses a consistent and simple API to build models leveraging TensorFlow and its ecosystem.\n\n## Exploring Keras in the Hub\n\nYou can find over 200 `keras` models by filtering at the left of the [models page](https://huggingface.co/models?library=keras&sort=downloads).\n\nAll models on the Hub come up with useful feature:\n1. An automatically generated model card with a description, a plot of the model, and more.\n2. Metadata tags that help for discoverability and contain information such as license.\n3. If provided by the model owner, TensorBoard logs are hosted on the Keras repositories.\n\n\n## Using existing models\n\nThe `huggingface_hub` library is a lightweight Python client with utility functions to download models from the Hub.\n\n```\npip install huggingface_hub[\"tensorflow\"]\n```", "## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive.", "## Support for third-party libraries\n\nCentral to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from \ud83e\udd17 Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.\n\nWhen you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement.", "---\n\n## Welcome to the huggingface_hub library\n\nThe `huggingface_hub` library allows you to interact with the [Hugging Face Hub](https://huggingface.co/), a platform democratizing open-source Machine Learning for creators and collaborators. Discover pre-trained models and datasets for your projects or play with the thousands of machine learning apps hosted on the Hub. You can also create and share your own models, datasets and demos with the community. The `huggingface_hub` library provides a simple way to do all these things with Python.\n\n## Key features", "Using fastai at Hugging Face\n\n`fastai` is an open-source Deep Learning library that leverages PyTorch and Python to provide high-level components to train fast and accurate neural networks with state-of-the-art outputs on text, vision, and tabular data. \n\n## Exploring fastai in the Hub\n\nYou can find `fastai` models by filtering at the left of the [models page](https://huggingface.co/models?library=fastai&sort=downloads).\n\nAll models on the Hub come up with the following features:\n1. An automatically generated model card with a brief description and metadata tags that help for discoverability.\n2. An interactive widget you can use to play out with the model directly in the browser (for Image Classification)\n3. An Inference API that allows to make inference requests (for Image Classification).\n\n\n## Using existing models\n\nThe `huggingface_hub` library is a lightweight Python client with utlity functions to download models from the Hub.\n\n```bash\npip install huggingface_hub[\"fastai\"]\n```", "Hugging Face is a collaborative Machine Learning platform in which the community has shared over 150,000 models, 25,000 datasets, and 30,000 ML apps. The Hub has model and dataset versioning tools, including model cards and client-side libraries to automate the versioning process. However, only including a model card with hyperparameters is not enough to provide the best reproducibility; this is where notebooks can help. Alongside these models, datasets, and demos, the Hub hosts over 7,000 notebooks. These notebooks often document the development process of a model or a dataset and can provide guidance and tutorials showing how others can use these resources. We\u2019re therefore excited about our improved support for notebook hosting on the Hub. \n\n## What have we changed?"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response correctly identifies \"Skops\" as the new library introduced by Hugging Face for hosting scikit-learn models, which is in line with the reference answer. The additional context provided in the response does not detract from the accuracy of the answer."}, {"question": "What is the purpose of Textual Inversion?\n", "true_answer": "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/loaders/textual_inversion.md", "generated_answer": "Textual Inversion is a method that allows for personalizing text-to-image models like Stable Diffusion on one's own images using just a few examples. Its purpose is to teach the model a specific visual concept and integrate it into the generated image, giving the user more control over the generated images and allowing for tailoring the model towards specific concepts. This technique enables the model to learn a new concept from just a few sample images, which can then be shared with the community.", "retrieved_docs": ["## Textual inversion\n\n[Textual inversion](https://textual-inversion.github.io/) is very similar to DreamBooth and it can also personalize a diffusion model to generate certain concepts (styles, objects) from just a few images. This method works by training and finding new embeddings that represent the images you provide with a special word in the prompt. As a result, the diffusion model weights stay the same and the training process produces a relatively tiny (a few KBs) file.\n\nBecause textual inversion creates embeddings, it cannot be used on its own like DreamBooth and requires another model.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n```", "## Textual Inversion\n\nTextual Inversion lets you personalize a Stable Diffusion model on your own images with just 3-5 samples. With this tool, you can train a model on a concept, and then share the concept with the rest of the community! \n\n![image info](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/diffusers-2nd-month/textual_inversion.jpeg)\n\nIn just a couple of days, the community shared over 200 concepts! Check them out!", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script.\n\nThe example supports both single node and multi-node distributed training:\n\n### Single node training\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"", "# Textual inversion\n\n[[open-in-colab]]\n\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer).\n\nThis guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text_inversion) training guide.\n\nImport the necessary libraries:\n\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.utils import make_image_grid\n```", "* [Textual Inversion](../training/text_inversion), an algorithm that teaches a model a specific visual concept and integrates it into the generated image.\n* [DreamBooth](../training/dreambooth), a technique for generating personalized images of a subject given several input images of the subject.\n* [Guide](../training/text2image) to finetuning a Stable Diffusion model on your own dataset.\n* [Guide](../training/lora) to using LoRA, a memory-efficient technique for finetuning really large models faster.", "## Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It correctly identifies Textual Inversion as a method for personalizing text-to-image models and explains its purpose in teaching the model a specific visual concept with a few examples. It also accurately states that this allows the user more control over the generated images and the ability to tailor the model towards specific concepts. Furthermore, the response mentions that the learned concept can be shared with the community, which, while not explicitly stated in the reference answer, is a factual aspect of how Textual Inversion can be used. However, the response could be more concise and focused on the core purpose of Textual Inversion as a training method for learning new text embeddings, as stated in the reference answer."}, {"question": "What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n", "true_answer": "64", "source_doc": "huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md", "generated_answer": "The recommended multiple of batch size for fp16 data type on an A100 GPU is 2^N, where N is typically a multiple of 8 but can be higher depending on the hardware being used and the model's dtype. This is recommended for optimal performance and should be used when identifying the appropriate batch size for fine-tuning models on an A100 GPU using fp16 precision. (Source: Extract 2)", "retrieved_docs": ["This is a benchmark sample for the batch size = 1 case. The benchmark was run on a single NVIDIA A100-SXM4-80GB GPU. We used a prompt length of 512, and generated exactly 512 new tokens. The first row is the unquantized `fp16` baseline, while the other rows show memory consumption and performance using different AutoGPTQ kernels.", "Finally, if all of the above is still not enough, even after switching to a server-grade GPU like A100, consider moving \nto a multi-GPU setup. All these approaches are still valid in a multi-GPU setup, plus you can leverage additional parallelism \ntechniques outlined in the [multi-GPU section](perf_train_gpu_many). \n\n## Batch size choice\n\nTo achieve optimal performance, start by identifying the appropriate batch size. It is recommended to use batch sizes and \ninput/output neuron counts that are of size 2^N. Often it's a multiple of 8, but it can be \nhigher depending on the hardware being used and the model's dtype.", "*BS* is the batch size.\nThe Gaudi runs were performed in *bfloat16* precision and the A100 runs in *fp16* precision (more information [here](https://huggingface.co/docs/diffusers/optimization/fp16)). All runs were *single-device* runs.\n\n\n### Fine-tuning T5-3B\n\nWith 96 GB of memory per device, Gaudi2 enables running much bigger models. For instance, we managed to fine-tune T5-3B (containing 3 billion parameters) with gradient checkpointing being the only applied memory optimization. This is not possible on first-gen Gaudi.\n[Here](https://huggingface.co/regisss/t5-3b-summarization-gaudi-2/tensorboard?scroll=1#scalars) are the logs of this run where the model was fine-tuned on the CNN DailyMail dataset for text summarization using [this script](https://github.com/huggingface/optimum-habana/tree/main/examples/summarization).", "Bitsandbytes uses the fp4 scheme, with the compute in fp16.\n\n### Batch size = 1\n\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|None|None     |None|None      |None  |26.0         |36.958                |27.058            |29152.98        |\n| gptq | False | 4 | 128 | exllamav2 | 36.07 | 32.25 | 31.01 | 11313.75 |\n|gptq |False    |4   |128       |exllama|36.2         |33.711                |29.663            |10484.34        |\n|gptq |False    |4   |128       |autogptq-cuda-old|36.2         |46.44                 |21.53             |10344.62        |\n|bitsandbytes|None     |None|None      |None  |37.64        |52.00                 |19.23             |11018.36       |\n\n### Batch size = 2", "with batch size = 1: \n\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|fp16|None     |None|None      |None  |26.0         |36.958                |27.058            |29152.98        |\n|gptq |False    |4   |128       |exllama|36.2         |33.711                |29.663            |10484.34        |\n|bitsandbytes|None     |4|None      |None  |37.64        |52.00                 |19.23             |11018.36       |\n\nwith batch size = 16:", "In terms of efficiency, we calculate the frames per second (FPS) for each model using the average evaluation time across the entire dataset, considering pre and post-processing steps. Given the variability in GPU memory requirements for each model, we chose to evaluate with a batch size of 1 (this choice is also influenced by our pre-processing step, which we'll delve into later). However, it's worth noting that this approach may not align perfectly with real-world performance, as larger batch sizes (often containing several images), are commonly used for better efficiency. \n\nNext, we will provide tips on choosing the best model based on the metrics and point out which parameters may interfere with the results. Understanding these nuances is crucial, as this might spark doubts and discussions within the community.\n\n\n### How to pick the best model based on the metrics?", "Whereas before you couldn't even generate a batch of 4 images, now you can generate a batch of 8 images at ~3.5 seconds per image! This is probably the fastest you can go on a T4 GPU without sacrificing quality.\n\n## Quality\n\nIn the last two sections, you learned how to optimize the speed of your pipeline by using `fp16`, reducing the number of inference steps by using a more performant scheduler, and enabling attention slicing to reduce memory consumption. Now you're going to focus on how to improve the quality of generated images.\n\n### Better checkpoints"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response provides a formula (2^N) for determining the batch size multiple, which is not directly incorrect, but it lacks the specificity of the reference answer (64). The response does not directly state the recommended multiple, which is 64, and instead gives a general guideline that could lead to many possible multiples. This makes the response somewhat correct but not completely accurate as per the reference answer."}, {"question": "How do you run a Gradio Blocks app in reload mode using a Python IDE?\n", "true_answer": "Run `gradio run.py` in the terminal.", "source_doc": "gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md", "generated_answer": "To run a Gradio Blocks app in reload mode using a Python IDE, you need to replace \"python\" with \"gradio\" in the command you use to run your script. For example, instead of running \"python run.py\", you should run \"gradio run.py\" in the terminal. This will start the backend server in reload mode, which will automatically reload the app whenever you make changes to the script. Note that Gradio specifically looks for a", "retrieved_docs": ["This short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster.\n\n## Python IDE Reload \ud83d\udd25\n\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.\n\nInstead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:\n\nIn the terminal, run `gradio run.py`. That's it!", "## \ud83d\ude80 Run a Gradio app\n\nYou can get started by creating an `app.py` file in the root:\n\n```\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n   gr.Button()\n    \nif __name__ == \"__main__\":\n   demo.launch()\n```\n\nthen run:\n\n```\ngradio app.py\n```\n\nThis will start the backend server in reload mode, which will watch for changes in the `gradio` folder and reload the app if changes are made. By default, Gradio will launch on port 7860. You can also just use `python app.py`, but this won't automatically trigger updates. \n\n\nIf you're making frontend changes, start the frontend server:\n\n```\npnpm dev\n```\nThis will open a separate browser tab. By default, Gradio will launch this on port 9876. Any changes to the frontend will also reload automatically in the browser. For more information about developing in the frontend, you can refer to [js/README.md](js/README.md).\n\nWe also have demos of all our components in the `/gradio/demo` directory. To get our simple gradio Chatbot running locally:", "There is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as my_demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    my_demo.launch()\n```\n\nThen you would launch it in reload mode like this: `gradio run.py my_demo`.\n\nBy default, the Gradio use UTF-8 encoding for scripts. **For reload mode**, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:", "> [!TIP]\n > When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**\n\nYou'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. \n\nThe `Interface` class has three core arguments:", "Developing Faster with Auto-Reloading\n\n**Prerequisite**: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control).\n\nThis guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks.\n\n## Why Auto-Reloading?\n\nWhen you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes.\n\nTo make it faster and more convenient to write your code, we've made it easier to \"reload\" your Gradio apps instantly when you are developing in a **Python IDE** (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \"magic command\" that allows you to re-run cells faster if you use **Jupyter Notebooks** (or any similar environment like Colab).", "The demo below will open in a browser on [http://localhost:7860](http://localhost:7860) if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.\n\n$demo_hello_world_4\n\nType your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\n\nTip: When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**", "Which you could run like this: `gradio run.py --name Gretel`\n\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code. Meaning that this can be useful if you decide to [contribute to Gradio itself](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md) \u2705\n\n## Jupyter Notebook Magic \ud83d\udd2e\n\nWhat about if you use Jupyter Notebooks (or Colab Notebooks, etc.) to develop code? We got something for you too!\n\nWe've developed a **magic command** that will create and run a Blocks demo for you. To use this, load the gradio extension at the top of your notebook:\n\n`%load_ext gradio`\n\nThen, in the cell that you are developing your Gradio demo, simply write the magic command **`%%blocks`** at the top, and then write the layout and components like you would normally:\n\n```py\n%%blocks\n\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(f\"# Greetings {args.name}!\")\n    inp = gr.Textbox()\n    out = gr.Textbox()"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "The response provided is somewhat correct in that it suggests using \"gradio\" in the command to run the app, which is in line with the reference answer. However, the response inaccurately suggests replacing \"python\" with \"gradio\" in the command, which is not a standard or recognized way to run Gradio Blocks in reload mode. The correct command should be `gradio run.py`, as per the reference answer, which is not dependent on the Python IDE used. The response also incorrectly implies that the command should be run in the terminal, which is not necessarily part of using a Python IDE. The response is partially factual as it correctly identifies that Gradio has a reload mode and that changes to the script will automatically reload the app, but it fails to provide the precise command or method to enable this mode within a Python IDE. Therefore, the response does not fully meet the criteria for being completely correct, accurate, and factual."}, {"question": "What command is used to install the development version of the \ud83e\udd17 Transformers library in a Python virtual environment?\n", "true_answer": "pip install \"transformers[sentencepiece]\"", "source_doc": "huggingface/course/blob/main/chapters/en/chapter0/1.mdx", "generated_answer": "The command to install the development version of the \ud83e\udd17 Transformers library in a Python virtual environment is:\n\n```bash\npip install transformers[sentencepiece]\n```\n\nThis command should be run after creating a virtual environment and activating it. It will install a light version of \ud83e\udd17 Transformers with all the required dependencies for most use cases. Note that this command may take some time to complete. For more", "retrieved_docs": ["<Tip warning={true}>\n\nYou must keep the `transformers` folder if you want to keep using the library.\n\n</Tip>\n\nNow you can easily update your clone to the latest version of \ud83e\udd17 Transformers with the following command:\n\n```bash\ncd ~/transformers/\ngit pull\n```\n\nYour Python environment will find the `main` version of \ud83e\udd17 Transformers on the next run.\n\n## Install with conda\n\nInstall from the conda channel `huggingface`:\n\n```bash\nconda install -c huggingface transformers\n```\n\n## Cache setup\n\nPretrained models are downloaded and locally cached at: `~/.cache/huggingface/hub`. This is the default directory given by the shell environment variable `TRANSFORMERS_CACHE`. On Windows, the default directory is given by `C:\\Users\\username\\.cache\\huggingface\\hub`. You can change the shell environment variables shown below - in order of priority - to specify a different cache directory:", "This installs a very light version of \ud83e\udd17 Transformers. In particular, no specific machine learning frameworks (like PyTorch or TensorFlow) are installed. Since we'll be using a lot of different features of the library, we recommend installing the development version, which comes with all the required dependencies for pretty much any imaginable use case:\n\n```\n!pip install transformers[sentencepiece]\n```\n\nThis will take a bit of time, but then you'll be ready to go for the rest of the course!\n\n## Using a Python virtual environment[[using-a-python-virtual-environment]]\n\nIf you prefer to use a Python virtual environment, the first step is to install Python on your system. We recommend following [this guide](https://realpython.com/installing-python/) to get started.", "## Installation\n\n### With pip\n\nThis repository is tested on Python 3.8+, Flax 0.4.1+, PyTorch 1.10+, and TensorFlow 2.6+.\n\nYou should install \ud83e\udd17 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n\nFirst, create a virtual environment with the version of Python you're going to use and activate it.\n\nThen, you will need to install at least one of Flax, PyTorch, or TensorFlow.\nPlease refer to [TensorFlow installation page](https://www.tensorflow.org/install/), [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) and/or [Flax](https://github.com/google/flax#quick-install) and [Jax](https://github.com/google/jax#installation) installation pages regarding the specific installation command for your platform.", "```bash\npython -m venv .env\n```\n\nActivate the virtual environment. On Linux and MacOs:\n\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n\n```bash\n.env/Scripts/activate\n```\n\nNow you're ready to install \ud83e\udd17 Transformers with the following command:\n\n```bash\npip install transformers\n```\n\nFor CPU-support only, you can conveniently install \ud83e\udd17 Transformers and a deep learning library in one line. For example, install \ud83e\udd17 Transformers and PyTorch with:\n\n```bash\npip install 'transformers[torch]'\n```\n\n\ud83e\udd17 Transformers and TensorFlow 2.0:\n\n```bash\npip install 'transformers[tf-cpu]'\n```\n\n<Tip warning={true}>\n\nM1 / ARM Users\n    \nYou will need to install the following before installing TensorFLow 2.0\n```\nbrew install cmake\nbrew install pkg-config\n```\n\n</Tip>\n\n\ud83e\udd17 Transformers and Flax:\n\n```bash\npip install 'transformers[flax]'\n```\n\nFinally, check if \ud83e\udd17 Transformers has been properly installed by running the following command. It will download a pretrained model:", "Check if \ud83e\udd17 Transformers has been properly installed by running the following command:\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"\n```\n\n## Editable install\n\nYou will need an editable install if you'd like to:\n\n* Use the `main` version of the source code.\n* Contribute to \ud83e\udd17 Transformers and need to test changes in the code.\n\nClone the repository and install \ud83e\udd17 Transformers with the following commands:\n\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python will also search the folder you cloned to: `~/transformers/`.\n\n<Tip warning={true}>", "```bash\n   pip install -e \".[dev]\"\n   ```\n\n   If \ud83e\udd17 Transformers was already installed in the virtual environment, remove\n   it with `pip uninstall transformers` before reinstalling it in editable\n   mode with the `-e` flag.\n   \n   Depending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\n   failure with this command. If that's the case make sure to install the Deep Learning framework you are working with\n   (PyTorch, TensorFlow and/or Flax) then do:\n\n   ```bash\n   pip install -e \".[quality]\"\n   ```\n\n   which should be enough for most use cases.\n\n5. Develop the features in your branch.\n\n   As you work on your code, you should make sure the test suite\n   passes. Run the tests impacted by your changes like this:\n\n   ```bash\n   pytest tests/<TEST_TO_RUN>.py\n   ```\n\n   For more information about tests, check out the\n   [Testing](https://huggingface.co/docs/transformers/testing) guide.", "```bash\n   pip install -e \".[dev]\"\n   ```\n\n   If \ud83e\udd17 Transformers was already installed in the virtual environment, remove\n   it with `pip uninstall transformers` before reinstalling it in editable\n   mode with the `-e` flag.\n   \n   Depending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\n   failure with this command. If that's the case make sure to install the Deep Learning framework you are working with\n   (PyTorch, TensorFlow and/or Flax) then do:\n\n   ```bash\n   pip install -e \".[quality]\"\n   ```\n\n   which should be enough for most use cases.\n\n5. Develop the features in your branch.\n\n   As you work on your code, you should make sure the test suite\n   passes. Run the tests impacted by your changes like this:\n\n   ```bash\n   pytest tests/<TEST_TO_RUN>.py\n   ```\n\n   For more information about tests, check out the\n   [Testing](https://huggingface.co/docs/transformers/testing) guide."], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct in that it specifies the use of pip and includes the 'transformers[sentencepiece]' package, which is part of the correct command. However, the response fails to specify that the version being installed is the development version, which is a critical part of the instruction. The command given installs the latest stable version with sentencepiece, not the development version. Therefore, the response is not completely accurate as per the instruction given."}, {"question": "How can you install the Hugging Face Unity API in your Unity project?\n", "true_answer": "To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.", "source_doc": "huggingface/blog/blob/main/unity-api.md", "generated_answer": "To install the Hugging Face Unity API in your Unity project, follow these steps:\n\n1. Open your Unity project.\n2. Go to \"Window\" > \"Package Manager\".\n3. Click the \"+\" button and select \"Add Package from git URL\".\n4. Enter \"https://github.com/huggingface/unity-api.git\" and click \"Add\".\n5. Once the package is installed, the Hugging Face API", "retrieved_docs": ["--\ntitle: \"How to Install and Use the Hugging Face Unity API\"\nthumbnail: /blog/assets/124_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n\n# How to Install and Use the Hugging Face Unity API\n\n<!-- {authors} --> \n\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API.\n\n## Installation\n\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`", "## Step 6: Optionally, Download the Hugging Face Unity WebGL Template\n\nYou can enhance your game's appearance in a Space by downloading the Hugging Face Unity WebGL template, available [here](https://github.com/huggingface/Unity-WebGL-template-for-Hugging-Face-Spaces). Just download the repository and drop it in your project files.\n\nThen, in the Player Settings panel, switch the WebGL template to Hugging Face. To do so, in Player Settings, click \"Resolution and Presentation\", then select the Hugging Face WebGL template.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/6.png\">\n</figure> \n\n## Step 7: Change the Compression Format to Disabled\n\nIn the Player Settings panel, navigate to the \"Publishing Settings\" section and change the Compression Format to \"Disabled\".", "### Prerequisites\n\nThis tutorial assumes basic knowledge of Unity. It also requires you to have installed the [Hugging Face Unity API](https://github.com/huggingface/unity-api). For instructions on setting up the API, check out our [earlier blog post](https://huggingface.co/blog/unity-api).\n\n## Steps\n\n### 1. Set up the Scene\n\nIn this tutorial, we'll set up a very simple scene where the player can start and stop a recording, and the result will be converted to text.\n\nBegin by creating a Unity project, then creating a Canvas with four UI elements:\n\n1. **Start Button**: This will start the recording.\n2. **Stop Button**: This will stop the recording.\n3. **Text (TextMeshPro)**: This is where the result of the speech recognition will be displayed.\n\n### 2. Set up the Script\n\nCreate a script called `SpeechRecognitionTest` and attach it to an empty GameObject.", "## Usage Tips\n\n1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.\n2. Address slow response times or performance issues by changing model endpoints to lower resource models.\n\n## Conclusion\n\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!", "<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif\">\n</figure> \n\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).\n7. Test the API key by clicking `Test API key` in the API Wizard.\n8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.\n9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard.", "## Step 3: Open your Unity Project\n\nOpen the Unity project you want to host in your Space.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/3.png\">\n</figure> \n\n## Step 4: Switch the Build Target to WebGL\n\nNavigate to `File > Build Settings` and switch the Build Target to WebGL.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/4.png\">\n</figure> \n\n## Step 5: Open Player Settings\n\nIn the Build Settings window, click the \"Player Settings\" button to open the Player Settings panel.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/5.png\">\n</figure> \n\n## Step 6: Optionally, Download the Hugging Face Unity WebGL Template", "/* other code */\n```\n\n## Supported Tasks and Custom Models\n\nThe Hugging Face Unity API also currently supports the following tasks:\n\n- [Conversation](https://huggingface.co/tasks/conversational)\n- [Text Generation](https://huggingface.co/tasks/text-generation)\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\n- [Text Classification](https://huggingface.co/tasks/text-classification)\n- [Question Answering](https://huggingface.co/tasks/question-answering)\n- [Translation](https://huggingface.co/tasks/translation)\n- [Summarization](https://huggingface.co/tasks/summarization)\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\n\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\n\nTo use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\n\n## Usage Tips"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and follows the steps outlined in the reference answer. It accurately describes the process of installing the Hugging Face Unity API by accessing the Package Manager and adding the package from a git URL. However, the response is incomplete as it ends abruptly without finishing the last sentence. This leaves out any final steps or confirmation that the installation was successful, which is a minor factual inaccuracy due to the lack of completion. Based on the score rubric, the response is not completely correct as it is not a finished answer."}, {"question": "What is the pretraining objective of the Wav2Vec2 context network?\n", "true_answer": "The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks_explained.md", "generated_answer": "The pretraining objective of the Wav2Vec2 context network is a contrastive task, where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label). (Source: Extracted document 0)", "retrieved_docs": ["4. The pretraining objective of the context network is a *contrastive task*. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label).\n\nNow that wav2vec2 is pretrained, you can finetune it on your data for audio classification or automatic speech recognition!\n\n### Audio classification\n\nTo use the pretrained model for audio classification, add a sequence classification head on top of the base Wav2Vec2 model. The classification head is a linear layer that accepts the encoder's hidden states. The hidden states represent the learned features from each audio frame which can have varying lengths. To create one vector of fixed-length, the hidden states are pooled first and then transformed into logits over the class labels. The cross-entropy loss is calculated between the logits and target to find the most likely class.", "Using a novel contrastive pretraining objective, Wav2Vec2 learns\npowerful speech representations from more than 50.000 hours of unlabeled\nspeech. Similar, to [BERT\\'s masked language\nmodeling](http://jalammar.github.io/illustrated-bert/), the model learns\ncontextualized speech representations by randomly masking feature\nvectors before passing them to a transformer network.\n\n![wav2vec2\\_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/wav2vec2.png)\n\nFor the first time, it has been shown that pretraining, followed by\nfine-tuning on very little labeled speech data achieves competitive\nresults to state-of-the-art ASR systems. Using as little as 10 minutes\nof labeled data, Wav2Vec2 yields a word error rate (WER) of less than 5%\non the clean test set of\n[LibriSpeech](https://huggingface.co/datasets/librispeech_asr) - *cf.*\nwith Table 9 of the [paper](https://arxiv.org/pdf/2006.11477.pdf).", "[Wav2Vec2](https://huggingface.co/Graphcore/wav2vec2-base-ipu) is a pretrained self-supervised model for automatic speech recognition. Using a novel contrastive pretraining objective, Wav2Vec2 learns powerful speech representations from large amounts of unlabelled speech data, followed by fine-tuning on a small amount of transcribed speech data, outperforming the best semi-supervised methods while being conceptually simpler.\n\n## Hugging Face Optimum Graphcore: building on a solid partnership\n\nGraphcore joined the [Hugging Face Hardware Partner Program](https://huggingface.co/hardware) in 2021 as a founding member, with both companies sharing the common goal of lowering the barriers for innovators seeking to harness the power of machine intelligence.\n\nSince then, Graphcore and Hugging Face have worked together extensively to make training of transformer models on IPUs fast and easy, with the first Optimum Graphcore model (BERT) being made available last year.", "### Pretraining Wav2Vec2\n\nThe `run_pretrain.py` script allows one to pretrain a Wav2Vec2 model from scratch using Wav2Vec2's contrastive loss objective (see official [paper](https://arxiv.org/abs/2006.11477) for more information). \nIt is recommended to pre-train Wav2Vec2 with Trainer + Deepspeed (please refer to [this guide](https://huggingface.co/transformers/main/main_classes/deepspeed.html#deepspeed-trainer-integration) for more information).\n\nHere is an example of how you can use DeepSpeed ZeRO-2 to pretrain a small Wav2Vec2 model:", "### Create Wav2Vec2CTCTokenizer\n\nThe [pretrained Wav2Vec2 checkpoint](https://huggingface.co/facebook/wav2vec2-base) maps the speech signal to a\nsequence of context representations as illustrated in the figure above.\nA fine-tuned Wav2Vec2 checkpoint needs to map this sequence of context\nrepresentations to its corresponding transcription so that a linear\nlayer has to be added on top of the transformer block (shown in yellow).\nThis linear layer is used to classifies each context representation to a\ntoken class analogous how, *e.g.*, after pretraining a linear layer is\nadded on top of BERT\\'s embeddings for further classification - *cf.*\nwith *\\\"BERT\\\"* section of this [blog post](https://huggingface.co/blog/warm-starting-encoder-decoder).", "**Wav2Vec2** is a popular pre-trained model for speech recognition.\nReleased in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\nby Meta AI Research, the novel architecture catalyzed progress in\nself-supervised pretraining for speech recognition, *e.g.* [*G. Ng et\nal.*, 2021](https://arxiv.org/pdf/2104.03416.pdf), [*Chen et al*,\n2021](https://arxiv.org/abs/2110.13900), [*Hsu et al.*,\n2021](https://arxiv.org/abs/2106.07447) and [*Babu et al.*,\n2021](https://arxiv.org/abs/2111.09296). On the Hugging Face Hub,\nWav2Vec2's most popular pre-trained checkpoint currently amounts to\nover [**250,000** monthly\ndownloads](https://huggingface.co/facebook/wav2vec2-base-960h).", "**Wav2Vec2** is a popular pre-trained model for speech recognition.\nReleased in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\nby Meta AI Research, the novel architecture catalyzed progress in\nself-supervised pretraining for speech recognition, *e.g.* [*G. Ng et\nal.*, 2021](https://arxiv.org/pdf/2104.03416.pdf), [*Chen et al*,\n2021](https://arxiv.org/abs/2110.13900), [*Hsu et al.*,\n2021](https://arxiv.org/abs/2106.07447) and [*Babu et al.*,\n2021](https://arxiv.org/abs/2111.09296). On the Hugging Face Hub,\nWav2Vec2's most popular pre-trained checkpoint currently amounts to\nover [**250,000** monthly\ndownloads](https://huggingface.co/facebook/wav2vec2-base-960h).\n\nUsing Connectionist Temporal Classification (CTC), pre-trained\nWav2Vec2-like checkpoints are extremely easy to fine-tune on downstream\nspeech recognition tasks. In a nutshell, fine-tuning pre-trained\nWav2Vec2 checkpoints works as follows:"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It aligns well with the reference answer, providing a clear explanation of the pretraining objective of the Wav2Vec2 context network as a contrastive task. The response also adds a bit more detail by mentioning the encouragement for the model to find the most similar context vector and quantized speech unit, which is implied in the reference answer's mention of predicting the true quantized speech representation. This additional detail does not detract from the correctness or factual nature of the response."}, {"question": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n", "true_answer": "distilbert base uncased finetuned sst2 english", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md", "generated_answer": "The default checkpoint used by the sentiment analysis pipeline in the Transformers library is distilbert base uncased finetuned sst2 english. (Source: Extract 1)", "retrieved_docs": [". They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True", ". They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True", "Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this guide, you'll use the [`pipeline`] for sentiment analysis as an example:\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(\"sentiment-analysis\")\n```\n\nThe [`pipeline`] downloads and caches a default [pretrained model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) and tokenizer for sentiment analysis. Now you can use the `classifier` on your target text:\n\n```py\n>>> classifier(\"We are very happy to show you the \ud83e\udd17 Transformers library.\")\n[{'label': 'POSITIVE', 'score': 0.9998}]\n```\n\nIf you have more than one input, pass your inputs as a list to the [`pipeline`] to return a list of dictionaries:", "## Using a model for question answering[[using-a-model-for-question-answering]]\n\nLike with any other pipeline, we start by tokenizing our input and then send it through the model. The checkpoint used by default for the `question-answering` pipeline is [`distilbert-base-cased-distilled-squad`](https://huggingface.co/distilbert-base-cased-distilled-squad) (the \"squad\" in the name comes from the dataset on which the model was fine-tuned; we'll talk more about the SQuAD dataset in [Chapter 7](/course/chapter7/7)):\n\n{#if fw === 'pt'}\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\nmodel_checkpoint = \"distilbert-base-cased-distilled-squad\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n\ninputs = tokenizer(question, context, return_tensors=\"pt\")\noutputs = model(**inputs)\n```\n\n{:else}\n\n```py\nfrom transformers import AutoTokenizer, TFAutoModelForQuestionAnswering", "## The Transformers library\nI was a bit confused at the beginning when I downloaded the .h5 file. I thought it would be compatible with `tensorflow.keras.models.load_model`, but this wasn't the case. After a few minutes of research I was able to figure out that the file was a weights checkpoint rather than a Keras model.\nAfter that, I tried out the API that Hugging Face offers and read a bit more about the pipeline feature they offer. Since the results of the API & the pipeline were great, I decided that I could serve the model through the pipeline on my own server.\n\nBelow is the [official example](https://github.com/huggingface/transformers#quick-tour) from the Transformers GitHub page.\n\n```python\nfrom transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\nclassifier = pipeline('sentiment-analysis')\nclassifier('We are very happy to include pipeline into the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9978193640708923}]\n```", "This code snippet uses the [pipeline class](https://huggingface.co/docs/transformers/main_classes/pipelines) to make predictions from models available in the Hub. It uses the [default model for sentiment analysis](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english?text=I+like+you.+I+love+you) to analyze the list of texts `data` and it outputs the following results:\n\n```python\n[{'label': 'POSITIVE', 'score': 0.9998},\n {'label': 'NEGATIVE', 'score': 0.9991}]\n```\n\nYou can use a specific sentiment analysis model that is better suited to your language or use case by providing the name of the model. For example, if you want a sentiment analysis model for tweets, you can specify the [model id](https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis):\n\n```python\nspecific_model = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")\nspecific_model(data)\n```", "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, indicating that the information given is precise and to the point."}, {"question": "What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n", "true_answer": "To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.", "source_doc": "huggingface/optimum/blob/main/docs/source/notebooks.md", "generated_answer": "The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to demonstrate how to utilize DeepSpeed, a library designed for speed and scale in distributed training of large models with billions of parameters, to pre-train and fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi, which is a high-performance computing (HPC", "retrieved_docs": ["| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |", "| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |", "To go further, we are looking forward to using HPU graphs for training models even faster and to presenting how to use DeepSpeed ZeRO-3 on Gaudi2 to accelerate the training of your LLMs. Stay tuned!\n\nIf you are interested in accelerating your Machine Learning training and inference workflows using the latest AI hardware accelerators and software libraries, check out our [Expert Acceleration Program](https://huggingface.co/support). To learn more about Habana solutions, [read about our partnership and contact them here](https://huggingface.co/hardware/habana). To learn more about Hugging Face efforts to make AI hardware accelerators easy to use, check out our [Hardware Partner Program](https://huggingface.co/hardware).\n\n\n### Related Topics", "The Habana team recently introduced support for DeepSpeed-inference in SynapseAI 1.8, and thereby quickly enabled inference for 100+ billion parameter models. **For the 176-billion-parameter checkpoint, Gaudi2 is 1.42x faster than A100 80GB**. Smaller checkpoints present interesting results too. **Gaudi2 is 2.89x faster than A100 for BLOOMZ-7B!** It is also interesting to note that it manages to benefit from model parallelism whereas A100 is faster on a single device.\n\nWe also ran these models on first-gen Gaudi. While it is slower than Gaudi2, it is interesting from a price perspective as a DL1 instance on AWS costs approximately 13\\$ per hour. Latency for BLOOMZ-7B on first-gen Gaudi is 2.387 seconds. Thus, **first-gen Gaudi offers for the 7-billion checkpoint a better price-performance ratio than A100** which costs more than 30\\$ per hour!", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# DeepSpeed\n\n[DeepSpeed](https://www.deepspeed.ai/) is a library designed for speed and scale for distributed training of large models with billions of parameters. At its core is the Zero Redundancy Optimizer (ZeRO) that shards optimizer states (ZeRO-1), gradients (ZeRO-2), and parameters (ZeRO-3) across data parallel processes. This drastically reduces memory usage, allowing you to scale your training to billion parameter models. To unlock even more memory efficiency, ZeRO-Offload reduces GPU compute and memory by leveraging CPU resources during optimization.", "Papers:\n\n- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)\n- [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840)\n- [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)\n\nFinally, please, remember that, HuggingFace [`Trainer`] only integrates DeepSpeed, therefore if you\nhave any problems or questions with regards to DeepSpeed usage, please, file an issue with [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed/issues).", "## Benchmarks\n\nIn this section, we are going to provide an early benchmark of BLOOMZ on Gaudi2, first-generation Gaudi and Nvidia A100 80GB. Although these devices have quite a lot of memory, the model is so large that a single device is not enough to contain a single instance of BLOOMZ. To solve this issue, we are going to use [DeepSpeed](https://www.deepspeed.ai/), which is a deep learning optimization library that enables many memory and speed improvements to accelerate the model and make it fit the device. In particular, we rely here on [DeepSpeed-inference](https://arxiv.org/abs/2207.00032): it introduces several features such as [model (or pipeline) parallelism](https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism) to make the most of the available devices. For Gaudi2, we use [Habana's DeepSpeed fork](https://github.com/HabanaAI/deepspeed) that adds support for HPUs.\n\n\n### Latency"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provided is mostly correct and factual. It accurately captures the essence of the notebook's purpose, which is to demonstrate the use of DeepSpeed for training large models, specifically mentioning the 1.6B-parameter GPT2-XL model and its application for causal language modeling on Habana Gaudi. However, the response is somewhat verbose and includes additional details that are not present in the reference answer, such as the mention of \"pre-train and fine-tune,\" which, while correct, are not required for a full score based on the reference answer. The response could be more concise to match the reference answer more closely."}, {"question": "What command line module does PyTorch provide to run a script on multiple GPUs?\n", "true_answer": "torchrun", "source_doc": "huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md", "generated_answer": "PyTorch provides the `torch.distributed.launch` command line module to run a script on multiple GPUs. This can be invoked using the following command:\n\n```bash\npython -m torch.distributed.launch \\\n    --nproc_per_node <number_of_gpus> \\\n    <script_name.py> \\\n    <args>\n```\n\nReplace `<number_of_gpus", "retrieved_docs": ["However, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through *distributed* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\n\n## PyTorch Distributed Data Parallelism\n\nAs the name implies, `torch.distributed` is meant to work on *distributed* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both.\n\nTo convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)", "```bash\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\n```\n\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.\n\nNow let's talk about Accelerate, a library aimed to make this process more seameless and also help with a few best practices\n\n## \ud83e\udd17 Accelerate\n\n[Accelerate](https://huggingface.co/docs/accelerate) is a library designed to allow you to perform what we just did above, without needing to modify your code greatly. On top of this, the data pipeline innate to Accelerate can also improve performance to your code as well.\n\nFirst, let's wrap all of the above code we just performed into a single function, to help us visualize the difference:\n\n```python\ndef train_ddp(rank, world_size):\n    setup(rank, world_size)\n    # Build DataLoaders\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307), (0.3081))\n    ])", "## Run a script on a TPU\n\n<frameworkcontent>\n<pt>\nTensor Processing Units (TPUs) are specifically designed to accelerate performance. PyTorch supports TPUs with the [XLA](https://www.tensorflow.org/xla) deep learning compiler (see [here](https://github.com/pytorch/xla/blob/master/README.md) for more details). To use a TPU, launch the `xla_spawn.py` script and use the `num_cores` argument to set the number of TPU cores you want to use.", "### Script to compare pre-training with PyTorch on 8 GPU V100's\n\nFor comparison you can run the same pre-training with PyTorch on GPU. Note that we have to make use of `gradient_accumulation` \nbecause the maximum batch size that fits on a single V100 GPU is 32 instead of 128.\nHaving created the tokenzier and configuration in `norwegian-roberta-base`, we create the following symbolic links:\n\n```bash\nln -s ~/transformers/examples/pytorch/language-modeling/run_mlm.py ./\n```\n\n, set some environment variables:\n\n```bash\nexport NUM_GPUS=8\nexport TOKENIZERS_PARALLELISM=0\nexport MODEL_DIR=\"./norwegian-roberta-base\"\nmkdir -p ${MODEL_DIR}\n```\n\n, and can start training as follows:", "image = sd(prompt).images[0]\n    image.save(f\"./{'_'.join(prompt)}.png\")\n```\n\nTo run the distributed inference, call [`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn) to run the `run_inference` function on the number of GPUs defined in `world_size`:\n\n```py\ndef main():\n    world_size = 2\n    mp.spawn(run_inference, args=(world_size,), nprocs=world_size, join=True)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\nOnce you've completed the inference script, use the `--nproc_per_node` argument to specify the number of GPUs to use and call `torchrun` to run the script:\n\n```bash\ntorchrun run_distributed.py --nproc_per_node=2\n```", "To see all the possible command line options, run:\n\n```bash\npython finetune_trainer.py --help\n```\n\nFor multi-gpu training use `torch.distributed.launch`, e.g. with 2 gpus:\n```bash\ntorchrun --nproc_per_node=2  finetune_trainer.py ...\n```\n\n**At the moment, `Seq2SeqTrainer` does not support *with teacher* distillation.**\n\nAll `Seq2SeqTrainer`-based fine-tuning scripts are included in the `builtin_trainer` directory.\n\n#### TPU Training\n`Seq2SeqTrainer` supports TPU training with few caveats\n1. As `generate` method does not work on TPU at the moment, `predict_with_generate` cannot be used. You should use `--prediction_loss_only` to only calculate loss, and do not set `--do_predict` and `--predict_with_generate`.\n2. All sequences should be padded to be of equal length to avoid extremely slow training. (`finetune_trainer.py` does this automatically when running on TPU.)", "**To lanuch fine-tuninig on multiple GPUs:**\n\t\n\t```bash\n\tpython -m torch.distributed.launch \\\n\t\t--nproc_per_node 4 run_common_voice.py \\\n\t\t--model_name_or_path=\"facebook/wav2vec2-large-xlsr-53\" \\\n\t\t--dataset_config_name=\"tr\" \\ # use this argument to specify the language code\n\t\t--output_dir=./wav2vec2-large-xlsr-turkish-demo \\\n\t\t--overwrite_output_dir \\\n\t\t--num_train_epochs=\"5\" \\\n\t\t--per_device_train_batch_size=\"16\" \\\n\t\t--learning_rate=\"3e-4\" \\\n\t\t--warmup_steps=\"500\" \\\n\t\t--evaluation_strategy=\"steps\" \\\n\t\t--save_steps=\"400\" \\\n\t\t--eval_steps=\"400\" \\\n\t\t--logging_steps=\"400\" \\\n\t\t--save_total_limit=\"3\" \\\n\t\t--freeze_feature_extractor \\\n\t\t--feat_proj_dropout=\"0.0\" \\\n\t\t--layerdrop=\"0.1\" \\\n\t\t--gradient_checkpointing \\\n\t\t--fp16 \\\n\t\t--group_by_length \\\n\t\t--do_train --do_eval\n\t```\n\n\tThe above command will launch the training on 4 GPUs. Use the `--nproc_per_node` option to specify the number of GPUs."], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response correctly identifies `torch.distributed.launch` as the command line module provided by PyTorch to run a script on multiple GPUs, which is accurate. However, the reference answer indicates that \"torchrun\" is the correct response. It is important to note that `torch.distributed.launch` is the older way to launch distributed training, and `torchrun` is the newer recommended utility as of PyTorch 1.9. Therefore, the response is not completely up-to-date with the latest PyTorch recommendations but is still factually correct in the context of the functionality it describes. Based on the rubric, the response is mostly correct, accurate, and factual, but not completely, as it does not mention the newer `torchrun` utility."}, {"question": "What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n", "true_answer": "google/vit-base-patch16-224", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/image-classification-with-vision-transformers.md", "generated_answer": "The most popular vision transformer model on the Hugging Face Model Hub for image classification, as mentioned in the context, is `google/vit-base-patch16-224`. This model, trained on ImageNet images at a resolution of 224x224 pixels, was contributed by nielsr and the original code can be found on the GitHub repository of Google Research's Vision Transformer (ViT). It is supported by an example script", "retrieved_docs": ["Let's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started).\n\n## Step 1 \u2014 Choosing a Vision Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks.\n\nExpand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.\n\nAt the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo.\n\n## Step 2 \u2014 Loading the Vision Transformer Model with Gradio", "## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive.", "This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code (written in JAX) can be\nfound [here](https://github.com/google-research/vision_transformer).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with ViT Hybrid.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`ViTHybridForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## ViTHybridConfig", "1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (\u6765\u81ea Google AI) \u4f34\u968f\u8bba\u6587 [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) \u7531 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby \u53d1\u5e03\u3002\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (\u6765\u81ea UCLA NLP) \u4f34\u968f\u8bba\u6587 [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) \u7531 Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang \u53d1\u5e03\u3002", "1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.", "1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.", "1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, stating that `google/vit-base-patch16-224` is the most popular vision transformer model on the Hugging Face Model Hub for image classification. The additional information about the model's training and contribution does not detract from the accuracy of the response with respect to the reference answer."}, {"question": "What is the command to upload an ESPnet model to a Hugging Face repository?\n", "true_answer": "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/espnet.md", "generated_answer": "The command to upload an ESPnet model to a Hugging Face repository is:\n\n```bash\nespnet_model.push_to_hub(\"my-awesome-model\")\n```\n\nThis command is part of the `espnet` toolkit for speech processing, which can be found in the Hugging Face Hub. To learn more about sharing models with Hugging Face, refer to the official guide provided in the context. Additionally, you can use the", "retrieved_docs": ["The `run.sh` script allows to upload a given model to a Hugging Face repository.\n\n```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```\n\n## Additional resources\n\n* ESPnet [docs](https://espnet.github.io/espnet/index.html).\n* ESPnet model zoo [repository](https://github.com/espnet/espnet_model_zoo).\n* Integration [docs](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md).", "Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\n\n### Finding models\n\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.\n\n\n### Widgets", "Using ESPnet at Hugging Face\n\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks.\n\n## Exploring ESPnet in the Hub\n\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). \n\nAll models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more.\n2. Metadata tags that help for discoverability and contain information such as license, language and datasets.\n3. An interactive widget you can use to play out with the model directly in the browser.\n4. An Inference API that allows to make inference requests.", "```py\nimport soundfile\nfrom espnet2.bin.tts_inference import Text2Speech\n\ntext2speech = Text2Speech.from_pretrained(\"model_name\")\nspeech = text2speech(\"foobar\")[\"wav\"]\nsoundfile.write(\"out.wav\", speech.numpy(), text2speech.fs, \"PCM_16\")\n```\n\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png\"/>\n</div>\n\n## Sharing your models\n\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo#register-your-model)).", "```py\n>>> tokenizer.push_to_hub(\"my-awesome-model\")\n```\n\nOr perhaps you'd like to add the TensorFlow version of your fine-tuned PyTorch model:\n\n```py\n>>> tf_model.push_to_hub(\"my-awesome-model\")\n```\n\nNow when you navigate to your Hugging Face profile, you should see your newly created model repository. Clicking on the **Files** tab will display all the files you've uploaded to the repository.\n\nFor more details on how to create and upload files to a repository, refer to the Hub documentation [here](https://huggingface.co/docs/hub/how-to-upstream).\n\n## Upload with the web interface\n\nUsers who prefer a no-code approach are able to upload a model through the Hub's web interface. Visit [huggingface.co/new](https://huggingface.co/new) to create a new repository:\n\n![new_model_repo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/new_model_repo.png)\n\nFrom here, add some information about your model:", "## Sharing your models\n\nYou can easily upload your models using `mlagents-push-to-hf`:\n\n```\nmlagents-push-to-hf --run-id=\"First Training\" --local-dir=\"results/First Training\" --repo-id=\"ThomasSimonini/MLAgents-Pyramids\" --commit-message=\"Pyramids\"\n```\n\nYou need to define four parameters:\n- `--run-id`: the name of the training run id.\n- `--local-dir`: where the model was saved.\n- `--repo-id`: the name of the Hugging Face repo you want to create or update. It\u2019s `<your huggingface username>/<the repo name>`.\n- `--commit-message`.\n\n\n## Additional resources\n\n* ML-Agents [documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Hugging-Face-Integration.md)\n* Official Unity ML-Agents Spaces [demos](https://huggingface.co/unity)", "```bash\nsudo apt install tesseract-ocr\npip install -q pytesseract\n```\n\nOnce you have installed all of the dependencies, restart your runtime.\n\nWe encourage you to share your model with the community. Log in to your Hugging Face account to upload it to the \ud83e\udd17 Hub.\nWhen prompted, enter your token to log in:\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```\n\nLet's define some global variables.\n\n```py\n>>> model_checkpoint = \"microsoft/layoutlmv2-base-uncased\"\n>>> batch_size = 4\n```\n\n## Load the data\n\nIn this guide we use a small sample of preprocessed DocVQA that you can find on \ud83e\udd17 Hub. If you'd like to use the full\nDocVQA dataset, you can register and download it on [DocVQA homepage](https://rrc.cvc.uab.es/?ch=17). If you do so, to\nproceed with this guide check out [how to load files into a \ud83e\udd17 dataset](https://huggingface.co/docs/datasets/loading#local-and-remote-files).\n\n```py\n>>> from datasets import load_dataset"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "The response provided does not accurately reflect the correct command as per the reference answer. The response suggests using a method `espnet_model.push_to_hub(\"my-awesome-model\")`, which implies a Python function call rather than a command-line instruction. The reference answer indicates that the correct command should be a shell command, specifically `./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo`. There is no mention of a `run.sh` script or the associated parameters in the evaluated response. Therefore, the response does not meet the criteria for being correct, accurate, and factual according to the provided score rubric."}, {"question": "What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n", "true_answer": "requirements.txt", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/custom_dependencies.mdx", "generated_answer": "The file that should be added to a model repository to install custom Python dependencies for Inference Endpoints is a `requirements.txt` file, as mentioned in the context provided. This file lists the Python dependencies that need to be installed during the creation of the Inference Endpoints and Image artifacts on the Hugging Face Hub. The dependencies listed in this file will be installed by Inference Endpoints when it checks for the presence of a `requirements.txt` file in the model", "retrieved_docs": ["Add custom Dependencies\n\nInference Endpoints\u2019 base image includes all required libraries to run inference on \ud83e\udd17 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\n* [customize your inference pipeline](/docs/inference-endpoints/guides/custom_handler) and need additional Python dependencies\n* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)).\n\nTo add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within.\n\n```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```", "Inference Endpoints Version\n\nHugging Face Inference Endpoints comes with a default serving container which is used for all [supported Transformers and Sentence-Transformers tasks](/docs/inference-endpoints/supported_tasks) and for [custom inference handler](/docs/inference-endpoints/guides/custom_handler) and implement batching.\nBelow you will find information about the installed packages and versions used.\n\nYou can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model repository. Read more in [Add custom Dependencies](/docs/inference-endpoints/guides/custom_dependencies).\n\n## Installed packages & version", "## User defined code and modules\n\nThe Hugging Face Inference Toolkit allows the user to override the default methods of the `HuggingFaceHandlerService`. You will need to create a folder named `code/` with an `inference.py` file in it. See [here](#create-a-model-artifact-for-deployment) for more details on how to archive your model artifacts. For example:  \n\n```bash\nmodel.tar.gz/\n|- pytorch_model.bin\n|- ....\n|- code/\n  |- inference.py\n  |- requirements.txt \n```\n\nThe `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:", "### Setting an Inference Endpoint\nBelow are the screenshots with the steps we followed to create our custom Inference Endpoint. We used our QLoRA model, exported as a full-sized _merged_ model that can be easily loaded in `transformers`.\n![ie_1](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/personal_copilot/inference_endpoint_1.png)\n![ie_2](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/personal_copilot/inference_endpoint_2.png)\n\n### Setting up the VS Code Extension\nJust follow the [installation steps](https://github.com/huggingface/llm-vscode#installation). In the settings, replace the endpoint in the field below, so it points to the HF Inference Endpoint you deployed.\n\n![vs_code_endpoint](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/personal_copilot/vs_code_endpoint.png)\n\nUsage will look like below:", "```python\n# copy inference.py into the code/ directory of the model directory.\n!cp -r code/ tmp/code/\n# create a model.tar.gz archive with all the model artifacts and the inference.py script.\n%cd tmp\n!tar zcvf model.tar.gz *\n%cd ..\n```\n\nNow we can upload our `model.tar.gz` to our session S3 bucket with `sagemaker`.\n\n```python\nfrom sagemaker.s3 import S3Uploader\n\n# create s3 uri\ns3_model_path = f\"s3://{sess.default_bucket()}/{model_id}\"\n\n# upload model.tar.gz\ns3_model_uri = S3Uploader.upload(local_path=\"tmp/model.tar.gz\",desired_s3_uri=s3_model_path)\nprint(f\"model artifcats uploaded to {s3_model_uri}\")\n```\n\n## 4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n\nAfter we have uploaded our\u00a0`model.tar.gz`\u00a0to Amazon S3 can we create a custom\u00a0`HuggingfaceModel`. This class will be used to create and deploy our real-time inference endpoint on Amazon SageMaker.\n\n```python\nfrom sagemaker.huggingface.model import HuggingFaceModel", "```python\n# Start an Inference Endpoint running Zephyr-7b-beta on TGI\n>>> from huggingface_hub import create_inference_endpoint\n>>> endpoint = create_inference_endpoint(\n...     \"aws-zephyr-7b-beta-0486\",\n...     repository=\"HuggingFaceH4/zephyr-7b-beta\",\n...     framework=\"pytorch\",\n...     task=\"text-generation\",\n...     accelerator=\"gpu\",\n...     vendor=\"aws\",\n...     region=\"us-east-1\",\n...     type=\"protected\",\n...     instance_size=\"medium\",\n...     instance_type=\"g5.2xlarge\",\n...     custom_image={\n...         \"health_route\": \"/health\",\n...         \"env\": {\n...             \"MAX_BATCH_PREFILL_TOKENS\": \"2048\",\n...             \"MAX_INPUT_LENGTH\": \"1024\",\n...             \"MAX_TOTAL_TOKENS\": \"1512\",\n...             \"MODEL_ID\": \"/repository\"\n...         },\n...         \"url\": \"ghcr.io/huggingface/text-generation-inference:1.1.0\",\n...     },\n... )\n```", "### Guides\n\n* [Access the solution (UI)](/docs/inference-endpoints/guides/access)\n* [Create your first Endpoint](/docs/inference-endpoints/guides/create_endpoint)\n* [Send Requests to Endpoints](/docs/inference-endpoints/guides/test_endpoint)\n* [Update your Endpoint](/docs/inference-endpoints/guides/update_endpoint)\n* [Advanced Setup (Instance Types, Auto Scaling, Versioning)](/docs/inference-endpoints/guides/advanced)\n* [Create a Private Endpoint with AWS PrivateLink](/docs/inference-endpoints/guides/private_link)\n* [Add custom Dependencies](/docs/inference-endpoints/guides/custom_dependencies)\n* [Create custom Inference Handler](/docs/inference-endpoints/guides/custom_handler)\n* [Use a custom Container Image](/docs/inference-endpoints/guides/custom_container)\n* [Access and read Logs](/docs/inference-endpoints/guides/logs)\n* [Access and view Metrics](/docs/inference-endpoints/guides/metrics)\n* [Change Organization or Account](/docs/inference-endpoints/guides/change_organization)"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It correctly identifies the `requirements.txt` file as the one needed in a model repository for installing custom Python dependencies for Inference Endpoints. The explanation aligns with the reference answer and provides additional context about the use of the `requirements.txt` file, which supports the correctness and factual nature of the response."}, {"question": "How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n", "true_answer": "3-5 images", "source_doc": "huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md", "generated_answer": "According to the context provided, just 3-5 images are needed to teach new concepts to Stable Diffusion using Textual Inversion. (Source: Extracted document 0)", "retrieved_docs": ["Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n\n## Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Prepare Datasets", "## Textual Inversion\n\nTextual Inversion lets you personalize a Stable Diffusion model on your own images with just 3-5 samples. With this tool, you can train a model on a concept, and then share the concept with the rest of the community! \n\n![image info](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/diffusers-2nd-month/textual_inversion.jpeg)\n\nIn just a couple of days, the community shared over 200 concepts! Check them out!", "# Textual inversion\n\n[[open-in-colab]]\n\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer).\n\nThis guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text_inversion) training guide.\n\nImport the necessary libraries:\n\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.utils import make_image_grid\n```", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script.\n\nThe example supports both single node and multi-node distributed training:\n\n### Single node training\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATA_DIR=\"path-to-dir-containing-dicoo-images\"", "## Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\n\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb)\n\n## Running locally with PyTorch\n### Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n**Important**"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that 3-5 images are needed to teach new concepts to Stable Diffusion using Textual Inversion. The inclusion of a source reference does not detract from the accuracy of the answer."}, {"question": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n", "true_answer": "10GB", "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md", "generated_answer": "The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB. This feature was introduced in version 4.18.0 to address the issue of large checkpoints taking up too much storage space. If a checkpoint exceeds 10GB, it will be automatically sharded into smaller pieces, each of which will be of size less than 10GB. The sharding process is", "retrieved_docs": ["</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```", "If you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)", "This option can be activated with `low_cpu_mem_usage=True`. The model is first created on the Meta device (with empty weights) and the state dict is then loaded inside it (shard by shard in the case of a sharded checkpoint). This way the maximum RAM used is the full size of the model only.\n\n```py\nfrom transformers import AutoModelForSeq2SeqLM\n\nt0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", low_cpu_mem_usage=True)\n```\n\nMoreover, you can directly place the model on different devices if it doesn't fully fit in RAM (only works for inference for now). With `device_map=\"auto\"`, Accelerate will determine where to put each layer to maximize the use of your fastest devices (GPUs) and offload the rest on the CPU, or even the hard drive if you don't have enough GPU RAM (or CPU RAM). Even if the model is split across several devices, it will run as you would normally expect.", "The `partition.py` file defines the `PyTree` of `ParitionSpec` for the GPTNeo model which describes how the model will be sharded.\nThe actual sharding is auto-matically handled by `pjit`. The weights are sharded across all local devices.\nTo adapt the script for other models, we need to also change the `ParitionSpec` accordingly.\n\nTODO: Add more explantion.\n\nBefore training, let's prepare our model first. To be able to shard the model, the sharded dimention needs to be a multiple of devices it'll be sharded on. But GPTNeo's vocab size is 50257, so we need to resize the embeddings accordingly. \n\n```python\nfrom transformers import FlaxGPTNeoForCausalLM, GPTNeoConfig \nmodel = FlaxGPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")", "Let's start by loading the model checkpoint using the convenient [`AutoModelForSequenceClassification`](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification). Since the checkpoint of the model repository is just a pretrained checkpoint we should define the size of the classification head by passing `num_lables=5` (since we have 5 sentiment classes).\n\n\n```python\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_repository, num_labels=5)\n```", "On top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json", "We call the checkpoints saved in several files like BLOOM *sharded checkpoints*, and we have standardized their format as such: \n- One file (called `pytorch_model.bin.index.json`) contains some metadata and a map parameter name to file name, indicating where to find each weight\n- All the other files are standard PyTorch state dicts, they just contain a part of the model instead of the whole one. You can have a look at the content of the index file [here](https://huggingface.co/bigscience/bloom/blob/main/pytorch_model.bin.index.json).\n\nTo load such a sharded checkpoint into a model, we just need to loop over the various shards. Accelerate provides a function called `load_checkpoint_in_model` that will do this for you if you have cloned one of the repos of the Hub, or you can directly use the `from_pretrained` method of Transformers, which will handle the downloading and caching for you:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM"], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the maximum size of a model checkpoint before it is automatically sharded as 10GB, which is in line with the reference answer. The additional information provided about the feature's introduction and the process of sharding is not required but does not detract from the accuracy of the answer to the specific question asked."}, {"question": "What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n", "true_answer": "To track their machine learning experiments at every stage, from training to production.", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md", "generated_answer": "The context provided does not directly answer the question about the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists. However, based on the information provided, it can be inferred that W&B may be a tool or platform that helps data scientists and machine learning scientists address the limitations and biases in their models, specifically the heavy weighting of outliers in MSE (Mean Squared Error) and the complementary nature of MAE", "retrieved_docs": ["## Limitations and Bias\nMSE has the disadvantage of heavily weighting outliers -- given that it squares them, this results in large errors weighing more heavily than small ones. It can be used alongside [MAE](https://huggingface.co/metrics/mae), which is complementary given that it does not square the errors. \n\n## Citation(s)\n```bibtex\n@article{scikit-learn,\n  title={Scikit-learn: Machine Learning in {P}ython},\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n  journal={Journal of Machine Learning Research},\n  volume={12},\n  pages={2825--2830},\n  year={2011}\n}\n```", "## Limitations and Bias\nMSE has the disadvantage of heavily weighting outliers -- given that it squares them, this results in large errors weighing more heavily than small ones. It can be used alongside [MAE](https://huggingface.co/metrics/mae), which is complementary given that it does not square the errors. \n\n## Citation(s)\n```bibtex\n@article{scikit-learn,\n  title={Scikit-learn: Machine Learning in {P}ython},\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n  journal={Journal of Machine Learning Research},\n  volume={12},\n  pages={2825--2830},\n  year={2011}\n}\n```", "--\ntitle: \"Let's talk about biases in machine learning! Ethics and Society Newsletter #2\" \nthumbnail: /blog/assets/122_ethics_soc_2/thumbnail-solstice.png\nauthors:\n- user: yjernite\n---\n\n# Machine Learning in development: Let's talk about bias!\n\n\n_Bias in ML is ubiquitous, and Bias in ML is complex; so complex in fact that no single technical intervention is likely to meaningfully address the problems it engenders. ML models, as sociotechnical systems, amplify social trends that may exacerbate inequities and harmful biases in ways that depend on their deployment context and are constantly evolving._\n\n_This means that developing ML systems with care requires vigilance and responding to feedback from those deployment contexts, which in turn we can facilitate by sharing lessons across contexts and developing tools to analyze signs of bias at every level of ML development._", "So, who\u2019s on the hook for machine biases in ML? These three cases illustrate one of the reasons why discussions about the responsibility of ML developers in addressing bias can get so complicated: depending on decisions made at other points in the ML system development process by other people, the biases in an ML dataset or model may land anywhere between being irrelevant to the application settings and directly leading to grievous harm. However, in all of these cases, **stronger biases in the model/dataset increase the risk of negative outcomes**. The European Union has started to develop frameworks that address this phenomenon in [recent regulatory efforts](https://ec.europa.eu/info/business-economy-euro/doing-business-eu/contract-rules/digital-contracts/liability-rules-artificial-intelligence_en): in short, a company that deploys an AI system based on a measurably biased model is liable for harm caused by the system.", "## Limitations and Bias\n\n\n## Citation(s)\n```bibtex\n@article{scikit-learn, title={Scikit-learn: Machine Learning in {P}ython}, author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.}, journal={Journal of Machine Learning Research}, volume={12}, pages={2825--2830}, year={2011}\n```\n\n\n## Further References", "## Limitations and Bias\n\n\n## Citation(s)\n```bibtex\n@article{scikit-learn, title={Scikit-learn: Machine Learning in {P}ython}, author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.}, journal={Journal of Machine Learning Research}, volume={12}, pages={2825--2830}, year={2011}\n```\n\n\n## Further References", "## Limitations and Bias\n\n\n\n## Citation(s)\n```bibtex\n@article{scikit-learn,\n    title={Scikit-learn: Machine Learning in {P}ython},\n    author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n           and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n           and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n           Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n    journal={Journal of Machine Learning Research},\n    volume={12},\n    pages={2825--2830},\n    year={2011}\n}\n```\n\n\n## Further References"], "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response does not directly state the purpose of Weights and Biases (W&B) as a tool for tracking machine learning experiments, which is the core functionality according to the reference answer. Instead, it vaguely suggests that W&B might help address certain limitations and biases in models, which is related but does not explicitly mention experiment tracking or any stage of the machine learning workflow. Therefore, the response is somewhat correct as it infers a purpose that could be part of what W&B offers but fails to accurately describe its primary function."}, {"question": "What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n", "true_answer": "Optimum", "source_doc": "huggingface/blog/blob/main/intel.md", "generated_answer": "The name of the open-source library created by Hugging Face to simplify Transformer acceleration is Optimum. (Source: Extract 0)", "retrieved_docs": ["The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Let\u2019s take a deeper look at it.\n\n## Get Peak Transformers Performance with Optimum Intel\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices. Thanks to built-in optimization techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep tweaking for maximum performance.", "Luckily, Hugging Face has introduced [Optimum](https://huggingface.co/hardware), an open source library which makes it much easier to reduce the prediction latency of Transformer models on a variety of hardware platforms. In this blog post, you will learn how to accelerate Transformer models for the Graphcore [Intelligence Processing Unit](https://www.graphcore.ai/products/ipu) (IPU), a highly flexible, easy-to-use parallel processor designed from the ground up for AI workloads.\n\n### Optimum Meets Graphcore IPU\n\nThrough this partnership between Graphcore and Hugging Face, we are now introducing BERT as the first IPU-optimized model. We will be introducing many more of these IPU-optimized models in the coming months, spanning applications such as vision, speech, translation and text generation.", "To target these needs, Hugging Face built two open-sourced libraries: __Accelerate__ and __Optimum__. While [\ud83e\udd17 Accelerate](https://huggingface.co/docs/accelerate/index) focuses on out-of-the-box distributed training, [\ud83e\udd17 Optimum](https://huggingface.co/docs/optimum/index), as an extension of transformers, accelerates model training and inference by leveraging the maximum efficiency of users\u2019 targeted hardware. Optimum integrated machine learning accelerators like ONNX Runtime and specialized hardware like [Intel's Habana Gaudi](https://huggingface.co/blog/habana-gaudi-2-benchmark), so users can benefit from considerable speedup in both training and inference. Besides, Optimum seamlessly integrates other Hugging Face\u2019s tools while inheriting the same ease of use as Transformers. Developers can easily adapt their work to achieve lower latency with less computing power.\n\n## ONNX Runtime Training", "* Our open-source optimization libraries, [\ud83e\udd17 Optimum Intel](https://huggingface.co/blog/openvino) and [\ud83e\udd17 Optimum ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview), to get the highest efficiency out of training and running models for inference.\n\n  * Hugging Face [Expert Acceleration Program](https://huggingface.co/support), a commercial service for Hugging Face experts to work directly with your team to accelerate your Machine Learning roadmap and models.\n</div>\n\n\n## Introduction \n\nTransfer learning has changed Machine Learning by reaching new levels of accuracy from Natural Language Processing (NLP) to Audio and Computer Vision tasks. At Hugging Face, we work hard to make these new complex models and large checkpoints as easily accessible and usable as possible. But while researchers and data scientists have converted to the new world of Transformers, few companies have been able to deploy these large, complex models in production at scale.", "At Hugging Face, we are on a mission to democratize state-of-the-art Machine Learning, and a critical part of our work is to make these state-of-the-art models as efficient as possible, to use less energy and memory at scale, and to be more affordable to run by companies of all sizes. \n\nOur collaboration with Intel through the \ud83e\udd17 [Hardware Partner Program](https://huggingface.co/hardware) enables us to make advanced efficiency and optimization techniques easily available to the community, through our new \ud83e\udd17 [Optimum open source library](https://github.com/huggingface/optimum) dedicated to production performance.\n\nFor companies looking to accelerate their Transformer models inference, our new \ud83e\udd17 [Infinity product offers a plug-and-play containerized solution](https://huggingface.co/infinity), achieving down to 1ms latency on GPU and 2ms on Intel Xeon Ice Lake CPUs.", "## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive.", "Transformers have proven to be extremely efficient for a wide range of functions, including feature extraction, text generation, sentiment analysis, translation and many more. Models like BERT are widely used by Graphcore customers in a huge array of applications including cybersecurity, voice call automation, drug discovery, and translation.\n\nOptimizing their performance in the real world requires considerable time, effort and skills that are beyond the reach of many companies and organizations. In providing an open-source library of transformer models, Hugging Face has directly addressed these issues. Integrating IPUs with HuggingFace also allows developers to leverage not just the models, but also datasets available in the HuggingFace Hub."], "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly and provides the correct name of the open-source library created by Hugging Face for Transformer acceleration."}, {"question": "What parameter is used to ensure that elements in a row have the same height in Gradio?\n", "true_answer": "equal_height", "source_doc": "gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md", "generated_answer": "The parameter used to ensure that elements in a row have the same height in Gradio is the `grow` parameter in the `Row` component. When set to True for a component, it will grow to fill any remaining vertical space in the row. This ensures that all components in the row have the same height. Here's an example:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row(grow=", "retrieved_docs": ["Learn more about Columns in the [docs](https://gradio.app/docs/#column).\n\n# Dimensions\n\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases.\n\nBelow is an example illustrating the use of viewport width (vw):\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    im = gr.ImageEditor(\n        width=\"50vw\",\n    )\n\ndemo.launch()\n```\n\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n\n\n```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"", "```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"\n\nwith gr.Blocks(css=css) as demo:\n    with gr.Column(elem_classes=[\"container\"]):\n        name = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\n\ndemo.launch()\n```\n\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height.\n\nYou can apply any valid CSS unit for these parameters. For a comprehensive list of CSS units, refer to [this guide](https://www.w3schools.com/cssref/css_units.php). We recommend you always consider responsiveness and test your interfaces on various screen sizes to ensure a consistent user experience.\n\n\n\n## Tabs and Accordions", "- [#5283](https://github.com/gradio-app/gradio/pull/5283) [`a7460557`](https://github.com/gradio-app/gradio/commit/a74605572dd0d6bb41df6b38b120d656370dd67d) - Add height parameter and scrolling to `gr.Dataframe`. Thanks [@abidlabs](https://github.com/abidlabs)!", "Here's an example of how to use these attributes to create a Gradio app that does not lazy load and has an initial height of 0px.\n\n```html\n<gradio-app\n\tspace=\"gradio/Echocardiogram-Segmentation\"\n\teager=\"true\"\n\tinitial_height=\"0px\"\n></gradio-app>\n```\n\nHere's another example of how to use the `render` event. An event listener is used to capture the `render` event and will call the `handleLoadComplete()` function once rendering is complete. \n\n```html\n<script>\n\tfunction handleLoadComplete() {\n\t\tconsole.log(\"Embedded space has finished rendering\");\n\t}\n\n\tconst gradioApp = document.querySelector(\"gradio-app\");\n\tgradioApp.addEventListener(\"render\", handleLoadComplete);\n</script>\n```", "- Fixed Dropdown height rendering in Columns by [@aliabid94](https://github.com/aliabid94) in [PR 4584](https://github.com/gradio-app/gradio/pull/4584)\n- Fixed bug where `AnnotatedImage` css styling was causing the annotation masks to not be displayed correctly by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4628](https://github.com/gradio-app/gradio/pull/4628)\n- Ensure that Gradio does not silently fail when running on a port that is occupied by [@abidlabs](https://github.com/abidlabs) in [PR 4624](https://github.com/gradio-app/gradio/pull/4624).\n- Fix double upload bug that caused lag in file uploads by [@aliabid94](https://github.com/aliabid94) in [PR 4661](https://github.com/gradio-app/gradio/pull/4661)\n- `Progress` component now appears even when no `iterable` is specified in `tqdm` constructor by [@itrushkin](https://github.com/itrushkin) in [PR 4475](https://github.com/gradio-app/gradio/pull/4475)", "- Fixed Dropdown height rendering in Columns by [@aliabid94](https://github.com/aliabid94) in [PR 4584](https://github.com/gradio-app/gradio/pull/4584)\n- Fixed bug where `AnnotatedImage` css styling was causing the annotation masks to not be displayed correctly by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4628](https://github.com/gradio-app/gradio/pull/4628)\n- Ensure that Gradio does not silently fail when running on a port that is occupied by [@abidlabs](https://github.com/abidlabs) in [PR 4624](https://github.com/gradio-app/gradio/pull/4624).\n- Fix double upload bug that caused lag in file uploads by [@aliabid94](https://github.com/aliabid94) in [PR 4661](https://github.com/gradio-app/gradio/pull/4661)\n- `Progress` component now appears even when no `iterable` is specified in `tqdm` constructor by [@itrushkin](https://github.com/itrushkin) in [PR 4475](https://github.com/gradio-app/gradio/pull/4475)", "Learn more about Rows in the [docs](https://gradio.app/docs/#row).\n\n## Columns and Nesting\n\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n\n$code_rows_and_columns\n$demo_rows_and_columns\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/#column).\n\n# Dimensions"], "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response provided is incorrect. The parameter mentioned, `grow`, does not exist in the context of Gradio's `Row` component as per the reference answer. The correct parameter for ensuring that elements in a row have the same height in Gradio is `equal_height`. The response does not align with the reference answer and is therefore not factual or accurate in the context of the question."}, {"question": "What is the command to install the latest version of Optimum with OpenVINO support?\n", "true_answer": "pip install --upgrade-strategy eager optimum[\"openvino\"]", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md", "generated_answer": "The command to install the latest version of Optimum with OpenVINO support is:\n\n```bash\npip install --upgrade-strategy eager optimum[openvino]\n```\n\nThis command will ensure that Optimum is using the latest version and will install the OpenVINO support for accelerated inference. If you want to install from source, you can use the following command:\n\n```bash\npython -m pip", "retrieved_docs": ["Optimum Intel supports [OpenVINO](https://docs.openvino.ai/latest/index.html), an Intel open-source toolkit for high-performance inference. \n\nOptimum Intel and OpenVINO can be installed as follows:\n\n```\npip install optimum[openvino]\n```\n\nStarting from the code above, we only need to replace `StableDiffusionPipeline` with `OVStableDiffusionPipeline`. To load a PyTorch model and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model.\n\n```python\nfrom optimum.intel.openvino import OVStableDiffusionPipeline\n...\nov_pipe = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\nlatency = elapsed_time(ov_pipe, prompt)\nprint(latency)\n\n# Don't forget to save the exported model\nov_pipe.save_pretrained(\"./openvino\")\n```\n\nOpenVINO automatically optimizes the model for the `bfloat16` format. Thanks to this, the average latency is now **16.7 seconds**, a sweet 2x speedup.", "You'll need to install \ud83e\udd17 Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\n\n```bash\npip install --upgrade-strategy eager optimum[\"openvino\"]\n```\n\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO.\n\n## Stable Diffusion\n\nTo load and run inference, use the [`~optimum.intel.OVStableDiffusionPipeline`]. If you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, set `export=True`:\n\n```python\nfrom optimum.intel import OVStableDiffusionPipeline\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipeline = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\nprompt = \"sailing ship in storm by Rembrandt\"\nimage = pipeline(prompt).images[0]\n\n# Don't forget to save the exported model\npipeline.save_pretrained(\"openvino-sd-v1-5\")\n```", "The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.\n\nTo install from source:\n\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\n\nFor the accelerator-specific features, append `optimum[accelerator_type]` to the above command:\n\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```\n\n## Accelerated Inference\n\n\ud83e\udd17 Optimum provides multiple tools to export and run optimized models on various ecosystems: \n\n- [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model) / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models)\n- TensorFlow Lite\n- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)\n- Habana first-gen Gaudi / Gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_inference)", "And run post-installation configuration:\n\n```bash\npython -m torch_ort.configure\n```\n\n### Install Optimum\n\nYou can install Optimum via pypi:\n\n```bash\npip install optimum\n```\n\nOr install from source:\n\n```bash\npip install git+https://github.com/huggingface/optimum.git\n```\n\nThis command installs the current main dev version of Optimum, which could include latest developments(new features, bug fixes). However, the\nmain version might not be very stable. If you run into any problem, please open an [issue](https://github.com/huggingface/optimum/issues) so\nthat we can fix it as soon as possible.\n\n## ORTTrainer\n\nThe [`ORTTrainer`] class inherits the [`Trainer`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#trainer)\nof Transformers. You can easily adapt the codes by replacing `Trainer` of transformers with `ORTTrainer` to take advantage of the acceleration\nempowered by ONNX Runtime. Here is an example of how to use `ORTTrainer` compared with `Trainer`:", "### OpenVINO\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install --upgrade-strategy eager optimum[openvino,nncf]\n```\n\nIt is possible to export \ud83e\udd17 Transformers and Diffusers models to the OpenVINO format easily:\n\n```bash\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert_sst2_ov\n```\n\nIf you add `--int8`, the weights will be quantized to INT8. Static quantization can also be applied on the activations using [NNCF](https://github.com/openvinotoolkit/nncf), more information can be found in the [documentation](https://huggingface.co/docs/optimum/main/en/intel/optimization_ov).\n\nTo load a model and run inference with OpenVINO Runtime, you can just replace your `AutoModelForXxx` class with the corresponding `OVModelForXxx` class. To load a PyTorch checkpoint and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model.", "The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.\n\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you can install the base library from source as follows:\n\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\n\nFor the accelerator-specific features, you can install them by appending `optimum[accelerator_type]` to the `pip` command, e.g.\n\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```", "```bash\n\tpip install -e \".[dev]\"\n\t```\n\n   (If optimum was already installed in the virtual environment, remove\n   it with `pip uninstall optimum` before reinstalling it in editable\n   mode with the `-e` flag.)\n\n5. Develop the features on your branch.\n\n6. Format your code. Run black and ruff so that your newly added files look nice with the following command:\n\n\t```bash\n\tmake style\n\t```\n\n7.  Once you're happy with your changes, add the changed files using `git add` and make a commit with `git commit` to record your changes locally:\n\n\t```bash\n\tgit add modified_file.py\n\tgit commit\n\t```\n\n\tIt is a good idea to sync your copy of the code with the original\n\trepository regularly. This way you can quickly account for changes:\n\n\t```bash\n\tgit fetch upstream\n\tgit rebase upstream/main\n    ```\n\n   Push the changes to your account using:\n\n   ```bash\n   git push -u origin a-descriptive-name-for-my-changes\n   ```"], "eval_score_GPT4": "4", "eval_feedback_GPT4": "The response provides a command that is mostly correct. The use of `pip install --upgrade-strategy eager optimum[openvino]` is accurate in terms of the package manager and the upgrade strategy. However, the response includes a syntax error by not enclosing `openvino` in quotes, which is required for the command to work correctly in some shells. Additionally, the response is incomplete as it cuts off during the explanation of how to install from source, which is not necessary for the instruction but shows a lack of attention to detail. Based on the rubric, the response is mostly correct but has a minor inaccuracy."}]