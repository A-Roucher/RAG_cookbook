{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection\n",
    "### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_LOCALLY = False\n",
    "\n",
    "if LOAD_LOCALLY:\n",
    "    from pathlib import Path\n",
    "    from datasets import Dataset\n",
    "\n",
    "    docs = []\n",
    "    sources = []\n",
    "    for p in Path(\"./data/datasets/huggingface_docs/\").iterdir():\n",
    "        if not p.is_dir():\n",
    "            with open(p) as f:\n",
    "                # the first line is the source of the text\n",
    "                source = f.readline().strip().replace('source: ', '').replace('https://github.com/', '')\n",
    "                content = f.read()[2:] # Remove the initial '\\n'\n",
    "                if len(content) > 0:\n",
    "                    docs.append(content)\n",
    "                    sources.append(source)\n",
    "        # break\n",
    "\n",
    "    ds = Dataset.from_dict({\"text\": docs, \"source\": sources})\n",
    "    ds.to_csv('./data/huggingface_doc.csv')\n",
    "    print(f'number of documents: {len(ds)}')\n",
    "\n",
    "else:\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    ds = load_dataset(\"A-Roucher/huggingface_doc\", split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup evaluation pipeline\n",
    "In this part, we build a synthetic dataset of questions and associated contexts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare source documents\n",
    "\n",
    "We use Langchain's `RecursiveCharacterTextSplitter`, which makes efficient use of code language detection to make better splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c44734b45b4b8fa4544f970a215903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [LangchainDocument(page_content=doc['text'], metadata={'source': doc['source']}) for doc in tqdm(ds)]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, chunk_overlap=200, add_start_index=True, separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup chains for question generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\"\n",
    "\n",
    "question_relatedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating)\n",
    "Total rating: (your rating)\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating)\n",
    "Total rating: (your rating)\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0.2)\n",
    "QA_generation_prompt = ChatPromptTemplate.from_template(QA_generation_prompt)\n",
    "QA_generation_chain = QA_generation_prompt | chat_model\n",
    "\n",
    "question_relatedness_critique_prompt = ChatPromptTemplate.from_template(question_relatedness_critique_prompt)\n",
    "question_relatedness_critique_chain = question_relatedness_critique_prompt | chat_model\n",
    "\n",
    "question_relevance_critique_prompt = ChatPromptTemplate.from_template(question_relevance_critique_prompt)\n",
    "question_relevance_critique_chain = question_relevance_critique_prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e25b3d9dd93466f8815ca7ce118536a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "outputs = []\n",
    "for context in tqdm(random.sample(langchain_docs, 30)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = QA_generation_chain.invoke({\"context\": context.page_content}).content\n",
    "    try:\n",
    "        question = output_QA_couple.split('Factoid question: ')[1].split('Answer: ')[0]\n",
    "        answer = output_QA_couple.split('Answer: ')[1]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # Critique QA couple\n",
    "    question_relatedness_evaluation = question_relatedness_critique_chain.invoke({\"context\": context.page_content, \"question\": question}).content\n",
    "    question_relevance_evaluation = question_relevance_critique_chain.invoke({\"question\": question}).content\n",
    "\n",
    "    try:\n",
    "        relatedness_score = int(question_relatedness_evaluation.split('Total rating: ')[1][0])\n",
    "        relatedness_eval = question_relatedness_evaluation.split('Total rating: ')[0].split('Evaluation: ')[1]\n",
    "        relevance_score = int(question_relevance_evaluation.split('Total rating: ')[1][0])\n",
    "        relevance_eval = question_relevance_evaluation.split('Total rating: ')[0].split('Evaluation: ')[1]\n",
    "\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"relatedness_score\": relatedness_score,\n",
    "                \"relatedness_eval\": relatedness_eval,\n",
    "                \"relevance_score\": relevance_score,\n",
    "                \"relevance_eval\": relevance_eval,\n",
    "                \"source_doc\": context.metadata['source'],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "generated_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = generated_questions.loc[(generated_questions['relatedness_score'] >= 4) & (generated_questions['relevance_score'] >= 3)]\n",
    "generated_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_questions.to_excel(\"generated_questions.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build RAG System\n",
    "## 1. Retriever - embeddings\n",
    "Here we use Langchain vector databases since it offers a convenient FAISS index and allows us to keep document metadata throughout the processing.\n",
    "\n",
    "\n",
    "\n",
    "Options:\n",
    "- change embedding model\n",
    "- normal embeddings vs instruct embeddings\n",
    "- Hyde\n",
    "- reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "In this part, we split the documennts from our knowledge base into smaller chunks which will be the snippets that will support our answer. The goal is to have semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large in order to be centered around a key information.\n",
    "\n",
    "We do this chunking with the `haystack` library, which offers a good `PreProcessor` class.\n",
    "\n",
    "[This space](https://huggingface.co/spaces/A-Roucher/chunk_visualizer) lets you visualize how different chunking options affect the chunks you get, to help you tune this step.\n",
    "\n",
    "Options:\n",
    "- split respecting sentence boundaries\n",
    "- semantic splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7dcddcd3a8456db586839fbb354f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:   0%|          | 0/2647 [00:00<?, ?docs/s]We found one or more sentences whose split count is higher than the split length.\n",
      "Preprocessing:   3%|▎         | 80/2647 [00:00<00:13, 196.63docs/s]Document 9c4032da1045295f2cfa5f5107e4aab1 is 11751 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document afd40958e2932034ec2f6f7d7b629dec is 11634 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:   5%|▍         | 125/2647 [00:00<00:13, 183.71docs/s]Document afd40958e2932034ec2f6f7d7b629dec is 11634 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 89027fae1c00a66bf57339b1cef1f01f is 28844 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 67b901bdbf6bcaa7ad4e2b2c5d2af82e is 18844 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:   7%|▋         | 193/2647 [00:01<00:17, 138.92docs/s]Document 6fc7d0d4aee543ddbbbb654dda005550 is 11548 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  12%|█▏        | 321/2647 [00:01<00:10, 222.13docs/s]Document b681d975149b4ed52e177edd31906fec is 12779 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  19%|█▉        | 497/2647 [00:02<00:10, 199.89docs/s]Document 41f9318f0c9c48a91cd0cf5e9b35f2c4 is 92876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document a49add40a6eac39fe04eb63f29b06fec is 82876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 9f52db69f26314dce813198cc6281b6a is 72876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document e18352c977f3b6eb3333156ec094ddab is 62876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 6f16f7f4aafecff9e7137897c91c4093 is 52876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document b4b63c9d71a8b3905ab45d8ef15a0057 is 42876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document ce51fbd952e4501c0ee9c04dab5db69b is 32876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 73f786b449c22a753e47678c75526ac5 is 22876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 25295a6100cfc0bd840cd67894435bcf is 12876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  25%|██▍       | 657/2647 [00:03<00:10, 187.52docs/s]Document 28f2b2417ea983dacc6690e8aa6b6650 is 10270 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  30%|██▉       | 781/2647 [00:04<00:08, 213.01docs/s]Document d9dcf60664f43e9769461ceef4477eb6 is 13213 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  35%|███▌      | 929/2647 [00:05<00:08, 195.38docs/s]Document 7d8978892372921d786e2cee0c7fa0 is 10759 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  37%|███▋      | 979/2647 [00:05<00:08, 206.96docs/s]Document c40dc7302f18d123f5dc307d4e5c64d2 is 11258 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  39%|███▊      | 1022/2647 [00:05<00:08, 191.36docs/s]Document e7994ba8ae2df1dd6350172e126a726a is 253887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document d6cc4a16bcee148c22d95875881db8b3 is 243887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 3c3ceb44b9156f9ecb34ba388fac11b9 is 233887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 75d4727f0c1b159f49dc5d6f522373ad is 223887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document b076aa94c87c4fc3150c19e47033e7b is 213887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 55829abb324e163fefb6b87ca0c207ff is 203887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 858d54e2e6c1dc54a4a4ec5f61e13e67 is 193887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 449180817164388971e5c2c817650f5c is 183887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 53ab3c269be7c19cb89d78c6b0a6686 is 173887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 6b72839b67167ba775fb0e3c875989ca is 163887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document a460787c8a5511dcb7f684130712e869 is 153887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 80a3a05428da1b1be2849fb6a772ac5e is 143887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 7b69e7b32b51f4165f34cb7aacb96550 is 133887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 72e2c9cec8f13d5cc1526c4eb7974b1c is 123887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 472c48e5ef83caa7a02f926b78e92dbf is 113887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 6598d51807e40be2fa71c9bd1b9fec93 is 103887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 79df15b23933bf1eca2ae8c14662345d is 93887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 87f25d16fe86e892eea50317dc759f50 is 83887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document c86063cc1c26ec2457908223f8ace345 is 73887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document cf1cfaf88b8fa354348bb490a18894f3 is 63887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 8e73139a9ebfe99fabb7a6fc1c2d1460 is 53887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 8b65c3fba9ec312eef20805b407f8a33 is 43887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document a7092831ebbfe108c6f750285b46dd45 is 33887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 948026f539fb7b46320a9013019321d4 is 23887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document cc03634b014efe0051c6c9c9e9c77109 is 13887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  41%|████      | 1082/2647 [00:06<00:16, 95.02docs/s] Document 257c6f99a23cafcf05b2a3ddcf5568ab is 11163 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  42%|████▏     | 1099/2647 [00:06<00:17, 88.38docs/s]Document 1c7e8b11707561d5ec0a5533e16353d0 is 12054 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  42%|████▏     | 1124/2647 [00:06<00:13, 113.12docs/s]Document 257c6f99a23cafcf05b2a3ddcf5568ab is 11163 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  44%|████▍     | 1166/2647 [00:07<00:14, 102.93docs/s]Document 10e7b073021c83c8ed38c2c562dc7ee2 is 12311 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  52%|█████▏    | 1389/2647 [00:08<00:05, 225.75docs/s]Document c40dc7302f18d123f5dc307d4e5c64d2 is 11258 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  54%|█████▎    | 1417/2647 [00:08<00:05, 233.58docs/s]Document d3622a51b8d2a5902055d66113a666e3 is 13691 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  58%|█████▊    | 1533/2647 [00:09<00:06, 166.40docs/s]Document c08857abeeeeb7c30ac9ffd53873ade0 is 11052 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  61%|██████    | 1604/2647 [00:09<00:05, 191.09docs/s]Document 56ec30806220a5f13e23b0a7276fa510 is 10534 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  76%|███████▌  | 1999/2647 [00:11<00:03, 212.74docs/s]Document b4b93b8ee70c565c7a14a517ad1c1bda is 32505 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 7c7be85c0d0ff46915877f50e38e9fc6 is 22505 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 1b6173f6c199736ebaebdd2e4ed827e5 is 12505 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  80%|███████▉  | 2105/2647 [00:11<00:02, 235.66docs/s]Document 4eb1a9a23f3a9d49dc63b1f49beea7e0 is 10052 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  81%|████████▏ | 2154/2647 [00:12<00:02, 208.41docs/s]Document 4eb1a9a23f3a9d49dc63b1f49beea7e0 is 10052 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  83%|████████▎ | 2207/2647 [00:12<00:01, 221.80docs/s]Document 28f2b2417ea983dacc6690e8aa6b6650 is 10270 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  88%|████████▊ | 2319/2647 [00:13<00:01, 188.89docs/s]Document 89027fae1c00a66bf57339b1cef1f01f is 28844 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 67b901bdbf6bcaa7ad4e2b2c5d2af82e is 18844 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  89%|████████▉ | 2363/2647 [00:13<00:02, 133.33docs/s]Document 10e7b073021c83c8ed38c2c562dc7ee2 is 12311 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  94%|█████████▍| 2485/2647 [00:14<00:00, 203.09docs/s]Document 1927770652c08da98f3c2f6b50aa15dd is 145352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document f198e9b51e661d70518821bc8944ab03 is 135352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 1cad3656bec2aa86c9d276984fc0a994 is 125352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document b7297df8e7e3b48902d407635267769e is 115352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 16a9b045fc05d4a371feec1d3ece59b7 is 105352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 6daaafba1cea32a31f79197f6b5bf9d4 is 95352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document f3f42787a885929525f3c2c2d6021409 is 85352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 9ee4de870e9431fc0dce667be46b0285 is 75352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document defc64b3dc61ff889db9a57775a0e26a is 65352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document e480acab82055fcd421f43ab29936984 is 55352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 5191a3852e558aaaa3e19b94c0e36336 is 45352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document c788c1452eef2fab6651ba46f9fe89a3 is 35352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document e4689903dea0c6f0802b46ab90fd6ba4 is 25352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 9a32874e906b6656d5cb72eab1e57913 is 15352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  95%|█████████▍| 2507/2647 [00:14<00:01, 129.95docs/s]Document e31401a0c51e00e283f2ae2923fb60c2 is 11076 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  96%|█████████▌| 2544/2647 [00:14<00:00, 143.32docs/s]Document e31401a0c51e00e283f2ae2923fb60c2 is 11076 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  98%|█████████▊| 2586/2647 [00:14<00:00, 169.97docs/s]Document d3622a51b8d2a5902055d66113a666e3 is 13691 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing: 100%|██████████| 2647/2647 [00:15<00:00, 176.45docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 35244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [LangchainDocument(page_content=doc['text'], metadata={'source': doc['source']}) for doc in tqdm(ds)]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True, separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_INSTRUCT_EMBEDDINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "\n",
    "if not USE_INSTRUCT_EMBEDDINGS:\n",
    "    model_name = 'BAAI/bge-base-en-v1.5'\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "else:\n",
    "    model_name = \"hkunlp/instructor-large\"\n",
    "    embed_instruction = \"Represent the Hugging Face library documentation\"\n",
    "    query_instruction = \"Query the most relevant piece of information from the Hugging Face documentation\"\n",
    "\n",
    "    embedding_model = HuggingFaceInstructEmbeddings(\n",
    "        model_name=model_name,\n",
    "        embed_instruction=embed_instruction,\n",
    "        query_instruction=query_instruction\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_model.embed_documents(texts=[d.page_content for d in langchain_docs[:1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "index = FAISS.from_documents(langchain_docs[:1000], embedding_model)\n",
    "\n",
    "index_name = 'index_1000'\n",
    "index.save_local(f'./data/indexes/{index_name}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = FAISS.load_local(f'./data/indexes/{index_name}/', embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = index.similarity_search(query='how to create a pipeline object?', k=5)\n",
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_index = FAISS.load_local(f'./data/indexes/{index_name}/', embedding_model)\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reader - LLM\n",
    "Options:\n",
    "- zero-shot vs few-shot prompting (cf [resource](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant#6-using-qdrant-to-improve-rag-prompt))\n",
    "- tune the number of examples retrieved\n",
    "- make conversational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context, \n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "  </s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\"text-generation\", model='HuggingFaceH4/zephyr-7b-beta')\n",
    "\n",
    "llm('Ok,', max_new_tokens=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HF_TOKEN = os.environ.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "API_URL = \"https://dxsuz0i09l5zzjh1.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "\n",
    "headers = {\n",
    "\t\"Authorization\": f\"Bearer {HF_TOKEN}\",\n",
    "\t\"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def llm(question):\n",
    "\tpayload = {\n",
    "\t\t\"inputs\": question,\n",
    "\t\t\"max_new_tokens\": 2000,\n",
    "\t}\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, llm, num_retrieved_docs: int = 15, num_reranked_docs: int = 7):\n",
    "    # Gather documents with retriever\n",
    "        \n",
    "    relevant_docs = knowledge_index.similarity_search(\n",
    "        query=question,\n",
    "        k=num_retrieved_docs\n",
    "    )\n",
    "\n",
    "    # Chosse the most relevant documents with reranker\n",
    "    cross_encoding_predictions = reranker.predict(\n",
    "        [(question, doc.page_content) for doc in relevant_docs]\n",
    "    )\n",
    "    relevant_docs = [\n",
    "        doc for _, doc in sorted(\n",
    "            zip(cross_encoding_predictions, relevant_docs),\n",
    "            reverse=True, key = lambda x: x[0]\n",
    "        )\n",
    "    ]\n",
    "    relevant_docs = relevant_docs[:num_reranked_docs]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = '\\nExtracted documents:\\n'\n",
    "    context += ''.join([f\"{str(i)}: \" + doc.page_content for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = prompt_template.format(\n",
    "        context=context,\n",
    "        question=question\n",
    "    )\n",
    "    print('Finished retrieving')\n",
    "    # Redact an answer\n",
    "    full_answer = llm(final_prompt)[0]['generated_text']\n",
    "    answer = full_answer[len(final_prompt):]\n",
    "    print(full_answer, answer)\n",
    "\n",
    "    return full_answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how to create a pipeline object?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, relevant_docs = answer_question(question, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_answer(answer, relevant_docs):\n",
    "    print(f'Answer: {answer}')\n",
    "    print('\\n\\nSource documents:')\n",
    "    for doc in relevant_docs:\n",
    "        print(f'{doc.metadata[\"source\"]}')\n",
    "        print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_answer(answer, relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking the chosen system on your evaluation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
