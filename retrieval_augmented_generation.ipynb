{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection\n",
    "### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_LOCALLY = False\n",
    "\n",
    "\n",
    "if LOAD_LOCALLY:\n",
    "    from pathlib import Path\n",
    "    from datasets import Dataset\n",
    "\n",
    "    docs = []\n",
    "    sources = []\n",
    "    for p in Path(\"./data/datasets/huggingface_docs/\").iterdir():\n",
    "        if not p.is_dir():\n",
    "            with open(p) as f:\n",
    "                # the first line is the source of the text\n",
    "                source = f.readline().strip().replace('source: ', '').replace('https://github.com/', '')\n",
    "                content = f.read()[2:] # Remove the initial '\\n'\n",
    "                if len(content) > 0:\n",
    "                    docs.append(content)\n",
    "                    sources.append(source)\n",
    "        # break\n",
    "\n",
    "    ds = Dataset.from_dict({\"text\": docs, \"source\": sources})\n",
    "    ds.to_csv('huggingface_doc.csv')\n",
    "    print(f'number of documents: {len(ds)}')\n",
    "\n",
    "else:\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    ds = load_dataset(\"A-Roucher/huggingface_doc\", split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Options:\n",
    "- split respecting sentence boundaries\n",
    "- semantic splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3244b1c51945eb8669b34c5490fe62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from haystack import Document\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "haystack_docs = []\n",
    "for doc in tqdm(ds):\n",
    "    if doc['text'] is None:\n",
    "        print(doc)\n",
    "    haystack_docs.append(Document(content=doc['text'], meta={'source': doc['source']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:   0%|          | 0/2647 [00:00<?, ?docs/s]We found one or more sentences whose split count is higher than the split length.\n",
      "Preprocessing:   3%|▎         | 80/2647 [00:00<00:13, 194.41docs/s]Document 9c4032da1045295f2cfa5f5107e4aab1 is 11751 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document afd40958e2932034ec2f6f7d7b629dec is 11634 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:   5%|▍         | 123/2647 [00:00<00:14, 179.41docs/s]Document afd40958e2932034ec2f6f7d7b629dec is 11634 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 89027fae1c00a66bf57339b1cef1f01f is 28844 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 67b901bdbf6bcaa7ad4e2b2c5d2af82e is 18844 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:   8%|▊         | 212/2647 [00:01<00:17, 137.99docs/s]Document 6fc7d0d4aee543ddbbbb654dda005550 is 11548 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  13%|█▎        | 347/2647 [00:02<00:09, 241.24docs/s]Document b681d975149b4ed52e177edd31906fec is 12779 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  19%|█▊        | 490/2647 [00:03<00:11, 191.03docs/s]Document 41f9318f0c9c48a91cd0cf5e9b35f2c4 is 92876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document a49add40a6eac39fe04eb63f29b06fec is 82876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 9f52db69f26314dce813198cc6281b6a is 72876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document e18352c977f3b6eb3333156ec094ddab is 62876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 6f16f7f4aafecff9e7137897c91c4093 is 52876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document b4b63c9d71a8b3905ab45d8ef15a0057 is 42876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document ce51fbd952e4501c0ee9c04dab5db69b is 32876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 73f786b449c22a753e47678c75526ac5 is 22876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 25295a6100cfc0bd840cd67894435bcf is 12876 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  25%|██▌       | 674/2647 [00:04<00:09, 208.07docs/s]Document 28f2b2417ea983dacc6690e8aa6b6650 is 10270 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  31%|███       | 812/2647 [00:04<00:07, 245.51docs/s]Document d9dcf60664f43e9769461ceef4477eb6 is 13213 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  35%|███▌      | 929/2647 [00:05<00:08, 193.33docs/s]Document 7d8978892372921d786e2cee0c7fa0 is 10759 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  37%|███▋      | 979/2647 [00:05<00:08, 204.71docs/s]Document c40dc7302f18d123f5dc307d4e5c64d2 is 11258 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  39%|███▊      | 1021/2647 [00:05<00:08, 186.46docs/s]Document e7994ba8ae2df1dd6350172e126a726a is 253887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document d6cc4a16bcee148c22d95875881db8b3 is 243887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 3c3ceb44b9156f9ecb34ba388fac11b9 is 233887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 75d4727f0c1b159f49dc5d6f522373ad is 223887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document b076aa94c87c4fc3150c19e47033e7b is 213887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 55829abb324e163fefb6b87ca0c207ff is 203887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 858d54e2e6c1dc54a4a4ec5f61e13e67 is 193887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 449180817164388971e5c2c817650f5c is 183887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 53ab3c269be7c19cb89d78c6b0a6686 is 173887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 6b72839b67167ba775fb0e3c875989ca is 163887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document a460787c8a5511dcb7f684130712e869 is 153887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 80a3a05428da1b1be2849fb6a772ac5e is 143887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 7b69e7b32b51f4165f34cb7aacb96550 is 133887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 72e2c9cec8f13d5cc1526c4eb7974b1c is 123887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 472c48e5ef83caa7a02f926b78e92dbf is 113887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 6598d51807e40be2fa71c9bd1b9fec93 is 103887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 79df15b23933bf1eca2ae8c14662345d is 93887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 87f25d16fe86e892eea50317dc759f50 is 83887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document c86063cc1c26ec2457908223f8ace345 is 73887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document cf1cfaf88b8fa354348bb490a18894f3 is 63887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 8e73139a9ebfe99fabb7a6fc1c2d1460 is 53887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 8b65c3fba9ec312eef20805b407f8a33 is 43887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document a7092831ebbfe108c6f750285b46dd45 is 33887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 948026f539fb7b46320a9013019321d4 is 23887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document cc03634b014efe0051c6c9c9e9c77109 is 13887 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  41%|████      | 1080/2647 [00:06<00:16, 94.30docs/s] Document 257c6f99a23cafcf05b2a3ddcf5568ab is 11163 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  42%|████▏     | 1118/2647 [00:07<00:14, 107.49docs/s]Document 1c7e8b11707561d5ec0a5533e16353d0 is 12054 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 257c6f99a23cafcf05b2a3ddcf5568ab is 11163 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  44%|████▍     | 1161/2647 [00:07<00:10, 147.76docs/s]Document 10e7b073021c83c8ed38c2c562dc7ee2 is 12311 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  52%|█████▏    | 1386/2647 [00:08<00:05, 233.66docs/s]Document c40dc7302f18d123f5dc307d4e5c64d2 is 11258 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  53%|█████▎    | 1412/2647 [00:08<00:05, 237.43docs/s]Document d3622a51b8d2a5902055d66113a666e3 is 13691 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  58%|█████▊    | 1530/2647 [00:09<00:09, 114.89docs/s]Document c08857abeeeeb7c30ac9ffd53873ade0 is 11052 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  61%|██████▏   | 1622/2647 [00:09<00:05, 177.84docs/s]Document 56ec30806220a5f13e23b0a7276fa510 is 10534 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  75%|███████▌  | 1997/2647 [00:11<00:03, 208.17docs/s]Document b4b93b8ee70c565c7a14a517ad1c1bda is 32505 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 7c7be85c0d0ff46915877f50e38e9fc6 is 22505 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 1b6173f6c199736ebaebdd2e4ed827e5 is 12505 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  79%|███████▉  | 2098/2647 [00:12<00:02, 226.12docs/s]Document 4eb1a9a23f3a9d49dc63b1f49beea7e0 is 10052 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  81%|████████  | 2150/2647 [00:12<00:02, 210.92docs/s]Document 4eb1a9a23f3a9d49dc63b1f49beea7e0 is 10052 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  83%|████████▎ | 2199/2647 [00:12<00:02, 218.80docs/s]Document 28f2b2417ea983dacc6690e8aa6b6650 is 10270 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  88%|████████▊ | 2332/2647 [00:13<00:01, 201.93docs/s]Document 89027fae1c00a66bf57339b1cef1f01f is 28844 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 67b901bdbf6bcaa7ad4e2b2c5d2af82e is 18844 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  90%|████████▉ | 2371/2647 [00:13<00:02, 124.29docs/s]Document 10e7b073021c83c8ed38c2c562dc7ee2 is 12311 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  93%|█████████▎| 2472/2647 [00:14<00:00, 192.47docs/s]Document 1927770652c08da98f3c2f6b50aa15dd is 145352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document f198e9b51e661d70518821bc8944ab03 is 135352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 1cad3656bec2aa86c9d276984fc0a994 is 125352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document b7297df8e7e3b48902d407635267769e is 115352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 16a9b045fc05d4a371feec1d3ece59b7 is 105352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 6daaafba1cea32a31f79197f6b5bf9d4 is 95352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document f3f42787a885929525f3c2c2d6021409 is 85352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 9ee4de870e9431fc0dce667be46b0285 is 75352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document defc64b3dc61ff889db9a57775a0e26a is 65352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document e480acab82055fcd421f43ab29936984 is 55352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 5191a3852e558aaaa3e19b94c0e36336 is 45352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document c788c1452eef2fab6651ba46f9fe89a3 is 35352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document e4689903dea0c6f0802b46ab90fd6ba4 is 25352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Document 9a32874e906b6656d5cb72eab1e57913 is 15352 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  95%|█████████▌| 2519/2647 [00:14<00:00, 134.54docs/s]Document e31401a0c51e00e283f2ae2923fb60c2 is 11076 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  96%|█████████▌| 2538/2647 [00:14<00:00, 143.54docs/s]Document e31401a0c51e00e283f2ae2923fb60c2 is 11076 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing:  98%|█████████▊| 2607/2647 [00:15<00:00, 190.83docs/s]Document d3622a51b8d2a5902055d66113a666e3 is 13691 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
      "Preprocessing: 100%|██████████| 2647/2647 [00:15<00:00, 171.93docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 35244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from haystack import Document\n",
    "from haystack.nodes import PreProcessor\n",
    "\n",
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=False,\n",
    "    split_by=\"word\",\n",
    "    split_length=100,\n",
    "    split_overlap=5,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")\n",
    "\n",
    "haystack_docs = preprocessor.process(\n",
    "    haystack_docs,\n",
    ")\n",
    "print(f'Number of chunks: {len(haystack_docs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we must convert haystack to langchain docs\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [LangchainDocument(page_content=doc.content, metadata=doc.meta) for doc in haystack_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever - embeddings\n",
    "Here we use Langchain vector databases since it offers a convenient FAISS index and allows us to keep document metadata throughout the processing.\n",
    "\n",
    "\n",
    "\n",
    "Options:\n",
    "- normal embeddings vs instruct embeddings\n",
    "- Hyde\n",
    "- reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_INSTRUCT_EMBEDDINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "\n",
    "if not USE_INSTRUCT_EMBEDDINGS:\n",
    "    model_name = 'BAAI/bge-base-en-v1.5'\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "else:\n",
    "    model_name = \"hkunlp/instructor-large\"\n",
    "    embed_instruction = \"Represent the Hugging Face library documentation\"\n",
    "    query_instruction = \"Query the most relevant piece of information from the Hugging Face documentation\"\n",
    "\n",
    "    embedding_model = HuggingFaceInstructEmbeddings(\n",
    "        model_name=model_name,\n",
    "        embed_instruction=embed_instruction,\n",
    "        query_instruction=query_instruction\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_model.embed_documents(texts=[d.page_content for d in langchain_docs[:1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "index = FAISS.from_documents(langchain_docs[:1000], embedding_model)\n",
    "\n",
    "index_name = 'index_1000'\n",
    "index.save_local(f'./data/indexes/{index_name}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = FAISS.load_local(f'./data/indexes/{index_name}/', embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# load the pipeline\n",
      "# make sure you're logged in with `huggingface-cli login`\n",
      "model_id_or_path = \"CompVis/stable-diffusion-v1-4\"\n",
      "scheduler = DDIMScheduler.from_pretrained(model_id_or_path, subfolder=\"scheduler\")\n",
      "pipe = CycleDiffusionPipeline.from_pretrained(model_id_or_path, scheduler=scheduler).to(\"cuda\")\n",
      "\n",
      "# let's download an initial image\n",
      "url = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/An%20astronaut%20riding%20a%20horse.png\"\n",
      "response = requests.get(url)\n",
      "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
      "init_image = init_image.resize((512, 512))\n",
      "init_image.save(\"horse.png\")\n",
      "\n",
      "# let's specify a prompt\n",
      "source_prompt = \"An astronaut riding a horse\"\n",
      "prompt = \"An astronaut riding an elephant\"\n",
      "\n",
      "# call the pipeline\n",
      "image = pipe(\n",
      "prompt=prompt,\n",
      "source_prompt=source_prompt,\n",
      "image=init_image,\n",
      "num_inference_steps=100,\n",
      "eta=0.1,\n",
      "strength=0.8,\n",
      "guidance_scale=2,\n",
      "source_guidance_scale=1,\n",
      ").images[0]\n",
      "\n",
      "image.save(\"horse_to_elephant.png\")\n",
      "\n",
      "# let's try another example\n",
      "# See more samples at the original repo: https://github.com/ChenWu98/cycle-diffusion\n",
      "url = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/A%20black%20colored%20car.png\"\n",
      "response = requests.get(url)\n",
      "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
      "init_image = init_image.resize((512, 512))\n",
      "init_image.save(\"black.png\")\n",
      "\n",
      "source_prompt = \"A black colored car\"\n",
      "prompt = \"A blue colored car\"\n",
      "\n",
      "# call the pipeline\n",
      "torch.manual_seed(0)\n",
      "image = pipe(\n",
      "prompt=prompt,\n",
      "source_prompt=source_prompt,\n",
      "image=init_image,\n",
      "num_inference_steps=100,\n",
      "eta=0.1,\n",
      "strength=0.85,\n",
      "guidance_scale=3,\n",
      "source_guidance_scale=1,\n",
      ").images[0]\n",
      "\n",
      "image.save(\"black_to_blue.png\")\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md',\n",
       " '_split_id': 8,\n",
       " '_split_overlap': []}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = index.similarity_search(query='how to create a pipeline object?', k=5)\n",
    "print(docs[0].page_content)\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# load the pipeline\n",
      "# make sure you're logged in with `huggingface-cli login`\n",
      "model_id_or_path = \"CompVis/stable-diffusion-v1-4\"\n",
      "scheduler = DDIMScheduler.from_pretrained(model_id_or_path, subfolder=\"scheduler\")\n",
      "pipe = CycleDiffusionPipeline.from_pretrained(model_id_or_path, scheduler=scheduler).to(\"cuda\")\n",
      "\n",
      "# let's download an initial image\n",
      "url = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/An%20astronaut%20riding%20a%20horse.png\"\n",
      "response = requests.get(url)\n",
      "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
      "init_image = init_image.resize((512, 512))\n",
      "init_image.save(\"horse.png\")\n",
      "\n",
      "# let's specify a prompt\n",
      "source_prompt = \"An astronaut riding a horse\"\n",
      "prompt = \"An astronaut riding an elephant\"\n",
      "\n",
      "# call the pipeline\n",
      "image = pipe(\n",
      "prompt=prompt,\n",
      "source_prompt=source_prompt,\n",
      "image=init_image,\n",
      "num_inference_steps=100,\n",
      "eta=0.1,\n",
      "strength=0.8,\n",
      "guidance_scale=2,\n",
      "source_guidance_scale=1,\n",
      ").images[0]\n",
      "\n",
      "image.save(\"horse_to_elephant.png\")\n",
      "\n",
      "# let's try another example\n",
      "# See more samples at the original repo: https://github.com/ChenWu98/cycle-diffusion\n",
      "url = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/A%20black%20colored%20car.png\"\n",
      "response = requests.get(url)\n",
      "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
      "init_image = init_image.resize((512, 512))\n",
      "init_image.save(\"black.png\")\n",
      "\n",
      "source_prompt = \"A black colored car\"\n",
      "prompt = \"A blue colored car\"\n",
      "\n",
      "# call the pipeline\n",
      "torch.manual_seed(0)\n",
      "image = pipe(\n",
      "prompt=prompt,\n",
      "source_prompt=source_prompt,\n",
      "image=init_image,\n",
      "num_inference_steps=100,\n",
      "eta=0.1,\n",
      "strength=0.85,\n",
      "guidance_scale=3,\n",
      "source_guidance_scale=1,\n",
      ").images[0]\n",
      "\n",
      "image.save(\"black_to_blue.png\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14fe181fa6c407ebdd55f10ef085f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/791 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe343cfd44844cdb3a6b652f686d4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/ml2/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025d6bce0e37486082bf0ec63a5927b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91424eab5ee64abb98ee85c137c4852b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0986bc10141e443fbe1dc54c6189bed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knowledge_index = FAISS.load_local(f'./data/indexes/{index_name}/', embedding_model)\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reader - LLM\n",
    "- zero-shot vs few-shot prompting (cf [resource](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant#6-using-qdrant-to-improve-rag-prompt))\n",
    "- tune the number of examples retrieved\n",
    "- make conversational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context, \n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "  </s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\"text2text-generation\", model='HuggingFaceH4/zephyr-7b-beta')\n",
    "\n",
    "llm('Ok,', max_new_tokens=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HF_TOKEN = os.environ.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "API_URL = \"https://dxsuz0i09l5zzjh1.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "\n",
    "headers = {\n",
    "\t\"Authorization\": f\"Bearer {HF_TOKEN}\",\n",
    "\t\"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def llm(question):\n",
    "\tpayload = {\n",
    "\t\t\"inputs\": question,\n",
    "\t\t\"max_new_tokens\": 2000,\n",
    "\t}\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = llm('Ok,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' so I\\'m not really a fan of the whole \"New Year, New Me\" thing.'}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, llm, num_retrieved_docs: int = 15, num_reranked_docs: int = 7):\n",
    "    # Gather documents with retriever\n",
    "        \n",
    "    relevant_docs = knowledge_index.similarity_search(\n",
    "        query=question,\n",
    "        k=num_retrieved_docs\n",
    "    )\n",
    "\n",
    "    # Chosse the most relevant documents with reranker\n",
    "    cross_encoding_predictions = reranker.predict(\n",
    "        [(question, doc.page_content) for doc in relevant_docs]\n",
    "    )\n",
    "    relevant_docs = [\n",
    "        doc for _, doc in sorted(\n",
    "            zip(cross_encoding_predictions, relevant_docs),\n",
    "            reverse=True, key = lambda x: x[0]\n",
    "        )\n",
    "    ]\n",
    "    relevant_docs = relevant_docs[:num_reranked_docs]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = '\\nExtracted documents:\\n'\n",
    "    context += ''.join([f\"{str(i)}: \" + doc.page_content for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = prompt_template.format(\n",
    "        context=context,\n",
    "        question=question\n",
    "    )\n",
    "    print('Finished retrieving')\n",
    "    # Redact an answer\n",
    "    full_answer = llm(final_prompt)[0]['generated_text']\n",
    "    answer = full_answer[len(final_prompt):]\n",
    "    print(full_answer, answer)\n",
    "\n",
    "    return full_answer, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how to create a pipeline object?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished retrieving\n",
      "To create a pipeline object in Hugging Face's Transformers library, you need to follow \n"
     ]
    }
   ],
   "source": [
    "answer, relevant_docs = answer_question(question, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_answer(answer, relevant_docs):\n",
    "    print(f'Answer: {answer}')\n",
    "    print('\\n\\nSource documents:')\n",
    "    for doc in relevant_docs:\n",
    "        print(f'{doc.metadata[\"source\"]}')\n",
    "        print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: To create a pipeline object in Hugging Face's Transformers library, you need to follow\n",
      "\n",
      "\n",
      "Source documents:\n",
      "huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md\n",
      "# load the pipeline\n",
      "# make sure you're logged in with `huggingface-cli login`\n",
      "model_id_or_path = \"CompVis/stable-diffusion-v1-4\"\n",
      "scheduler = DDIMScheduler.from_pretrained(model_id_or_path, subfolder=\"scheduler\")\n",
      "pipe = CycleDiffusionPipeline.from_pretrained(model_id_or_path, scheduler=scheduler).to(\"cuda\")\n",
      "\n",
      "# let's download an initial image\n",
      "url = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/An%20astronaut%20riding%20a%20horse.png\"\n",
      "response = requests.get(url)\n",
      "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
      "init_image = init_image.resize((512, 512))\n",
      "init_image.save(\"horse.png\")\n",
      "\n",
      "# let's specify a prompt\n",
      "source_prompt = \"An astronaut riding a horse\"\n",
      "prompt = \"An astronaut riding an elephant\"\n",
      "\n",
      "# call the pipeline\n",
      "image = pipe(\n",
      "prompt=prompt,\n",
      "source_prompt=source_prompt,\n",
      "image=init_image,\n",
      "num_inference_steps=100,\n",
      "eta=0.1,\n",
      "strength=0.8,\n",
      "guidance_scale=2,\n",
      "source_guidance_scale=1,\n",
      ").images[0]\n",
      "\n",
      "image.save(\"horse_to_elephant.png\")\n",
      "\n",
      "# let's try another example\n",
      "# See more samples at the original repo: https://github.com/ChenWu98/cycle-diffusion\n",
      "url = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/A%20black%20colored%20car.png\"\n",
      "response = requests.get(url)\n",
      "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
      "init_image = init_image.resize((512, 512))\n",
      "init_image.save(\"black.png\")\n",
      "\n",
      "source_prompt = \"A black colored car\"\n",
      "prompt = \"A blue colored car\"\n",
      "\n",
      "# call the pipeline\n",
      "torch.manual_seed(0)\n",
      "image = pipe(\n",
      "prompt=prompt,\n",
      "source_prompt=source_prompt,\n",
      "image=init_image,\n",
      "num_inference_steps=100,\n",
      "eta=0.1,\n",
      "strength=0.85,\n",
      "guidance_scale=3,\n",
      "source_guidance_scale=1,\n",
      ").images[0]\n",
      "\n",
      "image.save(\"black_to_blue.png\")\n",
      "```\n",
      "huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx\n",
      "For instance, here is how we can post-process\n",
      "to make the inputs suitable for the BERT model:\n",
      "\n",
      "<tokenizerslangcontent>\n",
      "<python>\n",
      "<literalinclude>\n",
      "{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\n",
      "\"language\": \"python\",\n",
      "\"start-after\": \"START setup_processor\",\n",
      "\"end-before\": \"END setup_processor\",\n",
      "\"dedent\": 8}\n",
      "</literalinclude>\n",
      "</python>\n",
      "<rust>\n",
      "<literalinclude>\n",
      "{\"path\": \"../../tokenizers/tests/documentation.rs\",\n",
      "\"language\": \"rust\",\n",
      "\"start-after\": \"START pipeline_setup_processor\",\n",
      "\"end-before\": \"END pipeline_setup_processor\",\n",
      "\"dedent\": 4}\n",
      "</literalinclude>\n",
      "</rust>\n",
      "<node>\n",
      "<literalinclude>\n",
      "{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\n",
      "\"language\": \"js\",\n",
      "\"start-after\": \"START setup_processor\",\n",
      "\"end-before\": \"END setup_processor\",\n",
      "\"dedent\": 8}\n",
      "</literalinclude>\n",
      "</node>\n",
      "</tokenizerslangcontent>\n",
      "\n",
      "Note that contrarily to the pre-tokenizer or the normalizer, you don't\n",
      "need to retrain a tokenizer after changing its post-processor.\n",
      "\n",
      "## All together: a BERT tokenizer from scratch\n",
      "\n",
      "Let's put all those pieces together to build a BERT tokenizer. \n",
      "huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx\n",
      "As we saw in the quick tour, we can customize the post processor of a\n",
      "`Tokenizer` by setting the\n",
      "corresponding attribute. For instance, here is how we can post-process\n",
      "to make the inputs suitable for the BERT model:\n",
      "\n",
      "<tokenizerslangcontent>\n",
      "<python>\n",
      "<literalinclude>\n",
      "{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\n",
      "\"language\": \"python\",\n",
      "\"start-after\": \"START setup_processor\",\n",
      "\"end-before\": \"END setup_processor\",\n",
      "\"dedent\": 8}\n",
      "</literalinclude>\n",
      "</python>\n",
      "<rust>\n",
      "<literalinclude>\n",
      "{\"path\": \"../../tokenizers/tests/documentation.rs\",\n",
      "\"language\": \"rust\",\n",
      "\"start-after\": \"START pipeline_setup_processor\",\n",
      "\"end-before\": \"END pipeline_setup_processor\",\n",
      "\"dedent\": 4}\n",
      "</literalinclude>\n",
      "</rust>\n",
      "<node>\n",
      "<literalinclude>\n",
      "{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\n",
      "\"language\": \"js\",\n",
      "\"start-after\": \"START setup_processor\",\n",
      "\"end-before\": \"END setup_processor\",\n",
      "\"dedent\": 8}\n",
      "</literalinclude>\n",
      "</node>\n",
      "</tokenizerslangcontent>\n",
      "\n",
      "Note that contrarily to the pre-tokenizer or the normalizer, you don't\n",
      "need to retrain a tokenizer after changing its post-processor.\n",
      "\n",
      "\n",
      "huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx\n",
      "If you're already\n",
      "familiar with those steps and want to learn by seeing some code, jump to\n",
      "`our BERT from scratch example <example>`.\n",
      "\n",
      "For the examples that require a `Tokenizer` we will use the tokenizer we trained in the\n",
      "`quicktour`, which you can load with:\n",
      "\n",
      "<tokenizerslangcontent>\n",
      "<python>\n",
      "<literalinclude>\n",
      "{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\n",
      "\"language\": \"python\",\n",
      "\"start-after\": \"START reload_tokenizer\",\n",
      "\"end-before\": \"END reload_tokenizer\",\n",
      "\"dedent\": 12}\n",
      "</literalinclude>\n",
      "</python>\n",
      "<rust>\n",
      "<literalinclude>\n",
      "{\"path\": \"../../tokenizers/tests/documentation.rs\",\n",
      "\"language\": \"rust\",\n",
      "\"start-after\": \"START pipeline_reload_tokenizer\",\n",
      "\"end-before\": \"END pipeline_reload_tokenizer\",\n",
      "\"dedent\": 4}\n",
      "</literalinclude>\n",
      "</rust>\n",
      "<node>\n",
      "<literalinclude>\n",
      "{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\n",
      "\"language\": \"js\",\n",
      "\"start-after\": \"START reload_tokenizer\",\n",
      "\"end-before\": \"END reload_tokenizer\",\n",
      "\"dedent\": 8}\n",
      "</literalinclude>\n",
      "</node>\n",
      "</tokenizerslangcontent>\n",
      "\n",
      "## Normalization\n",
      "\n",
      "Normalization is, in a nutshell, a set of operations you apply to a raw\n",
      "string to make it less random or \"cleaner\". \n",
      "huggingface/blog/blob/main/informer.md\n",
      "Again, we'll use the GluonTS library for this. We define a `Chain` of transformations (which is a bit comparable to `torchvision.transforms.Compose` for images). It allows us to combine several transformations into a single pipeline.\n",
      "\n",
      "```python\n",
      "from gluonts.time_feature import TimeFeature\n",
      "from gluonts.dataset.field_names import FieldName\n",
      "from gluonts.transform import (\n",
      "AddAgeFeature,\n",
      "AddObservedValuesIndicator,\n",
      "AddTimeFeatures,\n",
      "AsNumpyArray,\n",
      "Chain,\n",
      "ExpectedNumInstanceSampler,\n",
      "InstanceSplitter,\n",
      "RemoveFields,\n",
      "SelectFields,\n",
      "SetField,\n",
      "TestSplitSampler,\n",
      "Transformation,\n",
      "ValidationSplitSampler,\n",
      "VstackFeatures,\n",
      "RenameFields,\n",
      ")\n",
      "```\n",
      "\n",
      "The transformations below are annotated with comments, to explain what they do. \n",
      "huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md\n",
      "## Available Pipelines:\n",
      "\n",
      "| Pipeline | Tasks | Colab\n",
      "|---|---|:---:|\n",
      "| [pipeline_stable_diffusion.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py) | *Text-to-Image Generation* | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb)\n",
      "| [pipeline_stable_diffusion_img2img](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py) | *Image-to-Image Text-Guided Generation* | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/image_2_image_using_diffusers.ipynb)\n",
      "| [pipeline_stable_diffusion_inpaint](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py) | *Text-Guided Image Inpainting* | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/in_painting_with_stable_diffusion_using_diffusers.ipynb)\n",
      "\n",
      "## Examples:\n",
      "\n",
      "### Using Stable Diffusion without being logged into the Hub.\n",
      "\n",
      "If you want to download the model weights using a single Python line, you need to be logged in via `huggingface-cli login`.\n",
      "\n",
      "\n",
      "huggingface/simulate/blob/main/docs/source/howto/map_pools.mdx\n",
      "First define a function that will generate your environment, we call each environment instance a \"map\".\n",
      "\n",
      "```\n",
      "def generate_map(index):\n",
      "root = sm.Asset(name=f\"root_{index}\")\n",
      "root += sm.Box(\n",
      "name=f\"floor_{index}\",\n",
      "position=[0, -0.05, 0],\n",
      "scaling=[10, 0.1, 10],\n",
      "material=sm.Material.BLUE,\n",
      "with_collider=True,\n",
      ")\n",
      "root += sm.Box(\n",
      "name=f\"wall1_{index}\",\n",
      "position=[-1, 0.5, 0],\n",
      "scaling=[0.1, 1, 5.1],\n",
      "material=sm.Material.GRAY75,\n",
      "with_collider=True,\n",
      ")\n",
      "root += sm.Box(\n",
      "name=f\"wall2_{index}\",\n",
      "position=[1, 0.5, 0],\n",
      "scaling=[0.1, 1, 5.1],\n",
      "material=sm.Material.GRAY75,\n",
      "with_collider=True,\n",
      ")\n",
      "root += sm.Box(\n",
      "name=f\"wall3_{index}\",\n",
      "position=[0, 0.5, 4.5],\n",
      "scaling=[5.9, 1, 0.1],\n",
      "material=sm.Material.GRAY75,\n",
      "with_collider=True,\n",
      ")\n",
      "\n",
      "# add actors, sensors, reward functions etc ...\n",
      "\n",
      "return root\n",
      "```\n",
      "\n",
      "You can then provide the `generate_map` method as an argument to the `sm.ParallelRLEnv` class, which will instantiate `n_maps`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pretty_print_answer(answer, relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking the chosen system on your evaluation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
